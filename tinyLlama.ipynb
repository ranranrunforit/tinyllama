{"cells":[{"cell_type":"markdown","metadata":{"id":"bIoMiKrJyxKp"},"source":["# Set up Environment"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"6BIgVuLqy0qF","outputId":"02d09b6a-cbad-41c6-e10b-119a6796350a"},"outputs":[],"source":["!pip install pytorch==2.1.2 torchvision==0.16.2 torchaudio==2.1.2 pytorch-cuda=11.8 -c pytorch -c nvidia\n","!pip install tqdm==4.66.1\n","!pip install requests==2.31.0\n","!pip install importlib-metadata==4.13.0\n","!pip install filelock==3.0.12\n","!pip install scikit-learn==1.2.2\n","!pip install numpy==1.26.3\n","!pip install tokenizers==0.13.3\n","!pip install sentencepiece==0.1.99\n","#!wget https://www.cs.cmu.edu/~vijayv/stories42M.pt"]},{"cell_type":"markdown","metadata":{"id":"iX5HD8FWCWmV"},"source":["# utils.py"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":3860,"status":"ok","timestamp":1732188218034,"user":{"displayName":"Chaoran Zhou","userId":"09162553986537566448"},"user_tz":-480},"id":"dhsgkxJzCV4B"},"outputs":[],"source":["from typing import Dict, List, Optional, Union, Tuple, BinaryIO\n","import os\n","import sys\n","import json\n","import tempfile\n","import copy\n","from tqdm.auto import tqdm\n","from functools import partial\n","from urllib.parse import urlparse\n","from pathlib import Path\n","import requests\n","from hashlib import sha256\n","from filelock import FileLock\n","import importlib_metadata\n","import torch\n","import torch.nn as nn\n","from torch import Tensor\n","\n","__version__ = \"4.0.0\"\n","_torch_version = importlib_metadata.version(\"torch\")\n","\n","hf_cache_home = os.path.expanduser(os.getenv(\"HF_HOME\", os.path.join(os.getenv(\"XDG_CACHE_HOME\", \"~/.cache\"), \"huggingface\")))\n","default_cache_path = os.path.join(hf_cache_home, \"transformers\")\n","PYTORCH_PRETRAINED_BERT_CACHE = os.getenv(\"PYTORCH_PRETRAINED_BERT_CACHE\", default_cache_path)\n","PYTORCH_TRANSFORMERS_CACHE = os.getenv(\"PYTORCH_TRANSFORMERS_CACHE\", PYTORCH_PRETRAINED_BERT_CACHE)\n","TRANSFORMERS_CACHE = os.getenv(\"TRANSFORMERS_CACHE\", PYTORCH_TRANSFORMERS_CACHE)\n","\n","PRESET_MIRROR_DICT = {\n","    \"tuna\": \"https://mirrors.tuna.tsinghua.edu.cn/hugging-face-models\",\n","    \"bfsu\": \"https://mirrors.bfsu.edu.cn/hugging-face-models\",\n","}\n","HUGGINGFACE_CO_PREFIX = \"https://huggingface.co/{model_id}/resolve/{revision}/{filename}\"\n","WEIGHTS_NAME = \"pytorch_model.bin\"\n","CONFIG_NAME = \"config.json\"\n","\n","\n","def is_torch_available():\n","  return True\n","\n","\n","def is_tf_available():\n","  return False\n","\n","\n","def is_remote_url(url_or_filename):\n","  parsed = urlparse(url_or_filename)\n","  return parsed.scheme in (\"http\", \"https\")\n","\n","\n","def http_get(url: str, temp_file: BinaryIO, proxies=None, resume_size=0, headers: Optional[Dict[str, str]] = None):\n","  headers = copy.deepcopy(headers)\n","  if resume_size > 0:\n","    headers[\"Range\"] = \"bytes=%d-\" % (resume_size,)\n","  r = requests.get(url, stream=True, proxies=proxies, headers=headers)\n","  r.raise_for_status()\n","  content_length = r.headers.get(\"Content-Length\")\n","  total = resume_size + int(content_length) if content_length is not None else None\n","  progress = tqdm(\n","    unit=\"B\",\n","    unit_scale=True,\n","    total=total,\n","    initial=resume_size,\n","    desc=\"Downloading\",\n","    disable=False,\n","  )\n","  for chunk in r.iter_content(chunk_size=1024):\n","    if chunk:  # filter out keep-alive new chunks\n","      progress.update(len(chunk))\n","      temp_file.write(chunk)\n","  progress.close()\n","\n","\n","def url_to_filename(url: str, etag: Optional[str] = None) -> str:\n","  url_bytes = url.encode(\"utf-8\")\n","  filename = sha256(url_bytes).hexdigest()\n","\n","  if etag:\n","    etag_bytes = etag.encode(\"utf-8\")\n","    filename += \".\" + sha256(etag_bytes).hexdigest()\n","\n","  if url.endswith(\".h5\"):\n","    filename += \".h5\"\n","\n","  return filename\n","\n","\n","def hf_bucket_url(\n","  model_id: str, filename: str, subfolder: Optional[str] = None, revision: Optional[str] = None, mirror=None\n",") -> str:\n","  if subfolder is not None:\n","    filename = f\"{subfolder}/{filename}\"\n","\n","  if mirror:\n","    endpoint = PRESET_MIRROR_DICT.get(mirror, mirror)\n","    legacy_format = \"/\" not in model_id\n","    if legacy_format:\n","      return f\"{endpoint}/{model_id}-{filename}\"\n","    else:\n","      return f\"{endpoint}/{model_id}/{filename}\"\n","\n","  if revision is None:\n","    revision = \"main\"\n","  return HUGGINGFACE_CO_PREFIX.format(model_id=model_id, revision=revision, filename=filename)\n","\n","\n","def http_user_agent(user_agent: Union[Dict, str, None] = None) -> str:\n","  ua = \"transformers/{}; python/{}\".format(__version__, sys.version.split()[0])\n","  if is_torch_available():\n","    ua += f\"; torch/{_torch_version}\"\n","  if is_tf_available():\n","    ua += f\"; tensorflow/{_tf_version}\"\n","  if isinstance(user_agent, dict):\n","    ua += \"; \" + \"; \".join(\"{}/{}\".format(k, v) for k, v in user_agent.items())\n","  elif isinstance(user_agent, str):\n","    ua += \"; \" + user_agent\n","  return ua\n","\n","\n","def get_from_cache(\n","  url: str,\n","  cache_dir=None,\n","  force_download=False,\n","  proxies=None,\n","  etag_timeout=10,\n","  resume_download=False,\n","  user_agent: Union[Dict, str, None] = None,\n","  use_auth_token: Union[bool, str, None] = None,\n","  local_files_only=False,\n",") -> Optional[str]:\n","  if cache_dir is None:\n","    cache_dir = TRANSFORMERS_CACHE\n","  if isinstance(cache_dir, Path):\n","    cache_dir = str(cache_dir)\n","\n","  os.makedirs(cache_dir, exist_ok=True)\n","\n","  headers = {\"user-agent\": http_user_agent(user_agent)}\n","  if isinstance(use_auth_token, str):\n","    headers[\"authorization\"] = \"Bearer {}\".format(use_auth_token)\n","  elif use_auth_token:\n","    token = HfFolder.get_token()\n","    if token is None:\n","      raise EnvironmentError(\"You specified use_auth_token=True, but a huggingface token was not found.\")\n","    headers[\"authorization\"] = \"Bearer {}\".format(token)\n","\n","  url_to_download = url\n","  etag = None\n","  if not local_files_only:\n","    try:\n","      r = requests.head(url, headers=headers, allow_redirects=False, proxies=proxies, timeout=etag_timeout)\n","      r.raise_for_status()\n","      etag = r.headers.get(\"X-Linked-Etag\") or r.headers.get(\"ETag\")\n","      # We favor a custom header indicating the etag of the linked resource, and\n","      # we fallback to the regular etag header.\n","      # If we don't have any of those, raise an error.\n","      if etag is None:\n","        raise OSError(\n","          \"Distant resource does not have an ETag, we won't be able to reliably ensure reproducibility.\"\n","        )\n","      # In case of a redirect,\n","      # save an extra redirect on the request.get call,\n","      # and ensure we download the exact atomic version even if it changed\n","      # between the HEAD and the GET (unlikely, but hey).\n","      if 300 <= r.status_code <= 399:\n","        url_to_download = r.headers[\"Location\"]\n","    except (requests.exceptions.ConnectionError, requests.exceptions.Timeout):\n","      # etag is already None\n","      pass\n","\n","  filename = url_to_filename(url, etag)\n","\n","  # get cache path to put the file\n","  cache_path = os.path.join(cache_dir, filename)\n","\n","  # etag is None == we don't have a connection or we passed local_files_only.\n","  # try to get the last downloaded one\n","  if etag is None:\n","    if os.path.exists(cache_path):\n","      return cache_path\n","    else:\n","      matching_files = [\n","        file\n","        for file in fnmatch.filter(os.listdir(cache_dir), filename.split(\".\")[0] + \".*\")\n","        if not file.endswith(\".json\") and not file.endswith(\".lock\")\n","      ]\n","      if len(matching_files) > 0:\n","        return os.path.join(cache_dir, matching_files[-1])\n","      else:\n","        # If files cannot be found and local_files_only=True,\n","        # the models might've been found if local_files_only=False\n","        # Notify the user about that\n","        if local_files_only:\n","          raise FileNotFoundError(\n","            \"Cannot find the requested files in the cached path and outgoing traffic has been\"\n","            \" disabled. To enable model look-ups and downloads online, set 'local_files_only'\"\n","            \" to False.\"\n","          )\n","        else:\n","          raise ValueError(\n","            \"Connection error, and we cannot find the requested files in the cached path.\"\n","            \" Please try again or make sure your Internet connection is on.\"\n","          )\n","\n","  # From now on, etag is not None.\n","  if os.path.exists(cache_path) and not force_download:\n","    return cache_path\n","\n","  # Prevent parallel downloads of the same file with a lock.\n","  lock_path = cache_path + \".lock\"\n","  with FileLock(lock_path):\n","\n","    # If the download just completed while the lock was activated.\n","    if os.path.exists(cache_path) and not force_download:\n","      # Even if returning early like here, the lock will be released.\n","      return cache_path\n","\n","    if resume_download:\n","      incomplete_path = cache_path + \".incomplete\"\n","\n","      @contextmanager\n","      def _resumable_file_manager() -> \"io.BufferedWriter\":\n","        with open(incomplete_path, \"ab\") as f:\n","          yield f\n","\n","      temp_file_manager = _resumable_file_manager\n","      if os.path.exists(incomplete_path):\n","        resume_size = os.stat(incomplete_path).st_size\n","      else:\n","        resume_size = 0\n","    else:\n","      temp_file_manager = partial(tempfile.NamedTemporaryFile, mode=\"wb\", dir=cache_dir, delete=False)\n","      resume_size = 0\n","\n","    # Download to temporary file, then copy to cache dir once finished.\n","    # Otherwise you get corrupt cache entries if the download gets interrupted.\n","    with temp_file_manager() as temp_file:\n","      http_get(url_to_download, temp_file, proxies=proxies, resume_size=resume_size, headers=headers)\n","\n","    os.replace(temp_file.name, cache_path)\n","\n","    meta = {\"url\": url, \"etag\": etag}\n","    meta_path = cache_path + \".json\"\n","    with open(meta_path, \"w\") as meta_file:\n","      json.dump(meta, meta_file)\n","\n","  return cache_path\n","\n","\n","def cached_path(\n","  url_or_filename,\n","  cache_dir=None,\n","  force_download=False,\n","  proxies=None,\n","  resume_download=False,\n","  user_agent: Union[Dict, str, None] = None,\n","  extract_compressed_file=False,\n","  force_extract=False,\n","  use_auth_token: Union[bool, str, None] = None,\n","  local_files_only=False,\n",") -> Optional[str]:\n","  if cache_dir is None:\n","    cache_dir = TRANSFORMERS_CACHE\n","  if isinstance(url_or_filename, Path):\n","    url_or_filename = str(url_or_filename)\n","  if isinstance(cache_dir, Path):\n","    cache_dir = str(cache_dir)\n","\n","  if is_remote_url(url_or_filename):\n","    # URL, so get it from the cache (downloading if necessary)\n","    output_path = get_from_cache(\n","      url_or_filename,\n","      cache_dir=cache_dir,\n","      force_download=force_download,\n","      proxies=proxies,\n","      resume_download=resume_download,\n","      user_agent=user_agent,\n","      use_auth_token=use_auth_token,\n","      local_files_only=local_files_only,\n","    )\n","  elif os.path.exists(url_or_filename):\n","    # File, and it exists.\n","    output_path = url_or_filename\n","  elif urlparse(url_or_filename).scheme == \"\":\n","    # File, but it doesn't exist.\n","    raise EnvironmentError(\"file {} not found\".format(url_or_filename))\n","  else:\n","    # Something unknown\n","    raise ValueError(\"unable to parse {} as a URL or as a local path\".format(url_or_filename))\n","\n","  if extract_compressed_file:\n","    if not is_zipfile(output_path) and not tarfile.is_tarfile(output_path):\n","      return output_path\n","\n","    # Path where we extract compressed archives\n","    # We avoid '.' in dir name and add \"-extracted\" at the end: \"./model.zip\" => \"./model-zip-extracted/\"\n","    output_dir, output_file = os.path.split(output_path)\n","    output_extract_dir_name = output_file.replace(\".\", \"-\") + \"-extracted\"\n","    output_path_extracted = os.path.join(output_dir, output_extract_dir_name)\n","\n","    if os.path.isdir(output_path_extracted) and os.listdir(output_path_extracted) and not force_extract:\n","      return output_path_extracted\n","\n","    # Prevent parallel extractions\n","    lock_path = output_path + \".lock\"\n","    with FileLock(lock_path):\n","      shutil.rmtree(output_path_extracted, ignore_errors=True)\n","      os.makedirs(output_path_extracted)\n","      if is_zipfile(output_path):\n","        with ZipFile(output_path, \"r\") as zip_file:\n","          zip_file.extractall(output_path_extracted)\n","          zip_file.close()\n","      elif tarfile.is_tarfile(output_path):\n","        tar_file = tarfile.open(output_path)\n","        tar_file.extractall(output_path_extracted)\n","        tar_file.close()\n","      else:\n","        raise EnvironmentError(\"Archive format of {} could not be identified\".format(output_path))\n","\n","    return output_path_extracted\n","\n","  return output_path\n","\n","\n","def get_parameter_dtype(parameter: Union[nn.Module]):\n","  try:\n","    return next(parameter.parameters()).dtype\n","  except StopIteration:\n","    # For nn.DataParallel compatibility in PyTorch 1.5\n","\n","    def find_tensor_attributes(module: nn.Module) -> List[Tuple[str, Tensor]]:\n","      tuples = [(k, v) for k, v in module.__dict__.items() if torch.is_tensor(v)]\n","      return tuples\n","\n","    gen = parameter._named_members(get_members_fn=find_tensor_attributes)\n","    first_tuple = next(gen)\n","    return first_tuple[1].dtype\n","\n","\n","def get_extended_attention_mask(attention_mask: Tensor, dtype) -> Tensor:\n","  # attention_mask [batch_size, seq_length]\n","  assert attention_mask.dim() == 2\n","  # [batch_size, 1, 1, seq_length] for multi-head attention\n","  extended_attention_mask = attention_mask[:, None, None, :]\n","  extended_attention_mask = extended_attention_mask.to(dtype=dtype)  # fp16 compatibility\n","  extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n","  return extended_attention_mask"]},{"cell_type":"markdown","metadata":{"id":"53jlRB2ODv-R"},"source":["# config.py"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":676,"status":"ok","timestamp":1732188231047,"user":{"displayName":"Chaoran Zhou","userId":"09162553986537566448"},"user_tz":-480},"id":"CFRUVYnADvjJ"},"outputs":[],"source":["from typing import Union, Tuple, Dict, Any, Optional\n","import os\n","import json\n","from collections import OrderedDict\n","import torch\n","# from utils import CONFIG_NAME, hf_bucket_url, cached_path, is_remote_url\n","\n","class PretrainedConfig(object):\n","  model_type: str = \"\"\n","  is_composition: bool = False\n","\n","  def __init__(self, **kwargs):\n","    # Attributes with defaults\n","    self.return_dict = kwargs.pop(\"return_dict\", True)\n","    self.output_hidden_states = kwargs.pop(\"output_hidden_states\", False)\n","    self.output_attentions = kwargs.pop(\"output_attentions\", False)\n","    self.torchscript = kwargs.pop(\"torchscript\", False)  # Only used by PyTorch models\n","    self.use_bfloat16 = kwargs.pop(\"use_bfloat16\", False)\n","    self.pruned_heads = kwargs.pop(\"pruned_heads\", {})\n","    self.tie_word_embeddings = kwargs.pop(\n","      \"tie_word_embeddings\", True\n","    )  # Whether input and output word embeddings should be tied for all MLM, LM and Seq2Seq models.\n","\n","    # Is decoder is used in encoder-decoder models to differentiate encoder from decoder\n","    self.is_encoder_decoder = kwargs.pop(\"is_encoder_decoder\", False)\n","    self.is_decoder = kwargs.pop(\"is_decoder\", False)\n","    self.add_cross_attention = kwargs.pop(\"add_cross_attention\", False)\n","    self.tie_encoder_decoder = kwargs.pop(\"tie_encoder_decoder\", False)\n","\n","    # Parameters for sequence generation\n","    self.max_length = kwargs.pop(\"max_length\", 20)\n","    self.min_length = kwargs.pop(\"min_length\", 0)\n","    self.do_sample = kwargs.pop(\"do_sample\", False)\n","    self.early_stopping = kwargs.pop(\"early_stopping\", False)\n","    self.num_beams = kwargs.pop(\"num_beams\", 1)\n","    self.num_beam_groups = kwargs.pop(\"num_beam_groups\", 1)\n","    self.diversity_penalty = kwargs.pop(\"diversity_penalty\", 0.0)\n","    self.temperature = kwargs.pop(\"temperature\", 1.0)\n","    self.top_k = kwargs.pop(\"top_k\", 50)\n","    self.top_p = kwargs.pop(\"top_p\", 1.0)\n","    self.repetition_penalty = kwargs.pop(\"repetition_penalty\", 1.0)\n","    self.length_penalty = kwargs.pop(\"length_penalty\", 1.0)\n","    self.no_repeat_ngram_size = kwargs.pop(\"no_repeat_ngram_size\", 0)\n","    self.encoder_no_repeat_ngram_size = kwargs.pop(\"encoder_no_repeat_ngram_size\", 0)\n","    self.bad_words_ids = kwargs.pop(\"bad_words_ids\", None)\n","    self.num_return_sequences = kwargs.pop(\"num_return_sequences\", 1)\n","    self.chunk_size_feed_forward = kwargs.pop(\"chunk_size_feed_forward\", 0)\n","    self.output_scores = kwargs.pop(\"output_scores\", False)\n","    self.return_dict_in_generate = kwargs.pop(\"return_dict_in_generate\", False)\n","    self.forced_bos_token_id = kwargs.pop(\"forced_bos_token_id\", None)\n","    self.forced_eos_token_id = kwargs.pop(\"forced_eos_token_id\", None)\n","\n","    # Fine-tuning task arguments\n","    self.architectures = kwargs.pop(\"architectures\", None)\n","    self.finetuning_task = kwargs.pop(\"finetuning_task\", None)\n","    self.id2label = kwargs.pop(\"id2label\", None)\n","    self.label2id = kwargs.pop(\"label2id\", None)\n","    if self.id2label is not None:\n","      kwargs.pop(\"num_labels\", None)\n","      self.id2label = dict((int(key), value) for key, value in self.id2label.items())\n","      # Keys are always strings in JSON so convert ids to int here.\n","    else:\n","      self.num_labels = kwargs.pop(\"num_labels\", 2)\n","\n","    # Tokenizer arguments\n","    self.tokenizer_class = kwargs.pop(\"tokenizer_class\", None)\n","    self.prefix = kwargs.pop(\"prefix\", None)\n","    self.bos_token_id = kwargs.pop(\"bos_token_id\", None)\n","    self.pad_token_id = kwargs.pop(\"pad_token_id\", None)\n","    self.eos_token_id = kwargs.pop(\"eos_token_id\", None)\n","    self.sep_token_id = kwargs.pop(\"sep_token_id\", None)\n","\n","    self.decoder_start_token_id = kwargs.pop(\"decoder_start_token_id\", None)\n","\n","    # task specific arguments\n","    self.task_specific_params = kwargs.pop(\"task_specific_params\", None)\n","\n","    # TPU arguments\n","    self.xla_device = kwargs.pop(\"xla_device\", None)\n","\n","    # Name or path to the pretrained checkpoint\n","    self._name_or_path = str(kwargs.pop(\"name_or_path\", \"\"))\n","\n","    # Drop the transformers version info\n","    kwargs.pop(\"transformers_version\", None)\n","\n","    # Additional attributes without default values\n","    for key, value in kwargs.items():\n","      try:\n","        setattr(self, key, value)\n","      except AttributeError as err:\n","        raise err\n","\n","  @classmethod\n","  def from_pretrained(cls, pretrained_model_name_or_path: Union[str, os.PathLike], **kwargs) -> \"PretrainedConfig\":\n","    config_dict, kwargs = cls.get_config_dict(pretrained_model_name_or_path, **kwargs)\n","    return cls.from_dict(config_dict, **kwargs)\n","\n","  @classmethod\n","  def _dict_from_json_file(cls, json_file: Union[str, os.PathLike]):\n","    with open(json_file, \"r\", encoding=\"utf-8\") as reader:\n","      text = reader.read()\n","    return json.loads(text)\n","\n","  @classmethod\n","  def from_dict(cls, config_dict: Dict[str, Any], **kwargs) -> \"PretrainedConfig\":\n","    return_unused_kwargs = kwargs.pop(\"return_unused_kwargs\", False)\n","\n","    config = cls(**config_dict)\n","\n","    if hasattr(config, \"pruned_heads\"):\n","      config.pruned_heads = dict((int(key), value) for key, value in config.pruned_heads.items())\n","\n","    # Update config with kwargs if needed\n","    to_remove = []\n","    for key, value in kwargs.items():\n","      if hasattr(config, key):\n","        setattr(config, key, value)\n","        to_remove.append(key)\n","    for key in to_remove:\n","      kwargs.pop(key, None)\n","\n","    if return_unused_kwargs:\n","      return config, kwargs\n","    else:\n","      return config\n","\n","  @classmethod\n","  def get_config_dict(\n","    cls, pretrained_model_name_or_path: Union[str, os.PathLike], **kwargs\n","  ) -> Tuple[Dict[str, Any], Dict[str, Any]]:\n","    cache_dir = kwargs.pop(\"cache_dir\", None)\n","    force_download = kwargs.pop(\"force_download\", False)\n","    resume_download = kwargs.pop(\"resume_download\", False)\n","    proxies = kwargs.pop(\"proxies\", None)\n","    use_auth_token = kwargs.pop(\"use_auth_token\", None)\n","    local_files_only = kwargs.pop(\"local_files_only\", False)\n","    revision = kwargs.pop(\"revision\", None)\n","\n","    pretrained_model_name_or_path = str(pretrained_model_name_or_path)\n","    if os.path.isdir(pretrained_model_name_or_path):\n","      config_file = os.path.join(pretrained_model_name_or_path, CONFIG_NAME)\n","    elif os.path.isfile(pretrained_model_name_or_path) or is_remote_url(pretrained_model_name_or_path):\n","      config_file = pretrained_model_name_or_path\n","    else:\n","      config_file = hf_bucket_url(\n","        pretrained_model_name_or_path, filename=CONFIG_NAME, revision=revision, mirror=None\n","      )\n","\n","    try:\n","      # Load from URL or cache if already cached\n","      resolved_config_file = cached_path(\n","        config_file,\n","        cache_dir=cache_dir,\n","        force_download=force_download,\n","        proxies=proxies,\n","        resume_download=resume_download,\n","        local_files_only=local_files_only,\n","        use_auth_token=use_auth_token,\n","      )\n","      # Load config dict\n","      config_dict = cls._dict_from_json_file(resolved_config_file)\n","\n","    except EnvironmentError as err:\n","      msg = (\n","        f\"Can't load config for '{pretrained_model_name_or_path}'. Make sure that:\\n\\n\"\n","        f\"- '{pretrained_model_name_or_path}' is a correct model identifier listed on 'https://huggingface.co/models'\\n\\n\"\n","        f\"- or '{pretrained_model_name_or_path}' is the correct path to a directory containing a {CONFIG_NAME} file\\n\\n\"\n","      )\n","      raise EnvironmentError(msg)\n","\n","    except json.JSONDecodeError:\n","      msg = (\n","        \"Couldn't reach server at '{}' to download configuration file or \"\n","        \"configuration file is not a valid JSON file. \"\n","        \"Please check network or file content here: {}.\".format(config_file, resolved_config_file)\n","      )\n","      raise EnvironmentError(msg)\n","\n","    return config_dict, kwargs\n","\n","class LlamaConfig(PretrainedConfig):\n","  model_type = \"llama\"\n","  def __init__(\n","    self,\n","    vocab_size: int = 32000,\n","    dim: int = 512,\n","    dropout: int = 0.0,\n","    n_layers: int = 8,\n","    n_heads: int = 8,\n","    n_kv_heads: Optional[int] = 8,\n","    max_seq_len: int = 1024,\n","    layer_norm_eps: float = 1e-5,\n","    multiple_of: int = 32,\n","    hidden_dim: Optional[int] = None,\n","    position_embedding_type: str = \"rotary\",\n","    use_cache: bool = True,\n","    **kwargs\n","  ):\n","    super().__init__(**kwargs)\n","\n","    self.vocab_size = vocab_size\n","    self.dim = dim\n","    self.dropout = dropout\n","    self.n_layers = n_layers\n","    self.n_heads = n_heads\n","    self.max_seq_len = max_seq_len\n","    self.n_kv_heads = n_kv_heads\n","    self.layer_norm_eps = layer_norm_eps\n","    self.multiple_of = multiple_of\n","    self.hidden_dim = hidden_dim\n","    self.position_embedding_type = position_embedding_type\n","    self.use_cache = use_cache"]},{"cell_type":"markdown","metadata":{"id":"jwGVbFKuy0jh"},"source":["# base_llama.py"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":617,"status":"ok","timestamp":1732188240098,"user":{"displayName":"Chaoran Zhou","userId":"09162553986537566448"},"user_tz":-480},"id":"zZxYjqphy0bG"},"outputs":[],"source":["from dataclasses import dataclass\n","\n","import re\n","from torch import dtype\n","# from config import LlamaConfig\n","# from utils import *\n","\n","class LlamaPreTrainedModel(nn.Module):\n","  config_class = LlamaConfig\n","  base_model_prefix = \"llama\"\n","\n","  def __init__(self, config: LlamaConfig):\n","      super().__init__()\n","      self.config = config\n","      self.vocab_size = config.vocab_size\n","      self.n_layers = config.n_layers\n","\n","  def init_weights(self):\n","    # Initialize weights\n","    self.apply(self._init_weights)\n","\n","  def _init_weights(self, module):\n","    \"\"\" Initialize the weights \"\"\"\n","    if isinstance(module, nn.Linear):\n","        torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n","        if module.bias is not None:\n","            torch.nn.init.zeros_(module.bias)\n","    elif isinstance(module, nn.Embedding):\n","        torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n","\n","  @property\n","  def dtype(self) -> dtype:\n","    return get_parameter_dtype(self)"]},{"cell_type":"markdown","metadata":{"id":"MtlBGrtGI1hJ"},"source":["# rope.py"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":668,"status":"ok","timestamp":1732188244025,"user":{"displayName":"Chaoran Zhou","userId":"09162553986537566448"},"user_tz":-480},"id":"UzfkXeryI1KD"},"outputs":[],"source":["from typing import Tuple\n","import torch\n","\n","def reshape_for_broadcast(freqs_cis: torch.Tensor, x: torch.Tensor):\n","    \"\"\"\n","    Helper function to reshape frequency tensor to have the same shape as the target tensor 'x'\n","    for the purpose of broadcasting the frequency tensor during element-wise operations.\n","\n","    Args:\n","        freqs_cis (torch.Tensor): Frequency tensor to be reshaped.\n","        x (torch.Tensor): Target tensor for broadcasting compatibility.\n","\n","    Returns:\n","        torch.Tensor: Reshaped frequency tensor.\n","\n","    Raises:\n","        AssertionError: If the frequency tensor doesn't match the expected shape.\n","        AssertionError: If the target tensor 'x' doesn't have the expected number of dimensions.\n","    \"\"\"\n","    ndim = x.ndim\n","    assert 0 <= 1 < ndim\n","    assert freqs_cis.shape == (x.shape[1], x.shape[-1])\n","    shape = [d if i == 1 or i == ndim - 1 else 1 for i, d in enumerate(x.shape)]\n","    return freqs_cis.view(shape)\n","\n","def apply_rotary_emb(\n","    query: torch.Tensor,\n","    key: torch.Tensor,\n","    head_dim: int,\n","    max_seq_len: int,\n","    theta: float = 10000.0,\n",") -> Tuple[torch.Tensor, torch.Tensor]:\n","    \"\"\"\n","    Apply rotary embeddings to input tensors using the given frequency tensor.\n","\n","    This function applies rotary embeddings to the given query and key tensors. The rotation to each token\n","    embedding is a function of that token's position in the sequence, head_dim, and theta.\n","    The input tensors are reshaped as complex numbers to simplify your implementation.\n","\n","    Args:\n","        query (torch.Tensor): Query tensor to apply rotary embeddings.\n","                              Shape: (batch_size, seqlen, n_local_heads, self.head_dim)\n","        key (torch.Tensor): Key tensor to apply rotary embeddings.\n","                              Shape: (batch_size, seqlen, n_local_kv_heads, self.head_dim)\n","        head_dim (int): Dimension of each attention head.\n","        max_seq_len (int): Maximum sequence length supported by model.\n","    Returns:\n","        Tuple[torch.Tensor, torch.Tensor]: Tuple of modified query tensor and key tensor with rotary embeddings.\n","    \"\"\"\n","\n","    _, seqlen, _, _ = query.shape\n","    device = query.device\n","    # todo\n","    #\n","    # Please refer to slide 22 in https://phontron.com/class/anlp2024/assets/slides/anlp-05-transformers.pdf\n","    # and Section 3 in https://arxiv.org/abs/2104.09864.\n","\n","    # reshape xq and xk to match the complex representation\n","    query_real, query_imag = query.float().reshape(query.shape[:-1] + (-1, 2)).unbind(-1)\n","    key_real, key_imag = key.float().reshape(key.shape[:-1] + (-1, 2)).unbind(-1)\n","    # This separates each query/key vector into its odd and even indices (assuming *one-indexing*).\n","    # query_real contains q_1, q_3, q_5, ... and query_imag contains q_2, q_4, q_6, ...\n","\n","    # First, compute the trigonometric values in the second and fourth columns in\n","    # slide 22 (linked above).\n","    theta = 1.0 / torch.pow(theta, torch.arange(0, head_dim, 2).float() / head_dim)\n","    mtheta = torch.outer(torch.arange(query.shape[1]).float(), theta).to(device)\n","    cos = reshape_for_broadcast(torch.cos(mtheta), query_real)\n","    sin = reshape_for_broadcast(torch.sin(mtheta), query_real)\n","\n","    # Then, combine these trigonometric values with the tensors query_real, query_imag,\n","    # key_real, and key_imag.\n","\n","    # raise NotImplementedError\n","\n","    # query_out = None\n","    # key_out = None\n","    query_odd = query_real * cos - query_imag * sin\n","    query_even = query_real * sin + query_imag * cos\n","    key_odd = key_real * cos - key_imag * sin\n","    key_even = key_real * sin + key_imag * cos\n","\n","    query_out = torch.stack((query_odd, query_even), dim = -1).reshape(query.shape)\n","    key_out = torch.stack((key_odd, key_even), dim = -1).reshape(key.shape)\n","    # Return the rotary position embeddings for the query and key tensors\n","    return query_out, key_out"]},{"cell_type":"markdown","metadata":{"id":"rMBSsuN7y0UT"},"source":["# llama.py"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":671,"status":"ok","timestamp":1732188249466,"user":{"displayName":"Chaoran Zhou","userId":"09162553986537566448"},"user_tz":-480},"id":"COIniJmty0Ny"},"outputs":[],"source":["from contextlib import nullcontext\n","from typing import Optional, Tuple\n","import math\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","#from base_llama import LlamaPreTrainedModel, LlamaConfig\n","#from rope import apply_rotary_emb\n","#from utils import *\n","\n","# Root Mean Square Layer Normalization (https://arxiv.org/abs/1910.07467)\n","# borrowed from the official Llama implementation:\n","# https://github.com/facebookresearch/llama/blob/main/llama/model.py\n","class RMSNorm(torch.nn.Module):\n","    def __init__(self, dim: int, eps: float = 1e-6):\n","        \"\"\"\n","        Initialize the RMSNorm normalization layer.\n","\n","        Args:\n","            dim (int): The dimension of the input tensor.\n","            eps (float, optional): A small value added to the denominator for numerical stability. Default is 1e-6.\n","\n","        Attributes:\n","            eps (float): A small value added to the denominator for numerical stability.\n","            weight (nn.Parameter): Learnable scaling parameter.\n","\n","        \"\"\"\n","        super().__init__()\n","        self.eps = eps\n","        self.weight = nn.Parameter(torch.ones(dim))\n","\n","    def _norm(self, x):\n","        \"\"\"\n","        Compute the root mean square normalization. Use Equation 4 under\n","        Section 4 of https://arxiv.org/abs/1910.07467 as a reference. Add\n","        the given epsilon value (self.eps) to the tensor's norm (i.e. inside\n","        the square root in Equation 4) before normalizing the tensor.\n","\n","        Args:\n","            x (torch.Tensor): The input tensor.\n","\n","        Returns:\n","            torch.Tensor: The normalized tensor.\n","        \"\"\"\n","        # todo\n","        # raise NotImplementedError\n","        rms = torch.sqrt(torch.mean(x**2, dim = -1, keepdim = True) + self.eps)\n","        return x / rms\n","\n","    def forward(self, x):\n","        \"\"\"\n","        Apply the root mean square normalizer.\n","\n","        Args:\n","            x (torch.Tensor): The input tensor.\n","\n","        Returns:\n","            torch.Tensor: The output tensor after applying RMSNorm.\n","\n","        \"\"\"\n","        output = self._norm(x.float()).type_as(x)\n","        return output * self.weight\n","\n","class Attention(nn.Module):\n","    def __init__(self, config: LlamaConfig):\n","        super().__init__()\n","        self.n_kv_heads = config.n_heads if config.n_kv_heads is None else config.n_kv_heads\n","        assert config.n_heads % self.n_kv_heads == 0\n","        model_parallel_size = 1\n","        self.n_local_heads = config.n_heads // model_parallel_size\n","        self.n_local_kv_heads = self.n_kv_heads // model_parallel_size\n","        self.n_rep = self.n_local_heads // self.n_local_kv_heads\n","        self.head_dim = config.dim // config.n_heads\n","        self.max_seq_len = config.max_seq_len\n","        self.compute_query = nn.Linear(config.dim, config.n_heads * self.head_dim, bias=False)\n","        self.compute_key = nn.Linear(config.dim, self.n_kv_heads * self.head_dim, bias=False)\n","        self.compute_value = nn.Linear(config.dim, self.n_kv_heads * self.head_dim, bias=False)\n","        self.compute_output = nn.Linear(config.n_heads * self.head_dim, config.dim, bias=False)\n","        self.attn_dropout = nn.Dropout(config.dropout)\n","        self.resid_dropout = nn.Dropout(config.dropout)\n","        self.dropout = config.dropout\n","\n","    def compute_query_key_value_scores(self,\n","                                       query: torch.Tensor,\n","                                       key: torch.Tensor,\n","                                       value: torch.Tensor) -> torch.Tensor:\n","        '''\n","        Jointly compute Scaled Dot Product Attention (see Section 3.2.1 in\n","        https://arxiv.org/abs/1706.03762 for details). The query, key, and\n","        value tensors each have shape (bs, n_local_heads, seqlen, head_dim).\n","        An optimal implemention will jointly computing attention for multiple\n","        heads (n_local_heads of them) at once using matrix/tensor operations.\n","\n","        Make sure to use attention_dropout (self.attn_dropout) on the computed\n","        attention matrix before applying it to the value tensor.\n","        '''\n","        # todo\n","        # raise NotImplementedError\n","        score = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(query.shape[-1])\n","        attention = F.softmax(score, dim = -1)\n","        attention = self.attn_dropout(attention)\n","        output = torch.matmul(attention, value)\n","        return output\n","\n","\n","    def forward(\n","        self,\n","        x: torch.Tensor\n","    ):\n","        '''\n","        Llama2 uses Grouped-Query Attention. The details of GQA are actually\n","        not critical to solving this assignment; you are simply asked to\n","        compute Scaled Dot Product Attention (see above for details). GQA is\n","        a memory optimization to compute multi-head attention efficiently. See\n","        Section 2.2 in https://arxiv.org/abs/2305.13245 or\n","        https://ai.plainenglish.io/understanding-llama2-kv-cache-grouped-query-attention-rotary-embedding-and-more-c17e5f49a6d7\n","        for details.\n","        '''\n","        batch_size, seqlen, _ = x.shape\n","\n","        query = self.compute_query(x)\n","        key = self.compute_key(x)\n","        value = self.compute_value(x)\n","        query = query.view(batch_size, seqlen, self.n_local_heads, self.head_dim)\n","        key = key.view(batch_size, seqlen, self.n_local_kv_heads, self.head_dim)\n","        value = value.view(batch_size, seqlen, self.n_local_kv_heads, self.head_dim)\n","\n","        # RoPE relative positional embeddings\n","        query, key = apply_rotary_emb(query, key, self.head_dim, self.max_seq_len)\n","\n","        # Grouped multiquery attention: expand out keys and values.\n","        # Convert both to:\n","        # (bs, seqlen, n_local_heads, head_dim)\n","        key = torch.repeat_interleave(key, dim=2, repeats=self.n_rep)\n","        value = torch.repeat_interleave(value, dim=2, repeats=self.n_rep)\n","\n","        # make heads into a batch dimension\n","        query = query.transpose(1, 2)  # (bs, n_local_heads, seqlen, head_dim)\n","        key = key.transpose(1, 2)\n","        value = value.transpose(1, 2)\n","        output = self.compute_query_key_value_scores(query, key, value)\n","\n","        # restore time as batch dimension and concat heads\n","        output = output.transpose(1, 2).contiguous().view(batch_size, seqlen, -1)\n","\n","        # final projection into the residual stream\n","        output = self.resid_dropout(self.compute_output(output))\n","        return output\n","\n","\n","class FeedForward(nn.Module):\n","    def __init__(self, dim: int, hidden_dim: int, multiple_of: int, dropout: float):\n","        super().__init__()\n","        if hidden_dim is None:\n","            hidden_dim = 4 * dim\n","            hidden_dim = int(2 * hidden_dim / 3)\n","            hidden_dim = multiple_of * ((hidden_dim + multiple_of - 1) // multiple_of)\n","        self.w1 = nn.Linear(dim, hidden_dim, bias=False)\n","        self.w2 = nn.Linear(hidden_dim, dim, bias=False)\n","        self.w3 = nn.Linear(dim, hidden_dim, bias=False)\n","        self.dropout = nn.Dropout(dropout)\n","\n","    def SwiGLU(self, x: torch.Tensor) -> torch.Tensor:\n","        '''\n","        Compute the SwiGLU activation function (see Section 2 in\n","        https://arxiv.org/abs/2204.02311\n","        '''\n","        return F.silu(self.w1(x)) * self.w3(x)\n","\n","    def forward(self, x):\n","        return self.dropout(self.w2(self.SwiGLU(x)))\n","\n","\n","class LlamaLayer(nn.Module):\n","    def __init__(self, layer_id: int, config: LlamaConfig):\n","        super().__init__()\n","        self.n_heads = config.n_heads\n","        self.dim = config.dim\n","        self.head_dim = config.dim // config.n_heads\n","        self.attention = Attention(config)\n","        self.feed_forward = FeedForward(\n","            dim=config.dim,\n","            hidden_dim=config.hidden_dim,\n","            multiple_of=config.multiple_of,\n","            dropout=config.dropout,\n","        )\n","        self.layer_id = layer_id\n","        self.attention_norm = RMSNorm(config.dim, eps=config.layer_norm_eps)\n","        self.ffn_norm = RMSNorm(config.dim, eps=config.layer_norm_eps)\n","\n","    def forward(self, x):\n","        '''\n","        This is the forward pass of the basic transformer building block. This is a\n","        modernized version of the block shown on the left of Figure 1 on\n","        https://arxiv.org/pdf/1706.03762.pdf.\n","\n","        The transformer block should consist of:\n","        1) layer normalization of the input (via Root Mean Square layer normalization)\n","        2) self-attention on the layer-normalized input\n","        3) a residual connection (i.e., add the input to the output of the self-attention)\n","        3) layer normalization on the output of the self-attention\n","        4) a feed-forward network on the layer-normalized output of the self-attention\n","        5) add a residual connection from the unnormalized self-attention output to the\n","           output of the feed-forward network\n","        '''\n","        # todo\n","        # raise NotImplementedError\n","        # layer norm\n","        attention_norm = self.attention_norm(x)\n","        # self-attention\n","        attention = x + self.attention(attention_norm)\n","        # layer norm\n","        ffn_norm = self.ffn_norm(attention)\n","        # feed-forward network\n","        y = attention + self.feed_forward(ffn_norm)\n","        return y\n","\n","class Llama(LlamaPreTrainedModel):\n","    def __init__(self, config: LlamaConfig):\n","        '''\n","        You will probably never need to call this function, unless you decide\n","        to pretrain a Llama model from scratch.\n","        '''\n","        super().__init__(config)\n","        self.params = config\n","        self.vocab_size = config.vocab_size\n","        self.n_layers = config.n_layers\n","\n","        self.tok_embeddings = nn.Embedding(config.vocab_size, config.dim)\n","        self.dropout = nn.Dropout(config.dropout)\n","        self.layers = torch.nn.ModuleList()\n","        for layer_id in range(config.n_layers):\n","            self.layers.append(LlamaLayer(layer_id, config))\n","        self.norm = RMSNorm(config.dim, eps=config.layer_norm_eps)\n","        self.output = nn.Linear(config.dim, config.vocab_size, bias=False)\n","\n","        # share the unembedding parameters with the embedding parameters\n","        self.tok_embeddings.weight = self.output.weight # https://paperswithcode.com/method/weight-tying\n","\n","        # some useful precompute for the RoPE relative positional embeddings\n","\n","        # init all weights\n","        self.apply(self._init_weights)\n","        # apply special scaled init to the residual projections, per GPT-2 paper\n","        for pn, p in self.named_parameters():\n","            if pn.endswith('w3.weight') or pn.endswith('compute_output.weight'):\n","                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layers))\n","\n","    def _init_weights(self, module):\n","        if isinstance(module, nn.Linear):\n","            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n","            if module.bias is not None:\n","                torch.nn.init.zeros_(module.bias)\n","        elif isinstance(module, nn.Embedding):\n","            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n","\n","    def forward(self, tokens: torch.Tensor, targets: Optional[torch.Tensor] = None) -> torch.Tensor:\n","        _batch_size, seqlen = tokens.shape\n","        h = self.tok_embeddings(tokens)\n","        h = self.dropout(h)\n","\n","        for layer in self.layers:\n","            h = layer(h)\n","        h = self.norm(h)\n","\n","        if targets is not None:\n","            # if we are given some desired targets also calculate the loss\n","            logits = self.output(h)\n","        else:\n","            # inference-time mini-optimization: only forward the output on the very last position\n","            logits = self.output(h[:, [-1], :]) # note: using list [-1] to preserve the time dim\n","\n","        return logits, h\n","\n","    @torch.inference_mode()\n","    def generate(self, idx, max_new_tokens, temperature=1.0):\n","        \"\"\"\n","        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n","        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n","        We perform this generation using basic temperature sampling. Note that we are not using\n","        nucleus sampling (i.e. limiting ourselves to sampling from the top-k most probable tokens\n","        at each timestep), though this is often used in conjunction with temperature sampling,\n","        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n","        Also note this is a super inefficient version of sampling with no key/value cache.\n","        \"\"\"\n","        for _ in range(max_new_tokens):\n","            # if the sequence context is growing too long we must crop it at block_size\n","            idx_cond = idx if idx.size(1) <= self.params.max_seq_len else idx[:, -self.params.max_seq_len:]\n","            # forward the model to get the logits for the index in the sequence\n","            logits, _ = self(idx_cond)\n","            logits = logits[:, -1, :] # crop to just the final time step\n","            # todo\n","            # raise NotImplementedError\n","\n","            if temperature == 0.0:\n","                # select the single most likely index\n","                # idx_next = None\n","                idx_next = torch.argmax(logits, dim = -1, keepdim = True)\n","            else:\n","                '''\n","                Perform temperature sampling:\n","                1) identify  the logits at the final step.\n","                2) scale (divide) these probabilities by the given temperature.\n","                3) normalize the scaled logits with a softmax to obtain scaled probabilities.\n","                4) sample from the scaled probability distribution.\n","\n","                Note that we are not using top-k sampling/nucleus sampling in this procedure.\n","                '''\n","                # idx_next = None\n","                scaled_logits = logits / temperature\n","                probs = F.softmax(scaled_logits, dim = -1)\n","                idx_next = torch.multinomial(probs, num_samples = 1)\n","            # append sampled index to the running sequence and continue\n","            idx = torch.cat((idx, idx_next), dim=1)\n","\n","\n","        return idx\n","\n","def load_pretrained(checkpoint):\n","  device = 'cuda' if torch.cuda.is_available() else 'cpu' # examples: 'cpu', 'cuda', 'cuda:0', 'cuda:1', etc.\n","  #dtype = 'bfloat16' if torch.cuda.is_available() and torch.cuda.is_bf16_supported() else 'float16' # 'float32' or 'bfloat16' or 'float16'\n","  dtype = \"float32\"\n","\n","  torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n","  torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n","  device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n","  ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n","  ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n","\n","  # init from a model saved in a specific directory\n","  checkpoint_dict = torch.load(checkpoint, map_location=device)\n","  config = LlamaConfig(**checkpoint_dict['model_args'])\n","  model = Llama(config)\n","  state_dict = checkpoint_dict['model']\n","  unwanted_prefix = '_orig_mod.'\n","  for k,v in list(state_dict.items()):\n","      if k.startswith(unwanted_prefix):\n","          state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n","  model.load_state_dict(state_dict, strict=False)\n","  return model\n"]},{"cell_type":"markdown","metadata":{"id":"qhoQzQ3gKImK"},"source":["# tokenizer.py"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":617,"status":"ok","timestamp":1732188279599,"user":{"displayName":"Chaoran Zhou","userId":"09162553986537566448"},"user_tz":-480},"id":"eBdnTXC9KIFY"},"outputs":[],"source":["# Taken from llama code and modified by Andrej Karpathy.\n","# Copyright (c) Meta Platforms, Inc. and affiliates.\n","# This software may be used and distributed according to the terms of the Llama 2 Community License Agreement.\n","\n","import os\n","import struct\n","import argparse\n","from typing import List\n","\n","from sentencepiece import SentencePieceProcessor\n","\n","TOKENIZER_MODEL = \"tokenizer.model\" # the llama sentencepiece tokenizer model\n","\n","class Tokenizer:\n","    def __init__(self, max_len=None, tokenizer_model=None):\n","        model_path = tokenizer_model if tokenizer_model else TOKENIZER_MODEL\n","        assert os.path.isfile(model_path), model_path\n","        self.sp_model = SentencePieceProcessor(model_file=model_path)\n","        self.model_path = model_path\n","        self.max_len = max_len\n","\n","        # BOS / EOS token IDs\n","        self.n_words: int = self.sp_model.vocab_size()\n","        self.bos_id: int = self.sp_model.bos_id()\n","        self.eos_id: int = self.sp_model.eos_id()\n","        # Overwrite the default of pad_id=-1, which is problematic.\n","        self.pad_id: int = self.sp_model.piece_to_id(\"<0x00>\")\n","        #print(f\"#words: {self.n_words} - BOS ID: {self.bos_id} - EOS ID: {self.eos_id}\")\n","        assert self.sp_model.vocab_size() == self.sp_model.get_piece_size()\n","\n","    def encode(self, s: str, bos: bool, eos: bool) -> List[int]:\n","        assert type(s) is str\n","        t = self.sp_model.encode(s)\n","        if self.max_len is not None and len(t) > self.max_len:\n","            t = t[:self.max_len]\n","        if bos:\n","            t = [self.bos_id] + t\n","        if eos:\n","            t = t + [self.eos_id]\n","        return t\n","\n","    def decode(self, t: List[int]) -> str:\n","        return self.sp_model.decode(t)\n","\n","    def export(self):\n","\n","        # get all the tokens (postprocessed) and their scores as floats\n","        tokens, scores = [], []\n","        for i in range(self.n_words):\n","\n","            # decode the token and light postprocessing\n","            t = self.sp_model.id_to_piece(i)\n","            s = self.sp_model.get_score(i)\n","            if i == self.bos_id:\n","                t = '\\n<s>\\n'\n","            elif i == self.eos_id:\n","                t = '\\n</s>\\n'\n","            t = t.replace('‚ñÅ', ' ') # sentencepiece uses this character as whitespace\n","            b = t.encode('utf-8') # bytes of this token, utf-8 encoded\n","\n","            tokens.append(b)\n","            scores.append(s)\n","\n","        # record the max token length\n","        max_token_length = max(len(t) for t in tokens)\n","\n","        # write to a binary file\n","        # the tokenizer.bin file is the same as .model file, but .bin\n","        tokenizer_bin = self.model_path.replace('.model', '.bin')\n","        with open(tokenizer_bin, 'wb') as f:\n","            f.write(struct.pack(\"I\", max_token_length))\n","            for bytes, score in zip(tokens, scores):\n","                f.write(struct.pack(\"fI\", score, len(bytes)))\n","                f.write(bytes)\n","\n","#if __name__ == \"__main__\":\n","#    parser = argparse.ArgumentParser()\n","#    parser.add_argument(\"-t\", \"--tokenizer-model\", type=str, help=\"optional path to custom tokenizer \")\n","#    args = parser.parse_args()\n","\n","#    t = Tokenizer(args.tokenizer_model)\n","#    t.export()\n"]},{"cell_type":"markdown","metadata":{"id":"O8U7xjV9y0Gp"},"source":["# classifier.py"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":627,"status":"ok","timestamp":1732188284382,"user":{"displayName":"Chaoran Zhou","userId":"09162553986537566448"},"user_tz":-480},"id":"F2smeWubJzo6"},"outputs":[],"source":["import torch\n","import torch.nn.functional as F\n","\n","# change it with respect to the original model\n","# from config import LlamaConfig\n","# from llama import load_pretrained\n","# from tokenizer import Tokenizer\n","\n","class LlamaZeroShotClassifier(torch.nn.Module):\n","\tdef __init__(self, config: LlamaConfig, tokenizer: Tokenizer, label_names: list[str]):\n","\t\tsuper(LlamaZeroShotClassifier, self).__init__()\n","\t\tself.num_labels = config.num_labels\n","\t\tself.llama = load_pretrained(config.pretrained_model_path)\n","\t\t# Zero-shot classification does not require updating llama paramters.\n","\t\tfor param in self.llama.parameters():\n","\t\t\tparam.requires_grad = False\n","\t\tassert len(label_names) == self.num_labels\n","\t\tself.tokenizer = tokenizer\n","\t\tself.label_name_ids = [tokenizer.encode(label, bos=False, eos=False) for label in label_names]\n","\n","\n","\tdef forward(self, input_ids):\n","\t\t# compute the completion probability of each label string\n","\t\tlogits, _ = self.llama(input_ids)\n","\t\tlog_probabilities = F.log_softmax(logits, dim=-1)\n","\t\tlabel_probabilities = torch.zeros((log_probabilities.shape[0], self.num_labels), device=log_probabilities.device)\n","\t\tfor i, label_token_ids in enumerate(self.label_name_ids):\n","\t\t\ttotal_log_prob = torch.sum(log_probabilities[:, :, label_token_ids], axis=-1)\n","\t\t\tlabel_probabilities[:, i] = total_log_prob[:, 0]\n","\t\treturn label_probabilities\n","\n","class LlamaEmbeddingClassifier(torch.nn.Module):\n","\tdef __init__(self, config):\n","\t\tsuper(LlamaEmbeddingClassifier, self).__init__()\n","\t\tself.num_labels = config.num_labels\n","\t\tself.llama = load_pretrained(config.pretrained_model_path)\n","\t\t# If we use pretrain mode, we freeze Llama parameters.\n","\t\tfor param in self.llama.parameters():\n","\t\t\tif config.option == 'pretrain':\n","\t\t\t\tparam.requires_grad = False\n","\t\t\telif config.option == 'finetune':\n","\t\t\t\tparam.requires_grad = True\n","\n","\t\tself.dropout = torch.nn.Dropout(config.hidden_dropout_prob)\n","\t\tself.classifier_head = torch.nn.Linear(self.llama.config.dim, self.num_labels)\n","\n","\tdef forward(self, input_ids):\n","\t\t'''\n","\t\t1) Find the hidden state after the final token of the input sequence\n","\t\t2) Apply dropout (self.dropout) to the hidden state at training time to mitigate\n","\t\t   overfitting.\n","\t\t2) Pass this through the classifier head (self.classifier_head), which will return\n","\t\t   logits (unnormalized probabilities) over all classes.\n","\t\t3) Take the log-softmax of the logits and return log-probabilities over all classes.\n","\t\t'''\n","\t\t# todo\n","\t\t# raise NotImplementedError\n","\t\t_, hidden_states = self.llama(input_ids)\n","\t\thidden_state = self.dropout(hidden_states[:, -1, :])\n","\t\tlogits = self.classifier_head(hidden_state)\n","\t\treturn F.log_softmax(logits, dim = -1)\n"]},{"cell_type":"markdown","metadata":{"id":"MAExnRZ7J1Df"},"source":["# optimizer.py"]},{"cell_type":"code","execution_count":8,"metadata":{"executionInfo":{"elapsed":632,"status":"ok","timestamp":1732188289910,"user":{"displayName":"Chaoran Zhou","userId":"09162553986537566448"},"user_tz":-480},"id":"AvCHSAdny0An"},"outputs":[],"source":["from typing import Callable, Iterable, Tuple\n","\n","import torch\n","from torch.optim import Optimizer\n","\n","\n","class AdamW(Optimizer):\n","    def __init__(\n","            self,\n","            params: Iterable[torch.nn.parameter.Parameter],\n","            lr: float = 1e-3,\n","            betas: Tuple[float, float] = (0.9, 0.999),\n","            eps: float = 1e-6,\n","            weight_decay: float = 0.0,\n","            correct_bias: bool = True,\n","    ):\n","        if lr < 0.0:\n","            raise ValueError(\"Invalid learning rate: {} - should be >= 0.0\".format(lr))\n","        if not 0.0 <= betas[0] < 1.0:\n","            raise ValueError(\"Invalid beta parameter: {} - should be in [0.0, 1.0[\".format(betas[0]))\n","        if not 0.0 <= betas[1] < 1.0:\n","            raise ValueError(\"Invalid beta parameter: {} - should be in [0.0, 1.0[\".format(betas[1]))\n","        if not 0.0 <= eps:\n","            raise ValueError(\"Invalid epsilon value: {} - should be >= 0.0\".format(eps))\n","        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay, correct_bias=correct_bias)\n","        super().__init__(params, defaults)\n","\n","    def step(self, closure: Callable = None):\n","        loss = None\n","        if closure is not None:\n","            loss = closure()\n","\n","        for group in self.param_groups:\n","            for p in group[\"params\"]:\n","                if p.grad is None:\n","                    continue\n","                grad = p.grad.data\n","                if grad.is_sparse:\n","                    raise RuntimeError(\"Adam does not support sparse gradients, please consider SparseAdam instead\")\n","\n","                # raise NotImplementedError()\n","\n","                # State should be stored in this dictionary\n","                state = self.state[p]\n","\n","                # Access hyperparameters from the `group` dictionary\n","                # alpha = group[\"lr\"]\n","\n","                # Update first and second moments of the gradients\n","                t = state.get(\"t\", 0) + 1\n","                m = (\n","                    group[\"betas\"][0] * state.get(\"m\", torch.zeros_like(p))\n","                    + (1 - group[\"betas\"][0]) * grad\n","                )\n","                v = (\n","                    group[\"betas\"][1] * state.get(\"v\", torch.zeros_like(p))\n","                    + (1 - group[\"betas\"][1]) * grad**2\n","                )\n","                state[\"t\"] = t\n","                state[\"m\"] = m\n","                state[\"v\"] = v\n","\n","                # Bias correction\n","                # Please note that we are using the \"efficient version\" given in\n","                # https://arxiv.org/abs/1412.6980\n","                if group[\"correct_bias\"]:\n","                    alpha = (\n","                        group[\"lr\"]\n","                        * (1 - group[\"betas\"][1] ** t) ** 0.5\n","                        / (1 - group[\"betas\"][0] ** t)\n","                    )\n","                else:\n","                    alpha = group[\"lr\"]\n","\n","                # Update parameters\n","                p.data -= alpha * m / (v**0.5 + group[\"eps\"])\n","\n","                # Add weight decay after the main gradient-based updates.\n","                # Please note that the learning rate should be incorporated into this update.\n","                p.data -= group[\"lr\"] * group[\"weight_decay\"] * p.data\n","\n","        return loss"]},{"cell_type":"markdown","metadata":{"id":"10V5_InSyz6Z"},"source":["# run_llama.py"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":34356,"status":"ok","timestamp":1732188366012,"user":{"displayName":"Chaoran Zhou","userId":"09162553986537566448"},"user_tz":-480},"id":"BFv78yBPyzzt","outputId":"4b67bf8a-fc69-4a87-97ff-c1531eaf1c98"},"outputs":[{"name":"stdout","output_type":"stream","text":["args: {'train': 'data/cfimdb-train.txt', 'dev': 'data/cfimdb-dev.txt', 'test': 'data/cfimdb-test.txt', 'label_names': 'data/cfimdb-label-mapping.json', 'pretrained_model_path': 'stories42M.pt', 'max_sentence_len': None, 'seed': 1337, 'epochs': 5, 'option': 'generate', 'use_gpu': False, 'generated_sentence_low_temp_out': 'generated-sentence-temp-0.txt', 'generated_sentence_high_temp_out': 'generated-sentence-temp-1.txt', 'dev_out': 'cfimdb-dev-prompting-output.txt', 'test_out': 'cfimdb-test-prompting-output.txt', 'batch_size': 8, 'hidden_dropout_prob': 0.3, 'lr': 2e-05}\n"]},{"name":"stderr","output_type":"stream","text":["<ipython-input-5-6906fe08d67a>:332: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  checkpoint_dict = torch.load(checkpoint, map_location=device)\n"]},{"name":"stdout","output_type":"stream","text":["load model from stories42M.pt\n","Temperature is 0.0\n","I have wanted to see this thriller for a while, and it didn't disappoint. Keanu Reeves, playing the hero John Wick, is this day. He was playing with his toy car, driving it around the living room. Suddenly, he heard a loud crash. He had broken the car and was very sad.\n","John was angry and he shouted at his little brother. He was only three years old and he was only three. He was only three years old. He was very ups\n","---------------\n","Wrote generated sentence to generated-sentence-temp-0.txt.\n","load model from stories42M.pt\n","Temperature is 1.0\n","I have wanted to see this thriller for a while, and it didn't disappoint. Keanu Reeves, playing the hero John Wick, is it!\" As John toddled up to the sweet aroma, he held his mum's hand tight, hoping he would finally reach the top step.\n","But as he stepped, he felt so heavy, like an apple. He had made a mistake! His mum apologisedDen handlely leaving, so he started to cry.\n","His m\n","---------------\n","Wrote generated sentence to generated-sentence-temp-1.txt.\n"]}],"source":["from contextlib import nullcontext\n","import json\n","import time, random, numpy as np, argparse, sys, re, os\n","from types import SimpleNamespace\n","\n","import torch\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.metrics import classification_report, f1_score, recall_score, accuracy_score\n","\n","# change it with respect to the original model\n","# from classifier import LlamaZeroShotClassifier, LlamaEmbeddingClassifier\n","# from llama import Llama, load_pretrained\n","# from optimizer import AdamW\n","# from tokenizer import Tokenizer\n","from tqdm import tqdm\n","from typing import Optional\n","\n","\n","TQDM_DISABLE=False\n","# fix the random seed\n","def seed_everything(seed=11711):\n","\trandom.seed(seed)\n","\tnp.random.seed(seed)\n","\ttorch.manual_seed(seed)\n","\ttorch.cuda.manual_seed(seed)\n","\ttorch.cuda.manual_seed_all(seed)\n","\ttorch.backends.cudnn.benchmark = False\n","\ttorch.backends.cudnn.deterministic = True\n","\n","# create a custom Dataset Class to be used for the dataloader\n","class LlamaDataset(Dataset):\n","\tdef __init__(self, dataset, args, eos=False):\n","\t\tself.dataset = dataset\n","\t\tself.p = args\n","\t\tself.tokenizer = Tokenizer(max_len=args.max_sentence_len)\n","\t\tself.eos = eos\n","\n","\tdef __len__(self):\n","\t\treturn len(self.dataset)\n","\n","\tdef __getitem__(self, idx):\n","\t\tele = self.dataset[idx]\n","\t\treturn ele\n","\n","\tdef pad_data(self, data):\n","\t\tsents = [x[0] for x in data]\n","\t\tlabels = [x[1] for x in data]\n","\t\tencoding = [self.tokenizer.encode(s, bos=True, eos=self.eos) for s in sents]\n","\t\tmax_length_in_batch = max([len(sentence) for sentence in encoding])\n","\t\tencoding_padded = [sentence + [self.tokenizer.pad_id] * (max_length_in_batch - len(sentence)) for sentence in encoding]\n","\t\ttoken_ids = torch.LongTensor(encoding_padded)\n","\t\tlabels = torch.LongTensor(labels)\n","\n","\t\treturn token_ids, labels, sents\n","\n","\tdef collate_fn(self, all_data):\n","\n","\t\ttoken_ids, labels, sents = self.pad_data(all_data)\n","\t\tbatched_data = {\n","\t\t\t\t'token_ids': token_ids,\n","\t\t\t\t'labels': labels,\n","\t\t\t\t'sents': sents,\n","\t\t\t}\n","\n","\t\treturn batched_data\n","\n","\n","# create the data which is a list of (sentence, label, token for the labels)\n","def create_data(filename, tokenizer: Tokenizer, flag: str ='train', lower: bool = False, eos: bool = True, prompt_suffix: Optional[str]=None):\n","\t# specify the tokenizer\n","\tnum_labels = {}\n","\tdata = []\n","\n","\twith open(filename, \"r\", encoding=\"utf-8\") as fp:\n","\t\tfor line in fp:\n","\t\t\tlabel, org_sent = line.split(' ||| ')\n","\t\t\tif lower:\n","\t\t\t\torg_sent = org_sent.lower()\n","\t\t\tsent = org_sent.strip()\n","\t\t\tif prompt_suffix is not None:\n","\t\t\t\tsent = f\"{sent} {prompt_suffix}\"\n","\t\t\ttokens = tokenizer.encode(sent, bos=True, eos=eos)\n","\t\t\tlabel = int(label.strip())\n","\t\t\tif label not in num_labels:\n","\t\t\t\tnum_labels[label] = len(num_labels)\n","\t\t\tdata.append((sent, label, tokens))\n","\tprint(f\"load {len(data)} data from {filename}\")\n","\tif flag == 'train':\n","\t\treturn data, len(num_labels)\n","\telse:\n","\t\treturn data\n","\n","# perform model evaluation in terms of the accuracy and f1 score.\n","def model_eval(dataloader, model, device):\n","\tmodel.eval() # switch to eval model, will turn off randomness like dropout\n","\ty_true = []\n","\ty_pred = []\n","\tsents = []\n","\tfor step, batch in enumerate(tqdm(dataloader, desc=f'eval', disable=TQDM_DISABLE)):\n","\t\tb_ids, b_labels, b_sents = batch['token_ids'], batch['labels'], batch['sents']\n","\n","\t\tb_ids = b_ids.to(device)\n","\n","\t\tlogits = model(b_ids)\n","\t\tlogits = logits.detach().cpu().numpy()\n","\t\tpreds = np.argmax(logits, axis=1).flatten()\n","\n","\t\tb_labels = b_labels.flatten()\n","\t\ty_true.extend(b_labels)\n","\t\ty_pred.extend(preds)\n","\t\tsents.extend(b_sents)\n","\n","\tf1 = f1_score(y_true, y_pred, average='macro')\n","\tacc = accuracy_score(y_true, y_pred)\n","\n","\treturn acc, f1, y_pred, y_true, sents\n","\n","def save_model(model, optimizer, args, config, filepath):\n","\tsave_info = {\n","\t\t'model': model.state_dict(),\n","\t\t'optim': optimizer.state_dict(),\n","\t\t#'args': args,\n","\t\t'model_config': config,\n","\t\t'system_rng': random.getstate(),\n","\t\t'numpy_rng': np.random.get_state(),\n","\t\t'torch_rng': torch.random.get_rng_state(),\n","\t}\n","\n","\ttorch.save(save_info, filepath)\n","\tprint(f\"save the model to {filepath}\")\n","\n","def train(args):\n","\tdevice = torch.device('cuda') if args.use_gpu else torch.device('cpu')\n","\t#### Load data\n","\t# create the data and its corresponding datasets and dataloader\n","\ttokenizer = Tokenizer(args.max_sentence_len)\n","\ttrain_data, num_labels = create_data(args.train, tokenizer, 'train')\n","\tdev_data = create_data(args.dev, tokenizer, 'valid')\n","\n","\ttrain_dataset = LlamaDataset(train_data, args)\n","\tdev_dataset = LlamaDataset(dev_data, args)\n","\n","\ttrain_dataloader = DataLoader(train_dataset, shuffle=True, batch_size=args.batch_size,\n","\t\t\t\t\t\t\t\t  collate_fn=train_dataset.collate_fn)\n","\tdev_dataloader = DataLoader(dev_dataset, shuffle=False, batch_size=args.batch_size,\n","\t\t\t\t\t\t\t\tcollate_fn=dev_dataset.collate_fn)\n","\n","\t#### Init model\n","\tconfig = {'hidden_dropout_prob': args.hidden_dropout_prob,\n","\t\t\t  'pretrained_model_path': args.pretrained_model_path,\n","\t\t\t  'num_labels': num_labels,\n","\t\t\t  'data_dir': '.',\n","\t\t\t  'option': args.option}\n","\n","\tconfig = SimpleNamespace(**config)\n","\n","\t# initialize the Senetence Classification Model\n","\tmodel = LlamaEmbeddingClassifier(config)\n","\tmodel = model.to(device)\n","\n","\tlr = args.lr\n","\t## specify the optimizer\n","\toptimizer = AdamW(model.parameters(), lr=lr)\n","\tbest_dev_acc = 0\n","\n","\t## run for the specified number of epochs\n","\tfor epoch in tqdm(range(args.epochs)):\n","\t\tmodel.train()\n","\t\ttrain_loss = 0\n","\t\tnum_batches = 0\n","\t\tfor step, batch in enumerate(tqdm(train_dataloader, desc=f'train-{epoch}', disable=TQDM_DISABLE)):\n","\t\t\tb_ids, b_labels, b_sents = batch['token_ids'], batch['labels'], batch['sents']\n","\n","\t\t\tb_ids = b_ids.to(device)\n","\t\t\tb_labels = b_labels.to(device)\n","\n","\t\t\toptimizer.zero_grad()\n","\t\t\tlogits = model(b_ids)\n","\t\t\tloss = F.nll_loss(logits, b_labels.view(-1), reduction='sum') / args.batch_size\n","\n","\t\t\tloss.backward()\n","\t\t\toptimizer.step()\n","\n","\t\t\ttrain_loss += loss.item()\n","\t\t\tnum_batches += 1\n","\n","\t\ttrain_loss = train_loss / (num_batches)\n","\n","\t\ttrain_acc, train_f1, *_ = model_eval(train_dataloader, model, device)\n","\t\tdev_acc, dev_f1, *_ = model_eval(dev_dataloader, model, device)\n","\n","\t\tif dev_acc > best_dev_acc:\n","\t\t\tbest_dev_acc = dev_acc\n","\t\t\tsave_model(model, optimizer, args, config, args.filepath)\n","\n","\t\tprint(f\"epoch {epoch}: train loss :: {train_loss :.3f}, train acc :: {train_acc :.3f}, dev acc :: {dev_acc :.3f}\")\n","\n","def generate_sentence(args, prefix, outfile, max_new_tokens = 75, temperature = 0.0):\n","\twith torch.no_grad():\n","\t\tdevice = torch.device('cuda') if args.use_gpu else torch.device('cpu')\n","\t\tctx = torch.amp.autocast(device_type=\"cuda\", dtype=torch.float32) if args.use_gpu else nullcontext()\n","\t\tllama = load_pretrained(args.pretrained_model_path)\n","\t\tllama = llama.to(device)\n","\t\tprint(f\"load model from {args.pretrained_model_path}\")\n","\t\tenc = Tokenizer(args.max_sentence_len)\n","\n","\t\tstart_ids = enc.encode(prefix, bos=True, eos=False)\n","\t\tx = (torch.tensor(start_ids, dtype=torch.long, device=device)[None, ...])\n","\n","\t\t# run generation\n","\t\twith torch.no_grad():\n","\t\t\twith ctx:\n","\t\t\t\ty = llama.generate(x, max_new_tokens, temperature=temperature)\n","\t\t\t\tsentence = enc.decode(y[0].tolist())\n","\t\t\t\tprint(f\"Temperature is {temperature}\")\n","\t\t\t\tprint(sentence)\n","\t\t\t\tprint('---------------')\n","\t\t\t\twriter = open(outfile, 'w')\n","\t\t\t\twriter.write(sentence)\n","\t\t\t\tprint(f\"Wrote generated sentence to {outfile}.\")\n","\t\t\t\twriter.close()\n","\n","def write_predictions_to_file(split: str, outfile: str, acc: float, pred: list[str], sents: list[str]):\n","\twith open(outfile, \"w+\", encoding=\"utf-8\") as f:\n","\t\tprint(f\"{split} acc :: {acc :.3f}\")\n","\t\tfor s, p in zip(sents, pred):\n","\t\t\tf.write(f\"{p} ||| {s}\\n\")\n","\n","def test_with_prompting(args):\n","\tassert args.dev_out.endswith(\"dev-prompting-output.txt\"), 'For saving prompting results, please set the dev_out argument as \"<dataset>-dev-prompting-output.txt\"'\n","\tassert args.test_out.endswith(\"test-prompting-output.txt\"), 'For saving prompting results, please set the test_out argument as \"<dataset>-test-prompting-output.txt\"'\n","\n","\twith torch.no_grad():\n","\n","\t\tdevice = torch.device('cuda') if args.use_gpu else torch.device('cpu')\n","\t\t#### Load data\n","\t\t# create the data and its corresponding datasets and dataloader\n","\t\ttokenizer = Tokenizer(args.max_sentence_len)\n","\t\tlabel_names = json.load(open(args.label_names, 'r'))\n","\t\t_, num_labels = create_data(args.train, tokenizer, 'train')\n","\n","\t\t#### Init model\n","\t\tconfig = {'pretrained_model_path': args.pretrained_model_path,\n","\t\t\t\t'label_names': label_names,\n","\t\t\t\t'num_labels': num_labels,\n","\t\t\t\t'data_dir': '.',\n","\t\t\t\t'option': args.option}\n","\n","\t\tconfig = SimpleNamespace(**config)\n","\n","\t\tif len(label_names) == 2:\n","\t\t\tlabel_name_str = \" or \".join(label_names)\n","\t\telse:\n","\t\t\tlabel_name_str = \", \".join(label_names[:-1]) + \", or \" + label_names[-1]\n","\t\tprompt_suffix=f\"Is this movie {label_name_str}? This movie is \"\n","\t\tmodel = LlamaZeroShotClassifier(config, tokenizer, label_names)\n","\t\tmodel = model.to(device)\n","\n","\t\tdev_data = create_data(args.dev, tokenizer, 'valid', eos=False, prompt_suffix=prompt_suffix)\n","\t\tdev_dataset = LlamaDataset(dev_data, args, eos=False)\n","\t\tdev_dataloader = DataLoader(dev_dataset, shuffle=False, batch_size=args.batch_size, collate_fn=dev_dataset.collate_fn)\n","\n","\t\ttest_data = create_data(args.test, tokenizer, 'test', eos=False, prompt_suffix=prompt_suffix)\n","\t\ttest_dataset = LlamaDataset(test_data, args, eos=False)\n","\t\ttest_dataloader = DataLoader(test_dataset, shuffle=False, batch_size=args.batch_size, collate_fn=test_dataset.collate_fn)\n","\n","\t\tdev_acc, dev_f1, dev_pred, dev_true, dev_sents = model_eval(dev_dataloader, model, device)\n","\t\ttest_acc, test_f1, test_pred, test_true, test_sents = model_eval(test_dataloader, model, device)\n","\n","\t\twrite_predictions_to_file(\"dev\", args.dev_out, dev_acc, dev_pred, dev_sents)\n","\t\twrite_predictions_to_file(\"test\", args.test_out, test_acc, test_pred, test_sents)\n","\n","def test(args):\n","\tassert args.dev_out.endswith(\"dev-finetuning-output.txt\"), 'For saving finetuning results, please set the dev_out argument as \"<dataset>-dev-finetuning-output.txt\"'\n","\tassert args.test_out.endswith(\"test-finetuning-output.txt\"), 'For saving finetuning results, please set the test_out argument as \"<dataset>-test-finetuning-output.txt\"'\n","\twith torch.no_grad():\n","\t\tdevice = torch.device('cuda') if args.use_gpu else torch.device('cpu')\n","\t\tsaved = torch.load(args.filepath)\n","\t\tconfig = saved['model_config']\n","\t\tmodel = LlamaEmbeddingClassifier(config)\n","\t\tmodel.load_state_dict(saved['model'])\n","\t\tmodel = model.to(device)\n","\t\tprint(f\"load model from {args.filepath}\")\n","\t\ttokenizer = Tokenizer(args.max_sentence_len)\n","\t\tdev_data = create_data(args.dev, tokenizer, 'valid')\n","\t\tdev_dataset = LlamaDataset(dev_data, args)\n","\t\tdev_dataloader = DataLoader(dev_dataset, shuffle=False, batch_size=args.batch_size, collate_fn=dev_dataset.collate_fn)\n","\n","\t\ttest_data = create_data(args.test, tokenizer, 'test')\n","\t\ttest_dataset = LlamaDataset(test_data, args)\n","\t\ttest_dataloader = DataLoader(test_dataset, shuffle=False, batch_size=args.batch_size, collate_fn=test_dataset.collate_fn)\n","\n","\t\tdev_acc, dev_f1, dev_pred, dev_true, dev_sents = model_eval(dev_dataloader, model, device)\n","\t\ttest_acc, test_f1, test_pred, test_true, test_sents = model_eval(test_dataloader, model, device)\n","\n","\t\twrite_predictions_to_file(\"dev\", args.dev_out, dev_acc, dev_pred, dev_sents)\n","\t\twrite_predictions_to_file(\"test\", args.test_out, test_acc, test_pred, test_sents)\n","\n","def get_args():\n","    # Instead of using argparse, we define a simple class to hold our parameters\n","    class Args:\n","        def __init__(self):\n","            self.train = \"data/cfimdb-train.txt\"\n","            self.dev = \"data/cfimdb-dev.txt\"\n","            self.test = \"data/cfimdb-test.txt\"\n","            self.label_names = \"data/cfimdb-label-mapping.json\"\n","            self.pretrained_model_path = \"stories42M.pt\"\n","            self.max_sentence_len = None\n","            self.seed = 1337\n","            self.epochs = 5\n","            self.option = \"generate\"  # ('generate', 'prompt', 'finetune')'prompt: the Llama parameters are frozen; finetune: Llama parameters are updated',\n","            self.use_gpu = False  # Set to True if you want to use GPU\n","            self.generated_sentence_low_temp_out = \"generated-sentence-temp-0.txt\"\n","            self.generated_sentence_high_temp_out = \"generated-sentence-temp-1.txt\"\n","            self.dev_out = \"cfimdb-dev-prompting-output.txt\"\n","            self.test_out = \"cfimdb-test-prompting-output.txt\"\n","            self.batch_size = 8 # sst: 64, cfimdb: 8 can fit a 12GB GPU\n","            self.hidden_dropout_prob = 0.3\n","            self.lr = 2e-5 # default lr for 'pretrain': 1e-3, 'finetune': 1e-5\", default=2e-5\n","\n","    args = Args()\n","    print(f\"args: {vars(args)}\")\n","    return args\n","\n","if __name__ == \"__main__\":\n","    args = get_args()\n","    args.filepath = f'{args.option}-{args.epochs}-{args.lr}.pt'  # save path\n","    seed_everything(args.seed)  # fix the seed for reproducibility\n","\n","    if args.option == \"generate\":\n","        # Step 1\n","        # Complete this sentence to test your implementation!\n","        prefix = \"I have wanted to see this thriller for a while, and it didn't disappoint. Keanu Reeves, playing the hero John Wick, is\"\n","        generate_sentence(args, prefix, args.generated_sentence_low_temp_out, max_new_tokens=75, temperature=0.0)\n","        generate_sentence(args, prefix, args.generated_sentence_high_temp_out, max_new_tokens=75, temperature=1.0)\n","    elif args.option == \"prompt\":\n","        # Step 2\n","        # Solve this task with prompted language modeling\n","        test_with_prompting(args)\n","    elif args.option == \"finetune\":\n","        # Step 3\n","        # Finetune a classification model\n","        train(args)\n","\n","        # Step 4\n","        # Evaluate your model on the dev and test sets\n","        test(args)\n","    else:\n","        raise ValueError(f\"Invalid option: {args.option}\")"]},{"cell_type":"markdown","metadata":{"id":"k9iu6rJlyzs3"},"source":["# Zero-Shot Prompting for SST"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":276430,"status":"ok","timestamp":1732156751108,"user":{"displayName":"Chaoran Zhou","userId":"09162553986537566448"},"user_tz":-480},"id":"tIz9GP0Dyzke","outputId":"05546584-a6a9-451d-a40b-4424b44fc4d9"},"outputs":[{"name":"stdout","output_type":"stream","text":["args: {'train': 'data/sst-train.txt', 'dev': 'data/sst-dev.txt', 'test': 'data/sst-test.txt', 'label_names': 'data/sst-label-mapping.json', 'pretrained_model_path': 'stories42M.pt', 'max_sentence_len': None, 'seed': 1337, 'epochs': 10, 'option': 'prompt', 'use_gpu': False, 'generated_sentence_low_temp_out': 'generated-sentence-temp-0.txt', 'generated_sentence_high_temp_out': 'generated-sentence-temp-1.txt', 'dev_out': 'sst-dev-prompting-output.txt', 'test_out': 'sst-test-prompting-output.txt', 'batch_size': 10, 'hidden_dropout_prob': 0.3, 'lr': 2e-05}\n","load 8544 data from data/sst-train.txt\n"]},{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_7362/2561774016.py:332: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  checkpoint_dict = torch.load(checkpoint, map_location=device)\n"]},{"name":"stdout","output_type":"stream","text":["load 1101 data from data/sst-dev.txt\n","load 2210 data from data/sst-test.txt\n"]},{"name":"stderr","output_type":"stream","text":["eval: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 111/111 [01:32<00:00,  1.20it/s]\n","eval: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 221/221 [02:56<00:00,  1.25it/s]\n"]},{"name":"stdout","output_type":"stream","text":["dev acc :: 0.213\n","test acc :: 0.224\n"]}],"source":["def get_args_zero_shot():\n","    # Instead of using argparse, we define a simple class to hold our parameters\n","    class Args:\n","        def __init__(self):\n","            self.train = \"data/sst-train.txt\"\n","            self.dev = \"data/sst-dev.txt\"\n","            self.test = \"data/sst-test.txt\"\n","            self.label_names = \"data/sst-label-mapping.json\"\n","            self.pretrained_model_path = \"stories42M.pt\"\n","            self.max_sentence_len = None\n","            self.seed = 1337\n","            self.epochs = 10\n","            self.option = \"prompt\"  # ('generate', 'prompt', 'finetune')'prompt: the Llama parameters are frozen; finetune: Llama parameters are updated',\n","            self.use_gpu = False  # Set to True if you want to use GPU\n","            self.generated_sentence_low_temp_out = \"generated-sentence-temp-0.txt\"\n","            self.generated_sentence_high_temp_out = \"generated-sentence-temp-1.txt\"\n","            self.dev_out = \"sst-dev-prompting-output.txt\"\n","            self.test_out = \"sst-test-prompting-output.txt\"\n","            self.batch_size = 10 # sst: 64, cfimdb: 8 can fit a 12GB GPU\n","            self.hidden_dropout_prob = 0.3\n","            self.lr = 2e-5 # default lr for 'pretrain': 1e-3, 'finetune': 1e-5\", default=2e-5\n","\n","    args = Args()\n","    print(f\"args: {vars(args)}\")\n","    return args\n","\n","if __name__ == \"__main__\":\n","    args = get_args_zero_shot()\n","    args.filepath = f'{args.option}-{args.epochs}-{args.lr}.pt'  # save path\n","    seed_everything(args.seed)  # fix the seed for reproducibility\n","\n","    if args.option == \"generate\":\n","        # Step 1\n","        # Complete this sentence to test your implementation!\n","        prefix = \"I have wanted to see this thriller for a while, and it didn't disappoint. Keanu Reeves, playing the hero John Wick, is\"\n","        generate_sentence(args, prefix, args.generated_sentence_low_temp_out, max_new_tokens=75, temperature=0.0)\n","        generate_sentence(args, prefix, args.generated_sentence_high_temp_out, max_new_tokens=75, temperature=1.0)\n","    elif args.option == \"prompt\":\n","        # Step 2\n","        # Solve this task with prompted language modeling\n","        test_with_prompting(args)\n","    elif args.option == \"finetune\":\n","        # Step 3\n","        # Finetune a classification model\n","        train(args)\n","\n","        # Step 4\n","        # Evaluate your model on the dev and test sets\n","        test(args)\n","    else:\n","        raise ValueError(f\"Invalid option: {args.option}\")"]},{"cell_type":"markdown","metadata":{"id":"qLMvhX4-yzdx"},"source":["# Zero-Shot Prompting for CFIMDB"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":203987,"status":"ok","timestamp":1732157276948,"user":{"displayName":"Chaoran Zhou","userId":"09162553986537566448"},"user_tz":-480},"id":"d3hqusg3yzWg","outputId":"3f0d0fce-c2ef-4baa-db38-15dde1172a50"},"outputs":[{"name":"stdout","output_type":"stream","text":["args: {'train': 'data/cfimdb-train.txt', 'dev': 'data/cfimdb-dev.txt', 'test': 'data/cfimdb-test.txt', 'label_names': 'data/cfimdb-label-mapping.json', 'pretrained_model_path': 'stories42M.pt', 'max_sentence_len': None, 'seed': 1337, 'epochs': 10, 'option': 'prompt', 'use_gpu': False, 'generated_sentence_low_temp_out': 'generated-sentence-temp-0.txt', 'generated_sentence_high_temp_out': 'generated-sentence-temp-1.txt', 'dev_out': 'cfimdb-dev-prompting-output.txt', 'test_out': 'cfimdb-test-prompting-output.txt', 'batch_size': 10, 'hidden_dropout_prob': 0.3, 'lr': 2e-05}\n","load 1707 data from data/cfimdb-train.txt\n"]},{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_7362/2561774016.py:332: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  checkpoint_dict = torch.load(checkpoint, map_location=device)\n"]},{"name":"stdout","output_type":"stream","text":["load 245 data from data/cfimdb-dev.txt\n","load 488 data from data/cfimdb-test.txt\n"]},{"name":"stderr","output_type":"stream","text":["eval: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25/25 [02:47<00:00,  6.70s/it]\n","eval: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 49/49 [05:25<00:00,  6.65s/it]\n"]},{"name":"stdout","output_type":"stream","text":["dev acc :: 0.502\n","test acc :: 0.213\n"]}],"source":["def get_args_zero_shot():\n","    # Instead of using argparse, we define a simple class to hold our parameters\n","    class Args:\n","        def __init__(self):\n","            self.train = \"data/cfimdb-train.txt\"\n","            self.dev = \"data/cfimdb-dev.txt\"\n","            self.test = \"data/cfimdb-test.txt\"\n","            self.label_names = \"data/cfimdb-label-mapping.json\"\n","            self.pretrained_model_path = \"stories42M.pt\"\n","            self.max_sentence_len = None\n","            self.seed = 1337\n","            self.epochs = 10\n","            self.option = \"prompt\"  # ('generate', 'prompt', 'finetune')'prompt: the Llama parameters are frozen; finetune: Llama parameters are updated',\n","            self.use_gpu = False  # Set to True if you want to use GPU\n","            self.generated_sentence_low_temp_out = \"generated-sentence-temp-0.txt\"\n","            self.generated_sentence_high_temp_out = \"generated-sentence-temp-1.txt\"\n","            self.dev_out = \"cfimdb-dev-prompting-output.txt\"\n","            self.test_out = \"cfimdb-test-prompting-output.txt\"\n","            self.batch_size = 10 # sst: 64, cfimdb: 8 can fit a 12GB GPU\n","            self.hidden_dropout_prob = 0.3\n","            self.lr = 2e-5 # default lr for 'pretrain': 1e-3, 'finetune': 1e-5\", default=2e-5\n","\n","    args = Args()\n","    print(f\"args: {vars(args)}\")\n","    return args\n","\n","if __name__ == \"__main__\":\n","    args = get_args_zero_shot()\n","    args.filepath = f'{args.option}-{args.epochs}-{args.lr}.pt'  # save path\n","    seed_everything(args.seed)  # fix the seed for reproducibility\n","\n","    if args.option == \"generate\":\n","        # Step 1\n","        # Complete this sentence to test your implementation!\n","        prefix = \"I have wanted to see this thriller for a while, and it didn't disappoint. Keanu Reeves, playing the hero John Wick, is\"\n","        generate_sentence(args, prefix, args.generated_sentence_low_temp_out, max_new_tokens=75, temperature=0.0)\n","        generate_sentence(args, prefix, args.generated_sentence_high_temp_out, max_new_tokens=75, temperature=1.0)\n","    elif args.option == \"prompt\":\n","        # Step 2\n","        # Solve this task with prompted language modeling\n","        test_with_prompting(args)\n","    elif args.option == \"finetune\":\n","        # Step 3\n","        # Finetune a classification model\n","        train(args)\n","\n","        # Step 4\n","        # Evaluate your model on the dev and test sets\n","        test(args)\n","    else:\n","        raise ValueError(f\"Invalid option: {args.option}\")"]},{"cell_type":"markdown","metadata":{"id":"Kh81QdL5yzQW"},"source":["# Finetuning for SST"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":680996,"status":"ok","timestamp":1732170534026,"user":{"displayName":"Chaoran Zhou","userId":"09162553986537566448"},"user_tz":-480},"id":"VTf_sldqyzIa","outputId":"571924a8-88ad-4e2f-a86e-cec878b04860"},"outputs":[{"name":"stdout","output_type":"stream","text":["args: {'train': 'data/sst-train.txt', 'dev': 'data/sst-dev.txt', 'test': 'data/sst-test.txt', 'label_names': 'data/sst-label-mapping.json', 'pretrained_model_path': 'stories42M.pt', 'max_sentence_len': None, 'seed': 1337, 'epochs': 5, 'option': 'finetune', 'use_gpu': False, 'generated_sentence_low_temp_out': 'generated-sentence-temp-0.txt', 'generated_sentence_high_temp_out': 'generated-sentence-temp-1.txt', 'dev_out': 'sst-dev-finetuning-output.txt', 'test_out': 'sst-test-finetuning-output.txt', 'batch_size': 80, 'hidden_dropout_prob': 0.3, 'lr': 2e-05}\n","load 8544 data from data/sst-train.txt\n","load 1101 data from data/sst-dev.txt\n"]},{"name":"stderr","output_type":"stream","text":["/tmp/ipykernel_7362/2561774016.py:332: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  checkpoint_dict = torch.load(checkpoint, map_location=device)\n","  0%|          | 0/5 [00:00<?, ?it/s]\n","train-0:   0%|          | 0/107 [00:00<?, ?it/s]\u001b[A\n","train-0:   1%|          | 1/107 [00:21<37:30, 21.23s/it]\u001b[A\n","train-0:   2%|‚ñè         | 2/107 [00:41<36:05, 20.63s/it]\u001b[A\n","train-0:   3%|‚ñé         | 3/107 [00:57<32:19, 18.65s/it]\u001b[A\n","train-0:   4%|‚ñé         | 4/107 [01:13<29:56, 17.44s/it]\u001b[A\n","train-0:   5%|‚ñç         | 5/107 [01:28<28:04, 16.52s/it]\u001b[A\n","train-0:   6%|‚ñå         | 6/107 [01:42<26:41, 15.86s/it]\u001b[A\n","train-0:   7%|‚ñã         | 7/107 [01:59<26:55, 16.15s/it]\u001b[A\n","train-0:   7%|‚ñã         | 8/107 [02:14<26:10, 15.86s/it]\u001b[A\n","train-0:   8%|‚ñä         | 9/107 [02:32<26:56, 16.50s/it]\u001b[A\n","train-0:   9%|‚ñâ         | 10/107 [02:49<26:38, 16.48s/it]\u001b[A\n","train-0:  10%|‚ñà         | 11/107 [03:07<27:05, 16.93s/it]\u001b[A\n","train-0:  11%|‚ñà         | 12/107 [03:22<26:07, 16.50s/it]\u001b[A\n","train-0:  12%|‚ñà‚ñè        | 13/107 [03:39<25:50, 16.49s/it]\u001b[A\n","train-0:  13%|‚ñà‚ñé        | 14/107 [03:56<26:06, 16.85s/it]\u001b[A\n","train-0:  14%|‚ñà‚ñç        | 15/107 [04:17<27:47, 18.13s/it]\u001b[A\n","train-0:  15%|‚ñà‚ñç        | 16/107 [04:38<28:46, 18.97s/it]\u001b[A\n","train-0:  16%|‚ñà‚ñå        | 17/107 [04:53<26:34, 17.72s/it]\u001b[A\n","train-0:  17%|‚ñà‚ñã        | 18/107 [05:11<26:09, 17.63s/it]\u001b[A\n","train-0:  18%|‚ñà‚ñä        | 19/107 [05:25<24:39, 16.81s/it]\u001b[A\n","train-0:  19%|‚ñà‚ñä        | 20/107 [05:41<23:57, 16.52s/it]\u001b[A\n","train-0:  20%|‚ñà‚ñâ        | 21/107 [06:01<25:14, 17.61s/it]\u001b[A\n","train-0:  21%|‚ñà‚ñà        | 22/107 [06:18<24:42, 17.44s/it]\u001b[A\n","train-0:  21%|‚ñà‚ñà‚ñè       | 23/107 [06:36<24:37, 17.58s/it]\u001b[A\n","train-0:  22%|‚ñà‚ñà‚ñè       | 24/107 [06:52<23:35, 17.05s/it]\u001b[A\n","train-0:  23%|‚ñà‚ñà‚ñé       | 25/107 [07:06<22:09, 16.21s/it]\u001b[A\n","train-0:  24%|‚ñà‚ñà‚ñç       | 26/107 [07:23<22:10, 16.43s/it]\u001b[A\n","train-0:  25%|‚ñà‚ñà‚ñå       | 27/107 [07:41<22:18, 16.73s/it]\u001b[A\n","train-0:  26%|‚ñà‚ñà‚ñå       | 28/107 [07:59<22:37, 17.18s/it]\u001b[A\n","train-0:  27%|‚ñà‚ñà‚ñã       | 29/107 [08:14<21:20, 16.42s/it]\u001b[A\n","train-0:  28%|‚ñà‚ñà‚ñä       | 30/107 [08:29<20:47, 16.21s/it]\u001b[A\n","train-0:  29%|‚ñà‚ñà‚ñâ       | 31/107 [08:47<20:58, 16.56s/it]\u001b[A\n","train-0:  30%|‚ñà‚ñà‚ñâ       | 32/107 [09:08<22:16, 17.82s/it]\u001b[A\n","train-0:  31%|‚ñà‚ñà‚ñà       | 33/107 [09:26<22:05, 17.91s/it]\u001b[A\n","train-0:  32%|‚ñà‚ñà‚ñà‚ñè      | 34/107 [09:42<21:05, 17.33s/it]\u001b[A\n","train-0:  33%|‚ñà‚ñà‚ñà‚ñé      | 35/107 [10:00<21:15, 17.71s/it]\u001b[A\n","train-0:  34%|‚ñà‚ñà‚ñà‚ñé      | 36/107 [10:14<19:34, 16.54s/it]\u001b[A\n","train-0:  35%|‚ñà‚ñà‚ñà‚ñç      | 37/107 [10:33<20:18, 17.41s/it]\u001b[A\n","train-0:  36%|‚ñà‚ñà‚ñà‚ñå      | 38/107 [10:48<19:00, 16.53s/it]\u001b[A\n","train-0:  36%|‚ñà‚ñà‚ñà‚ñã      | 39/107 [11:04<18:40, 16.48s/it]\u001b[A\n","train-0:  37%|‚ñà‚ñà‚ñà‚ñã      | 40/107 [11:21<18:31, 16.59s/it]\u001b[A\n","train-0:  38%|‚ñà‚ñà‚ñà‚ñä      | 41/107 [11:37<18:00, 16.38s/it]\u001b[A\n","train-0:  39%|‚ñà‚ñà‚ñà‚ñâ      | 42/107 [11:52<17:18, 15.98s/it]\u001b[A\n","train-0:  40%|‚ñà‚ñà‚ñà‚ñà      | 43/107 [12:09<17:21, 16.27s/it]\u001b[A\n","train-0:  41%|‚ñà‚ñà‚ñà‚ñà      | 44/107 [12:27<17:34, 16.74s/it]\u001b[A\n","train-0:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 45/107 [12:41<16:36, 16.08s/it]\u001b[A\n","train-0:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 46/107 [12:57<16:05, 15.82s/it]\u001b[A\n","train-0:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 47/107 [13:11<15:20, 15.34s/it]\u001b[A\n","train-0:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 48/107 [13:33<16:57, 17.25s/it]\u001b[A\n","train-0:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 49/107 [13:47<15:51, 16.40s/it]\u001b[A\n","train-0:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 50/107 [14:05<15:56, 16.78s/it]\u001b[A\n","train-0:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 51/107 [14:19<15:01, 16.10s/it]\u001b[A\n","train-0:  49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 52/107 [14:37<15:06, 16.48s/it]\u001b[A\n","train-0:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 53/107 [14:52<14:38, 16.26s/it]\u001b[A\n","train-0:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 54/107 [15:08<14:17, 16.19s/it]\u001b[A\n","train-0:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 55/107 [15:36<16:54, 19.52s/it]\u001b[A\n","train-0:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 56/107 [15:54<16:26, 19.34s/it]\u001b[A\n","train-0:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 57/107 [16:11<15:30, 18.60s/it]\u001b[A\n","train-0:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 58/107 [16:27<14:33, 17.82s/it]\u001b[A\n","train-0:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 59/107 [16:44<13:58, 17.48s/it]\u001b[A\n","train-0:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 60/107 [16:59<13:01, 16.64s/it]\u001b[A\n","train-0:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 61/107 [17:16<12:58, 16.93s/it]\u001b[A\n","train-0:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 62/107 [17:33<12:38, 16.86s/it]\u001b[A\n","train-0:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 63/107 [17:46<11:34, 15.79s/it]\u001b[A\n","train-0:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 64/107 [18:07<12:23, 17.29s/it]\u001b[A\n","train-0:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 65/107 [18:24<12:02, 17.21s/it]\u001b[A\n","train-0:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 66/107 [18:39<11:12, 16.40s/it]\u001b[A\n","train-0:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 67/107 [18:54<10:47, 16.19s/it]\u001b[A\n","train-0:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 68/107 [19:12<10:50, 16.69s/it]\u001b[A\n","train-0:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 69/107 [19:34<11:32, 18.22s/it]\u001b[A\n","train-0:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 70/107 [19:49<10:37, 17.23s/it]\u001b[A\n","train-0:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 71/107 [20:06<10:20, 17.24s/it]\u001b[A\n","train-0:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 72/107 [20:22<09:44, 16.70s/it]\u001b[A\n","train-0:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 73/107 [20:38<09:27, 16.68s/it]\u001b[A\n","train-0:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 74/107 [20:55<09:09, 16.66s/it]\u001b[A\n","train-0:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 75/107 [21:10<08:36, 16.13s/it]\u001b[A\n","train-0:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 76/107 [21:26<08:23, 16.24s/it]\u001b[A\n","train-0:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 77/107 [21:41<07:54, 15.83s/it]\u001b[A\n","train-0:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 78/107 [21:57<07:35, 15.72s/it]\u001b[A\n","train-0:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 79/107 [22:15<07:45, 16.61s/it]\u001b[A\n","train-0:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 80/107 [22:33<07:35, 16.89s/it]\u001b[A\n","train-0:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 81/107 [22:51<07:30, 17.31s/it]\u001b[A\n","train-0:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 82/107 [23:10<07:21, 17.67s/it]\u001b[A\n","train-0:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 83/107 [23:25<06:48, 17.03s/it]\u001b[A\n","train-0:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 84/107 [23:41<06:25, 16.74s/it]\u001b[A\n","train-0:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 85/107 [23:58<06:06, 16.68s/it]\u001b[A\n","train-0:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 86/107 [24:16<05:59, 17.11s/it]\u001b[A\n","train-0:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 87/107 [24:41<06:30, 19.54s/it]\u001b[A\n","train-0:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 88/107 [24:55<05:39, 17.85s/it]\u001b[A\n","train-0:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 89/107 [25:14<05:24, 18.05s/it]\u001b[A\n","train-0:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 90/107 [25:32<05:09, 18.19s/it]\u001b[A\n","train-0:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 91/107 [25:48<04:40, 17.51s/it]\u001b[A\n","train-0:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 92/107 [26:06<04:25, 17.69s/it]\u001b[A\n","train-0:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 93/107 [26:24<04:08, 17.75s/it]\u001b[A\n","train-0:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 94/107 [26:41<03:49, 17.66s/it]\u001b[A\n","train-0:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 95/107 [26:58<03:29, 17.47s/it]\u001b[A\n","train-0:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 96/107 [27:17<03:17, 17.94s/it]\u001b[A\n","train-0:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 97/107 [27:35<02:57, 17.80s/it]\u001b[A\n","train-0:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 98/107 [27:53<02:41, 17.89s/it]\u001b[A\n","train-0:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 99/107 [28:10<02:20, 17.52s/it]\u001b[A\n","train-0:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 100/107 [28:24<01:54, 16.41s/it]\u001b[A\n","train-0:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 101/107 [28:42<01:41, 16.95s/it]\u001b[A\n","train-0:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 102/107 [28:57<01:22, 16.51s/it]\u001b[A\n","train-0:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 103/107 [29:14<01:05, 16.49s/it]\u001b[A\n","train-0:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 104/107 [29:31<00:50, 16.77s/it]\u001b[A\n","train-0:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 105/107 [29:48<00:33, 16.82s/it]\u001b[A\n","train-0:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 106/107 [30:04<00:16, 16.64s/it]\u001b[A\n","train-0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 107/107 [30:18<00:00, 17.00s/it]\n","\n","eval:   0%|          | 0/107 [00:00<?, ?it/s]\u001b[A\n","eval:   1%|          | 1/107 [00:07<12:31,  7.09s/it]\u001b[A\n","eval:   2%|‚ñè         | 2/107 [00:11<09:17,  5.31s/it]\u001b[A\n","eval:   3%|‚ñé         | 3/107 [00:15<08:17,  4.78s/it]\u001b[A\n","eval:   4%|‚ñé         | 4/107 [00:22<09:31,  5.55s/it]\u001b[A\n","eval:   5%|‚ñç         | 5/107 [00:27<09:14,  5.44s/it]\u001b[A\n","eval:   6%|‚ñå         | 6/107 [00:32<08:52,  5.27s/it]\u001b[A\n","eval:   7%|‚ñã         | 7/107 [00:41<10:58,  6.59s/it]\u001b[A\n","eval:   7%|‚ñã         | 8/107 [00:49<11:36,  7.04s/it]\u001b[A\n","eval:   8%|‚ñä         | 9/107 [00:55<11:00,  6.74s/it]\u001b[A\n","eval:   9%|‚ñâ         | 10/107 [01:01<10:21,  6.41s/it]\u001b[A\n","eval:  10%|‚ñà         | 11/107 [01:09<10:57,  6.84s/it]\u001b[A\n","eval:  11%|‚ñà         | 12/107 [01:14<09:55,  6.27s/it]\u001b[A\n","eval:  12%|‚ñà‚ñè        | 13/107 [01:22<10:48,  6.90s/it]\u001b[A\n","eval:  13%|‚ñà‚ñé        | 14/107 [01:27<09:49,  6.33s/it]\u001b[A\n","eval:  14%|‚ñà‚ñç        | 15/107 [01:31<08:49,  5.75s/it]\u001b[A\n","eval:  15%|‚ñà‚ñç        | 16/107 [01:38<09:16,  6.12s/it]\u001b[A\n","eval:  16%|‚ñà‚ñå        | 17/107 [01:43<08:39,  5.77s/it]\u001b[A\n","eval:  17%|‚ñà‚ñã        | 18/107 [01:50<08:52,  5.98s/it]\u001b[A\n","eval:  18%|‚ñà‚ñä        | 19/107 [01:56<08:57,  6.11s/it]\u001b[A\n","eval:  19%|‚ñà‚ñä        | 20/107 [02:02<08:32,  5.89s/it]\u001b[A\n","eval:  20%|‚ñà‚ñâ        | 21/107 [02:08<08:51,  6.18s/it]\u001b[A\n","eval:  21%|‚ñà‚ñà        | 22/107 [02:13<08:09,  5.76s/it]\u001b[A\n","eval:  21%|‚ñà‚ñà‚ñè       | 23/107 [02:20<08:42,  6.23s/it]\u001b[A\n","eval:  22%|‚ñà‚ñà‚ñè       | 24/107 [02:25<07:54,  5.71s/it]\u001b[A\n","eval:  23%|‚ñà‚ñà‚ñé       | 25/107 [02:30<07:29,  5.49s/it]\u001b[A\n","eval:  24%|‚ñà‚ñà‚ñç       | 26/107 [02:37<08:05,  5.99s/it]\u001b[A\n","eval:  25%|‚ñà‚ñà‚ñå       | 27/107 [02:42<07:30,  5.63s/it]\u001b[A\n","eval:  26%|‚ñà‚ñà‚ñå       | 28/107 [02:48<07:38,  5.81s/it]\u001b[A\n","eval:  27%|‚ñà‚ñà‚ñã       | 29/107 [02:54<07:29,  5.76s/it]\u001b[A\n","eval:  28%|‚ñà‚ñà‚ñä       | 30/107 [02:58<06:49,  5.31s/it]\u001b[A\n","eval:  29%|‚ñà‚ñà‚ñâ       | 31/107 [03:03<06:46,  5.35s/it]\u001b[A\n","eval:  30%|‚ñà‚ñà‚ñâ       | 32/107 [03:10<06:59,  5.59s/it]\u001b[A\n","eval:  31%|‚ñà‚ñà‚ñà       | 33/107 [03:14<06:34,  5.33s/it]\u001b[A\n","eval:  32%|‚ñà‚ñà‚ñà‚ñè      | 34/107 [03:21<06:52,  5.65s/it]\u001b[A\n","eval:  33%|‚ñà‚ñà‚ñà‚ñé      | 35/107 [03:25<06:23,  5.33s/it]\u001b[A\n","eval:  34%|‚ñà‚ñà‚ñà‚ñé      | 36/107 [03:30<06:14,  5.28s/it]\u001b[A\n","eval:  35%|‚ñà‚ñà‚ñà‚ñç      | 37/107 [03:36<06:22,  5.46s/it]\u001b[A\n","eval:  36%|‚ñà‚ñà‚ñà‚ñå      | 38/107 [03:41<06:06,  5.32s/it]\u001b[A\n","eval:  36%|‚ñà‚ñà‚ñà‚ñã      | 39/107 [03:47<06:02,  5.34s/it]\u001b[A\n","eval:  37%|‚ñà‚ñà‚ñà‚ñã      | 40/107 [03:53<06:25,  5.76s/it]\u001b[A\n","eval:  38%|‚ñà‚ñà‚ñà‚ñä      | 41/107 [04:02<07:06,  6.46s/it]\u001b[A\n","eval:  39%|‚ñà‚ñà‚ñà‚ñâ      | 42/107 [04:08<06:55,  6.39s/it]\u001b[A\n","eval:  40%|‚ñà‚ñà‚ñà‚ñà      | 43/107 [04:13<06:29,  6.08s/it]\u001b[A\n","eval:  41%|‚ñà‚ñà‚ñà‚ñà      | 44/107 [04:20<06:44,  6.42s/it]\u001b[A\n","eval:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 45/107 [04:25<05:59,  5.80s/it]\u001b[A\n","eval:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 46/107 [04:30<05:44,  5.65s/it]\u001b[A\n","eval:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 47/107 [04:37<05:53,  5.90s/it]\u001b[A\n","eval:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 48/107 [04:41<05:26,  5.54s/it]\u001b[A\n","eval:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 49/107 [04:47<05:24,  5.59s/it]\u001b[A\n","eval:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 50/107 [04:53<05:20,  5.62s/it]\u001b[A\n","eval:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 51/107 [04:57<04:55,  5.28s/it]\u001b[A\n","eval:  49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 52/107 [05:04<05:24,  5.90s/it]\u001b[A\n","eval:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 53/107 [05:09<04:59,  5.54s/it]\u001b[A\n","eval:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 54/107 [05:14<04:43,  5.34s/it]\u001b[A\n","eval:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 55/107 [05:20<04:52,  5.62s/it]\u001b[A\n","eval:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 56/107 [05:25<04:31,  5.32s/it]\u001b[A\n","eval:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 57/107 [05:29<04:14,  5.08s/it]\u001b[A\n","eval:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 58/107 [05:36<04:29,  5.49s/it]\u001b[A\n","eval:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 59/107 [05:41<04:20,  5.42s/it]\u001b[A\n","eval:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 60/107 [05:49<04:45,  6.08s/it]\u001b[A\n","eval:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 61/107 [05:54<04:33,  5.95s/it]\u001b[A\n","eval:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 62/107 [06:00<04:16,  5.70s/it]\u001b[A\n","eval:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 63/107 [06:06<04:25,  6.03s/it]\u001b[A\n","eval:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 64/107 [06:10<03:55,  5.47s/it]\u001b[A\n","eval:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 65/107 [06:17<03:58,  5.69s/it]\u001b[A\n","eval:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 66/107 [06:22<03:49,  5.59s/it]\u001b[A\n","eval:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 67/107 [06:27<03:34,  5.35s/it]\u001b[A\n","eval:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 68/107 [06:33<03:43,  5.73s/it]\u001b[A\n","eval:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 69/107 [06:38<03:25,  5.40s/it]\u001b[A\n","eval:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 70/107 [06:43<03:17,  5.33s/it]\u001b[A\n","eval:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 71/107 [06:49<03:16,  5.45s/it]\u001b[A\n","eval:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 72/107 [06:53<02:58,  5.09s/it]\u001b[A\n","eval:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 73/107 [06:58<02:51,  5.04s/it]\u001b[A\n","eval:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 74/107 [07:04<02:53,  5.24s/it]\u001b[A\n","eval:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 75/107 [07:09<02:42,  5.08s/it]\u001b[A\n","eval:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 76/107 [07:14<02:41,  5.23s/it]\u001b[A\n","eval:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 77/107 [07:21<02:48,  5.60s/it]\u001b[A\n","eval:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 78/107 [07:26<02:40,  5.55s/it]\u001b[A\n","eval:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 79/107 [07:32<02:42,  5.79s/it]\u001b[A\n","eval:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 80/107 [07:37<02:27,  5.47s/it]\u001b[A\n","eval:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 81/107 [07:43<02:28,  5.69s/it]\u001b[A\n","eval:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 82/107 [07:48<02:16,  5.46s/it]\u001b[A\n","eval:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 83/107 [07:53<02:05,  5.24s/it]\u001b[A\n","eval:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 84/107 [07:59<02:09,  5.61s/it]\u001b[A\n","eval:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 85/107 [08:05<02:03,  5.62s/it]\u001b[A\n","eval:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 86/107 [08:10<01:53,  5.39s/it]\u001b[A\n","eval:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 87/107 [08:17<01:56,  5.81s/it]\u001b[A\n","eval:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 88/107 [08:22<01:45,  5.55s/it]\u001b[A\n","eval:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 89/107 [08:29<01:47,  5.98s/it]\u001b[A\n","eval:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 90/107 [08:35<01:41,  5.94s/it]\u001b[A\n","eval:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 91/107 [08:40<01:30,  5.67s/it]\u001b[A\n","eval:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 92/107 [08:46<01:29,  5.95s/it]\u001b[A\n","eval:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 93/107 [08:51<01:18,  5.64s/it]\u001b[A\n","eval:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 94/107 [08:57<01:12,  5.59s/it]\u001b[A\n","eval:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 95/107 [09:03<01:08,  5.72s/it]\u001b[A\n","eval:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 96/107 [09:07<00:59,  5.45s/it]\u001b[A\n","eval:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 97/107 [09:14<00:58,  5.87s/it]\u001b[A\n","eval:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 98/107 [09:18<00:47,  5.32s/it]\u001b[A\n","eval:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 99/107 [09:23<00:40,  5.02s/it]\u001b[A\n","eval:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 100/107 [09:28<00:36,  5.20s/it]\u001b[A\n","eval:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 101/107 [09:34<00:31,  5.26s/it]\u001b[A\n","eval:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 102/107 [09:39<00:26,  5.25s/it]\u001b[A\n","eval:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 103/107 [09:45<00:22,  5.63s/it]\u001b[A\n","eval:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 104/107 [09:51<00:16,  5.66s/it]\u001b[A\n","eval:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 105/107 [09:58<00:11,  5.94s/it]\u001b[A\n","eval:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 106/107 [10:03<00:05,  5.70s/it]\u001b[A\n","eval: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 107/107 [10:06<00:00,  5.67s/it]\n","\n","eval:   0%|          | 0/14 [00:00<?, ?it/s]\u001b[A\n","eval:   7%|‚ñã         | 1/14 [00:05<01:10,  5.43s/it]\u001b[A\n","eval:  14%|‚ñà‚ñç        | 2/14 [00:10<01:02,  5.21s/it]\u001b[A\n","eval:  21%|‚ñà‚ñà‚ñè       | 3/14 [00:16<00:59,  5.45s/it]\u001b[A\n","eval:  29%|‚ñà‚ñà‚ñä       | 4/14 [00:23<01:03,  6.36s/it]\u001b[A\n","eval:  36%|‚ñà‚ñà‚ñà‚ñå      | 5/14 [00:28<00:50,  5.67s/it]\u001b[A\n","eval:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 6/14 [00:33<00:44,  5.55s/it]\u001b[A\n","eval:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 7/14 [00:39<00:39,  5.68s/it]\u001b[A\n","eval:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 8/14 [00:44<00:31,  5.33s/it]\u001b[A\n","eval:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 9/14 [00:50<00:27,  5.54s/it]\u001b[A\n","eval:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 10/14 [00:55<00:21,  5.39s/it]\u001b[A\n","eval:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 11/14 [01:00<00:15,  5.21s/it]\u001b[A\n","eval:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 12/14 [01:06<00:11,  5.58s/it]\u001b[A\n","eval:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 13/14 [01:11<00:05,  5.33s/it]\u001b[A\n","eval: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 14/14 [01:15<00:00,  5.37s/it]\n"," 20%|‚ñà‚ñà        | 1/5 [41:49<2:47:16, 2509.23s/it]"]},{"name":"stdout","output_type":"stream","text":["save the model to finetune-5-2e-05.pt\n","epoch 0: train loss :: 1.882, train acc :: 0.261, dev acc :: 0.262\n"]},{"name":"stderr","output_type":"stream","text":["\n","train-1:   0%|          | 0/107 [00:00<?, ?it/s]\u001b[A\n","train-1:   1%|          | 1/107 [00:19<33:37, 19.03s/it]\u001b[A\n","train-1:   2%|‚ñè         | 2/107 [00:40<35:21, 20.20s/it]\u001b[A\n","train-1:   3%|‚ñé         | 3/107 [00:59<34:18, 19.79s/it]\u001b[A\n","train-1:   4%|‚ñé         | 4/107 [01:15<31:41, 18.46s/it]\u001b[A\n","train-1:   5%|‚ñç         | 5/107 [01:31<29:42, 17.48s/it]\u001b[A\n","train-1:   6%|‚ñå         | 6/107 [01:47<28:46, 17.09s/it]\u001b[A\n","train-1:   7%|‚ñã         | 7/107 [02:04<28:06, 16.86s/it]\u001b[A\n","train-1:   7%|‚ñã         | 8/107 [02:19<26:49, 16.26s/it]\u001b[A\n","train-1:   8%|‚ñä         | 9/107 [02:35<26:42, 16.35s/it]\u001b[A\n","train-1:   9%|‚ñâ         | 10/107 [02:51<26:04, 16.13s/it]\u001b[A\n","train-1:  10%|‚ñà         | 11/107 [03:08<26:02, 16.27s/it]\u001b[A\n","train-1:  11%|‚ñà         | 12/107 [03:25<26:07, 16.50s/it]\u001b[A\n","train-1:  12%|‚ñà‚ñè        | 13/107 [03:43<26:50, 17.13s/it]\u001b[A\n","train-1:  13%|‚ñà‚ñé        | 14/107 [03:59<25:52, 16.69s/it]\u001b[A\n","train-1:  14%|‚ñà‚ñç        | 15/107 [04:13<24:14, 15.81s/it]\u001b[A\n","train-1:  15%|‚ñà‚ñç        | 16/107 [04:31<25:04, 16.53s/it]\u001b[A\n","train-1:  16%|‚ñà‚ñå        | 17/107 [04:47<24:34, 16.38s/it]\u001b[A\n","train-1:  17%|‚ñà‚ñã        | 18/107 [05:03<24:04, 16.23s/it]\u001b[A\n","train-1:  18%|‚ñà‚ñä        | 19/107 [05:24<26:08, 17.83s/it]\u001b[A\n","train-1:  19%|‚ñà‚ñä        | 20/107 [05:40<24:54, 17.18s/it]\u001b[A\n","train-1:  20%|‚ñà‚ñâ        | 21/107 [05:58<25:07, 17.53s/it]\u001b[A\n","train-1:  21%|‚ñà‚ñà        | 22/107 [06:16<24:48, 17.52s/it]\u001b[A\n","train-1:  21%|‚ñà‚ñà‚ñè       | 23/107 [06:34<24:55, 17.80s/it]\u001b[A\n","train-1:  22%|‚ñà‚ñà‚ñè       | 24/107 [06:48<23:08, 16.73s/it]\u001b[A\n","train-1:  23%|‚ñà‚ñà‚ñé       | 25/107 [07:05<22:47, 16.68s/it]\u001b[A\n","train-1:  24%|‚ñà‚ñà‚ñç       | 26/107 [07:25<23:58, 17.76s/it]\u001b[A\n","train-1:  25%|‚ñà‚ñà‚ñå       | 27/107 [07:45<24:28, 18.36s/it]\u001b[A\n","train-1:  26%|‚ñà‚ñà‚ñå       | 28/107 [08:04<24:16, 18.43s/it]\u001b[A\n","train-1:  27%|‚ñà‚ñà‚ñã       | 29/107 [08:19<22:40, 17.44s/it]\u001b[A\n","train-1:  28%|‚ñà‚ñà‚ñä       | 30/107 [08:34<21:42, 16.91s/it]\u001b[A\n","train-1:  29%|‚ñà‚ñà‚ñâ       | 31/107 [08:49<20:34, 16.24s/it]\u001b[A\n","train-1:  30%|‚ñà‚ñà‚ñâ       | 32/107 [09:05<20:19, 16.26s/it]\u001b[A\n","train-1:  31%|‚ñà‚ñà‚ñà       | 33/107 [09:23<20:33, 16.66s/it]\u001b[A\n","train-1:  32%|‚ñà‚ñà‚ñà‚ñè      | 34/107 [09:43<21:26, 17.62s/it]\u001b[A\n","train-1:  33%|‚ñà‚ñà‚ñà‚ñé      | 35/107 [10:01<21:24, 17.84s/it]\u001b[A\n","train-1:  34%|‚ñà‚ñà‚ñà‚ñé      | 36/107 [10:26<23:29, 19.85s/it]\u001b[A\n","train-1:  35%|‚ñà‚ñà‚ñà‚ñç      | 37/107 [10:42<21:45, 18.65s/it]\u001b[A\n","train-1:  36%|‚ñà‚ñà‚ñà‚ñå      | 38/107 [10:58<20:41, 18.00s/it]\u001b[A\n","train-1:  36%|‚ñà‚ñà‚ñà‚ñã      | 39/107 [11:15<20:00, 17.65s/it]\u001b[A\n","train-1:  37%|‚ñà‚ñà‚ñà‚ñã      | 40/107 [11:31<19:11, 17.19s/it]\u001b[A\n","train-1:  38%|‚ñà‚ñà‚ñà‚ñä      | 41/107 [11:50<19:20, 17.58s/it]\u001b[A\n","train-1:  39%|‚ñà‚ñà‚ñà‚ñâ      | 42/107 [12:06<18:47, 17.35s/it]\u001b[A\n","train-1:  40%|‚ñà‚ñà‚ñà‚ñà      | 43/107 [12:25<18:52, 17.70s/it]\u001b[A\n","train-1:  41%|‚ñà‚ñà‚ñà‚ñà      | 44/107 [12:38<17:07, 16.31s/it]\u001b[A\n","train-1:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 45/107 [12:55<16:58, 16.42s/it]\u001b[A\n","train-1:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 46/107 [13:12<17:03, 16.78s/it]\u001b[A\n","train-1:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 47/107 [13:27<16:04, 16.07s/it]\u001b[A\n","train-1:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 48/107 [13:44<16:19, 16.60s/it]\u001b[A\n","train-1:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 49/107 [14:03<16:34, 17.15s/it]\u001b[A\n","train-1:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 50/107 [14:22<16:53, 17.79s/it]\u001b[A\n","train-1:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 51/107 [14:40<16:29, 17.68s/it]\u001b[A\n","train-1:  49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 52/107 [14:56<15:45, 17.20s/it]\u001b[A\n","train-1:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 53/107 [15:13<15:29, 17.22s/it]\u001b[A\n","train-1:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 54/107 [15:27<14:29, 16.40s/it]\u001b[A\n","train-1:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 55/107 [15:42<13:51, 15.99s/it]\u001b[A\n","train-1:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 56/107 [16:00<14:06, 16.60s/it]\u001b[A\n","train-1:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 57/107 [16:18<14:03, 16.87s/it]\u001b[A\n","train-1:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 58/107 [16:33<13:14, 16.21s/it]\u001b[A\n","train-1:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 59/107 [16:53<13:53, 17.36s/it]\u001b[A\n","train-1:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 60/107 [17:08<13:04, 16.70s/it]\u001b[A\n","train-1:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 61/107 [17:23<12:26, 16.23s/it]\u001b[A\n","train-1:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 62/107 [17:41<12:37, 16.82s/it]\u001b[A\n","train-1:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 63/107 [17:57<12:11, 16.61s/it]\u001b[A\n","train-1:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 64/107 [18:14<11:57, 16.68s/it]\u001b[A\n","train-1:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 65/107 [18:31<11:44, 16.76s/it]\u001b[A\n","train-1:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 66/107 [18:48<11:26, 16.74s/it]\u001b[A\n","train-1:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 67/107 [19:13<12:53, 19.33s/it]\u001b[A\n","train-1:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 68/107 [19:31<12:12, 18.78s/it]\u001b[A\n","train-1:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 69/107 [19:47<11:24, 18.00s/it]\u001b[A\n","train-1:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 70/107 [20:04<11:01, 17.89s/it]\u001b[A\n","train-1:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 71/107 [20:21<10:28, 17.47s/it]\u001b[A\n","train-1:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 72/107 [20:36<09:49, 16.84s/it]\u001b[A\n","train-1:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 73/107 [20:55<09:53, 17.45s/it]\u001b[A\n","train-1:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 74/107 [21:10<09:06, 16.55s/it]\u001b[A\n","train-1:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 75/107 [21:29<09:12, 17.27s/it]\u001b[A\n","train-1:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 76/107 [21:47<09:07, 17.68s/it]\u001b[A\n","train-1:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 77/107 [22:04<08:39, 17.31s/it]\u001b[A\n","train-1:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 78/107 [22:22<08:34, 17.73s/it]\u001b[A\n","train-1:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 79/107 [22:38<07:58, 17.09s/it]\u001b[A\n","train-1:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 80/107 [22:55<07:40, 17.07s/it]\u001b[A\n","train-1:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 81/107 [23:12<07:23, 17.05s/it]\u001b[A\n","train-1:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 82/107 [23:29<07:08, 17.12s/it]\u001b[A\n","train-1:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 83/107 [23:44<06:32, 16.35s/it]\u001b[A\n","train-1:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 84/107 [23:58<06:01, 15.74s/it]\u001b[A\n","train-1:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 85/107 [24:16<06:00, 16.40s/it]\u001b[A\n","train-1:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 86/107 [24:33<05:48, 16.60s/it]\u001b[A\n","train-1:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 87/107 [24:49<05:29, 16.50s/it]\u001b[A\n","train-1:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 88/107 [25:05<05:06, 16.15s/it]\u001b[A\n","train-1:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 89/107 [25:21<04:49, 16.07s/it]\u001b[A\n","train-1:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 90/107 [25:35<04:26, 15.66s/it]\u001b[A\n","train-1:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 91/107 [25:53<04:19, 16.21s/it]\u001b[A\n","train-1:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 92/107 [26:08<03:58, 15.90s/it]\u001b[A\n","train-1:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 93/107 [26:23<03:39, 15.65s/it]\u001b[A\n","train-1:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 94/107 [26:39<03:22, 15.57s/it]\u001b[A\n","train-1:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 95/107 [26:55<03:11, 15.93s/it]\u001b[A\n","train-1:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 96/107 [27:11<02:55, 15.93s/it]\u001b[A\n","train-1:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 97/107 [27:28<02:41, 16.18s/it]\u001b[A\n","train-1:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 98/107 [27:49<02:38, 17.66s/it]\u001b[A\n","train-1:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 99/107 [28:05<02:18, 17.29s/it]\u001b[A\n","train-1:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 100/107 [28:20<01:55, 16.46s/it]\u001b[A\n","train-1:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 101/107 [28:37<01:39, 16.60s/it]\u001b[A\n","train-1:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 102/107 [28:55<01:25, 17.01s/it]\u001b[A\n","train-1:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 103/107 [29:13<01:08, 17.22s/it]\u001b[A\n","train-1:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 104/107 [29:30<00:52, 17.38s/it]\u001b[A\n","train-1:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 105/107 [29:46<00:33, 16.98s/it]\u001b[A\n","train-1:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 106/107 [30:03<00:16, 16.95s/it]\u001b[A\n","train-1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 107/107 [30:17<00:00, 16.99s/it]\n","\n","eval:   0%|          | 0/107 [00:00<?, ?it/s]\u001b[A\n","eval:   1%|          | 1/107 [00:04<08:20,  4.73s/it]\u001b[A\n","eval:   2%|‚ñè         | 2/107 [00:10<09:49,  5.62s/it]\u001b[A\n","eval:   3%|‚ñé         | 3/107 [00:16<09:24,  5.42s/it]\u001b[A\n","eval:   4%|‚ñé         | 4/107 [00:20<08:19,  4.85s/it]\u001b[A\n","eval:   5%|‚ñç         | 5/107 [00:27<09:49,  5.78s/it]\u001b[A\n","eval:   6%|‚ñå         | 6/107 [00:32<09:30,  5.65s/it]\u001b[A\n","eval:   7%|‚ñã         | 7/107 [00:38<09:35,  5.75s/it]\u001b[A\n","eval:   7%|‚ñã         | 8/107 [00:44<09:37,  5.84s/it]\u001b[A\n","eval:   8%|‚ñä         | 9/107 [00:49<08:56,  5.48s/it]\u001b[A\n","eval:   9%|‚ñâ         | 10/107 [00:56<09:18,  5.75s/it]\u001b[A\n","eval:  10%|‚ñà         | 11/107 [01:00<08:36,  5.38s/it]\u001b[A\n","eval:  11%|‚ñà         | 12/107 [01:05<08:16,  5.23s/it]\u001b[A\n","eval:  12%|‚ñà‚ñè        | 13/107 [01:11<08:27,  5.40s/it]\u001b[A\n","eval:  13%|‚ñà‚ñé        | 14/107 [01:15<08:04,  5.21s/it]\u001b[A\n","eval:  14%|‚ñà‚ñç        | 15/107 [01:25<09:51,  6.43s/it]\u001b[A\n","eval:  15%|‚ñà‚ñç        | 16/107 [01:30<09:05,  6.00s/it]\u001b[A\n","eval:  16%|‚ñà‚ñå        | 17/107 [01:34<08:25,  5.61s/it]\u001b[A\n","eval:  17%|‚ñà‚ñã        | 18/107 [01:41<08:44,  5.89s/it]\u001b[A\n","eval:  18%|‚ñà‚ñä        | 19/107 [01:46<08:15,  5.63s/it]\u001b[A\n","eval:  19%|‚ñà‚ñä        | 20/107 [01:52<08:15,  5.69s/it]\u001b[A\n","eval:  20%|‚ñà‚ñâ        | 21/107 [01:56<07:39,  5.34s/it]\u001b[A\n","eval:  21%|‚ñà‚ñà        | 22/107 [02:01<07:21,  5.19s/it]\u001b[A\n","eval:  21%|‚ñà‚ñà‚ñè       | 23/107 [02:07<07:30,  5.36s/it]\u001b[A\n","eval:  22%|‚ñà‚ñà‚ñè       | 24/107 [02:12<07:26,  5.38s/it]\u001b[A\n","eval:  23%|‚ñà‚ñà‚ñé       | 25/107 [02:17<07:04,  5.18s/it]\u001b[A\n","eval:  24%|‚ñà‚ñà‚ñç       | 26/107 [02:23<07:19,  5.43s/it]\u001b[A\n","eval:  25%|‚ñà‚ñà‚ñå       | 27/107 [02:28<07:08,  5.35s/it]\u001b[A\n","eval:  26%|‚ñà‚ñà‚ñå       | 28/107 [02:34<07:03,  5.36s/it]\u001b[A\n","eval:  27%|‚ñà‚ñà‚ñã       | 29/107 [02:40<07:17,  5.61s/it]\u001b[A\n","eval:  28%|‚ñà‚ñà‚ñä       | 30/107 [02:45<06:53,  5.38s/it]\u001b[A\n","eval:  29%|‚ñà‚ñà‚ñâ       | 31/107 [02:51<07:12,  5.69s/it]\u001b[A\n","eval:  30%|‚ñà‚ñà‚ñâ       | 32/107 [02:57<07:16,  5.83s/it]\u001b[A\n","eval:  31%|‚ñà‚ñà‚ñà       | 33/107 [03:02<06:53,  5.59s/it]\u001b[A\n","eval:  32%|‚ñà‚ñà‚ñà‚ñè      | 34/107 [03:08<06:59,  5.74s/it]\u001b[A\n","eval:  33%|‚ñà‚ñà‚ñà‚ñé      | 35/107 [03:12<06:17,  5.24s/it]\u001b[A\n","eval:  34%|‚ñà‚ñà‚ñà‚ñé      | 36/107 [03:17<05:55,  5.01s/it]\u001b[A\n","eval:  35%|‚ñà‚ñà‚ñà‚ñç      | 37/107 [03:23<06:19,  5.42s/it]\u001b[A\n","eval:  36%|‚ñà‚ñà‚ñà‚ñå      | 38/107 [03:28<06:04,  5.28s/it]\u001b[A\n","eval:  36%|‚ñà‚ñà‚ñà‚ñã      | 39/107 [03:34<06:11,  5.46s/it]\u001b[A\n","eval:  37%|‚ñà‚ñà‚ñà‚ñã      | 40/107 [03:41<06:40,  5.97s/it]\u001b[A\n","eval:  38%|‚ñà‚ñà‚ñà‚ñä      | 41/107 [03:45<05:58,  5.43s/it]\u001b[A\n","eval:  39%|‚ñà‚ñà‚ñà‚ñâ      | 42/107 [03:52<06:12,  5.73s/it]\u001b[A\n","eval:  40%|‚ñà‚ñà‚ñà‚ñà      | 43/107 [03:57<05:59,  5.62s/it]\u001b[A\n","eval:  41%|‚ñà‚ñà‚ñà‚ñà      | 44/107 [04:02<05:39,  5.39s/it]\u001b[A\n","eval:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 45/107 [04:09<06:06,  5.91s/it]\u001b[A\n","eval:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 46/107 [04:16<06:20,  6.24s/it]\u001b[A\n","eval:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 47/107 [04:23<06:19,  6.32s/it]\u001b[A\n","eval:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 48/107 [04:27<05:38,  5.74s/it]\u001b[A\n","eval:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 49/107 [04:32<05:12,  5.39s/it]\u001b[A\n","eval:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 50/107 [04:39<05:39,  5.95s/it]\u001b[A\n","eval:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 51/107 [04:44<05:14,  5.62s/it]\u001b[A\n","eval:  49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 52/107 [04:51<05:32,  6.05s/it]\u001b[A\n","eval:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 53/107 [04:55<04:58,  5.53s/it]\u001b[A\n","eval:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 54/107 [05:01<04:57,  5.61s/it]\u001b[A\n","eval:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 55/107 [05:08<05:12,  6.00s/it]\u001b[A\n","eval:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 56/107 [05:13<04:51,  5.71s/it]\u001b[A\n","eval:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 57/107 [05:20<05:09,  6.19s/it]\u001b[A\n","eval:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 58/107 [05:25<04:36,  5.64s/it]\u001b[A\n","eval:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 59/107 [05:29<04:19,  5.40s/it]\u001b[A\n","eval:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 60/107 [05:37<04:41,  5.98s/it]\u001b[A\n","eval:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 61/107 [05:41<04:10,  5.46s/it]\u001b[A\n","eval:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 62/107 [05:45<03:49,  5.11s/it]\u001b[A\n","eval:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 63/107 [05:51<03:57,  5.40s/it]\u001b[A\n","eval:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 64/107 [05:57<03:50,  5.36s/it]\u001b[A\n","eval:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 65/107 [06:04<04:05,  5.85s/it]\u001b[A\n","eval:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 66/107 [06:09<03:50,  5.62s/it]\u001b[A\n","eval:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 67/107 [06:14<03:37,  5.44s/it]\u001b[A\n","eval:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 68/107 [06:21<03:53,  5.98s/it]\u001b[A\n","eval:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 69/107 [06:26<03:34,  5.64s/it]\u001b[A\n","eval:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 70/107 [06:32<03:36,  5.86s/it]\u001b[A\n","eval:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 71/107 [06:38<03:26,  5.73s/it]\u001b[A\n","eval:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 72/107 [06:43<03:11,  5.46s/it]\u001b[A\n","eval:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 73/107 [06:49<03:17,  5.80s/it]\u001b[A\n","eval:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 74/107 [06:54<03:01,  5.49s/it]\u001b[A\n","eval:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 75/107 [07:03<03:29,  6.53s/it]\u001b[A\n","eval:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 76/107 [07:08<03:08,  6.10s/it]\u001b[A\n","eval:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 77/107 [07:14<03:01,  6.04s/it]\u001b[A\n","eval:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 78/107 [07:20<02:56,  6.08s/it]\u001b[A\n","eval:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 79/107 [07:26<02:46,  5.93s/it]\u001b[A\n","eval:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 80/107 [07:31<02:39,  5.92s/it]\u001b[A\n","eval:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 81/107 [07:37<02:29,  5.74s/it]\u001b[A\n","eval:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 82/107 [07:42<02:18,  5.54s/it]\u001b[A\n","eval:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 83/107 [07:48<02:19,  5.82s/it]\u001b[A\n","eval:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 84/107 [07:53<02:05,  5.44s/it]\u001b[A\n","eval:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 85/107 [07:58<01:54,  5.21s/it]\u001b[A\n","eval:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 86/107 [08:04<01:57,  5.59s/it]\u001b[A\n","eval:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 87/107 [08:09<01:48,  5.43s/it]\u001b[A\n","eval:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 88/107 [08:15<01:47,  5.67s/it]\u001b[A\n","eval:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 89/107 [08:21<01:41,  5.62s/it]\u001b[A\n","eval:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 90/107 [08:25<01:29,  5.27s/it]\u001b[A\n","eval:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 91/107 [08:31<01:28,  5.51s/it]\u001b[A\n","eval:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 92/107 [08:36<01:17,  5.17s/it]\u001b[A\n","eval:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 93/107 [08:43<01:22,  5.89s/it]\u001b[A\n","eval:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 94/107 [08:50<01:18,  6.06s/it]\u001b[A\n","eval:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 95/107 [08:54<01:05,  5.46s/it]\u001b[A\n","eval:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 96/107 [08:59<01:00,  5.47s/it]\u001b[A\n","eval:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 97/107 [09:04<00:52,  5.30s/it]\u001b[A\n","eval:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 98/107 [09:08<00:44,  4.90s/it]\u001b[A\n","eval:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 99/107 [09:16<00:46,  5.78s/it]\u001b[A\n","eval:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 100/107 [09:21<00:39,  5.63s/it]\u001b[A\n","eval:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 101/107 [09:27<00:33,  5.51s/it]\u001b[A\n","eval:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 102/107 [09:32<00:28,  5.65s/it]\u001b[A\n","eval:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 103/107 [09:38<00:22,  5.51s/it]\u001b[A\n","eval:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 104/107 [09:44<00:17,  5.86s/it]\u001b[A\n","eval:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 105/107 [09:50<00:11,  5.92s/it]\u001b[A\n","eval:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 106/107 [09:55<00:05,  5.52s/it]\u001b[A\n","eval: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 107/107 [10:00<00:00,  5.62s/it]\n","\n","eval:   0%|          | 0/14 [00:00<?, ?it/s]\u001b[A\n","eval:   7%|‚ñã         | 1/14 [00:04<00:52,  4.07s/it]\u001b[A\n","eval:  14%|‚ñà‚ñç        | 2/14 [00:08<00:52,  4.39s/it]\u001b[A\n","eval:  21%|‚ñà‚ñà‚ñè       | 3/14 [00:14<00:56,  5.16s/it]\u001b[A\n","eval:  29%|‚ñà‚ñà‚ñä       | 4/14 [00:20<00:52,  5.29s/it]\u001b[A\n","eval:  36%|‚ñà‚ñà‚ñà‚ñå      | 5/14 [00:25<00:47,  5.25s/it]\u001b[A\n","eval:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 6/14 [00:31<00:43,  5.38s/it]\u001b[A\n","eval:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 7/14 [00:35<00:36,  5.21s/it]\u001b[A\n","eval:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 8/14 [00:41<00:32,  5.48s/it]\u001b[A\n","eval:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 9/14 [00:46<00:26,  5.28s/it]\u001b[A\n","eval:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 10/14 [00:51<00:20,  5.07s/it]\u001b[A\n","eval:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 11/14 [00:57<00:16,  5.53s/it]\u001b[A\n","eval:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 12/14 [01:02<00:10,  5.26s/it]\u001b[A\n","eval:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 13/14 [01:07<00:05,  5.12s/it]\u001b[A\n","eval: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 14/14 [01:13<00:00,  5.22s/it]\n"," 40%|‚ñà‚ñà‚ñà‚ñà      | 2/5 [1:23:21<2:04:58, 2499.54s/it]"]},{"name":"stdout","output_type":"stream","text":["epoch 1: train loss :: 1.654, train acc :: 0.273, dev acc :: 0.253\n"]},{"name":"stderr","output_type":"stream","text":["\n","train-2:   0%|          | 0/107 [00:00<?, ?it/s]\u001b[A\n","train-2:   1%|          | 1/107 [00:16<29:51, 16.90s/it]\u001b[A\n","train-2:   2%|‚ñè         | 2/107 [00:31<27:32, 15.74s/it]\u001b[A\n","train-2:   3%|‚ñé         | 3/107 [00:47<27:32, 15.89s/it]\u001b[A\n","train-2:   4%|‚ñé         | 4/107 [01:05<28:08, 16.39s/it]\u001b[A\n","train-2:   5%|‚ñç         | 5/107 [01:21<27:48, 16.36s/it]\u001b[A\n","train-2:   6%|‚ñå         | 6/107 [01:36<26:34, 15.78s/it]\u001b[A\n","train-2:   7%|‚ñã         | 7/107 [01:54<27:29, 16.50s/it]\u001b[A\n","train-2:   7%|‚ñã         | 8/107 [02:07<25:38, 15.54s/it]\u001b[A\n","train-2:   8%|‚ñä         | 9/107 [02:25<26:48, 16.41s/it]\u001b[A\n","train-2:   9%|‚ñâ         | 10/107 [02:41<26:22, 16.32s/it]\u001b[A\n","train-2:  10%|‚ñà         | 11/107 [02:57<25:55, 16.21s/it]\u001b[A\n","train-2:  11%|‚ñà         | 12/107 [03:15<26:29, 16.73s/it]\u001b[A\n","train-2:  12%|‚ñà‚ñè        | 13/107 [03:31<25:48, 16.47s/it]\u001b[A\n","train-2:  13%|‚ñà‚ñé        | 14/107 [03:52<27:21, 17.65s/it]\u001b[A\n","train-2:  14%|‚ñà‚ñç        | 15/107 [04:07<26:09, 17.06s/it]\u001b[A\n","train-2:  15%|‚ñà‚ñç        | 16/107 [04:23<25:25, 16.77s/it]\u001b[A\n","train-2:  16%|‚ñà‚ñå        | 17/107 [04:40<25:06, 16.74s/it]\u001b[A\n","train-2:  17%|‚ñà‚ñã        | 18/107 [04:52<22:47, 15.36s/it]\u001b[A\n","train-2:  18%|‚ñà‚ñä        | 19/107 [05:11<24:00, 16.37s/it]\u001b[A\n","train-2:  19%|‚ñà‚ñä        | 20/107 [05:27<23:43, 16.36s/it]\u001b[A\n","train-2:  20%|‚ñà‚ñâ        | 21/107 [05:42<22:42, 15.84s/it]\u001b[A\n","train-2:  21%|‚ñà‚ñà        | 22/107 [05:58<22:45, 16.07s/it]\u001b[A\n","train-2:  21%|‚ñà‚ñà‚ñè       | 23/107 [06:15<22:47, 16.28s/it]\u001b[A\n","train-2:  22%|‚ñà‚ñà‚ñè       | 24/107 [06:33<23:20, 16.88s/it]\u001b[A\n","train-2:  23%|‚ñà‚ñà‚ñé       | 25/107 [06:51<23:16, 17.03s/it]\u001b[A\n","train-2:  24%|‚ñà‚ñà‚ñç       | 26/107 [07:07<22:43, 16.83s/it]\u001b[A\n","train-2:  25%|‚ñà‚ñà‚ñå       | 27/107 [07:24<22:34, 16.94s/it]\u001b[A\n","train-2:  26%|‚ñà‚ñà‚ñå       | 28/107 [07:40<21:56, 16.67s/it]\u001b[A\n","train-2:  27%|‚ñà‚ñà‚ñã       | 29/107 [07:58<22:07, 17.02s/it]\u001b[A\n","train-2:  28%|‚ñà‚ñà‚ñä       | 30/107 [08:14<21:14, 16.55s/it]\u001b[A\n","train-2:  29%|‚ñà‚ñà‚ñâ       | 31/107 [08:30<21:00, 16.58s/it]\u001b[A\n","train-2:  30%|‚ñà‚ñà‚ñâ       | 32/107 [08:49<21:33, 17.25s/it]\u001b[A\n","train-2:  31%|‚ñà‚ñà‚ñà       | 33/107 [09:07<21:22, 17.33s/it]\u001b[A\n","train-2:  32%|‚ñà‚ñà‚ñà‚ñè      | 34/107 [09:22<20:27, 16.82s/it]\u001b[A\n","train-2:  33%|‚ñà‚ñà‚ñà‚ñé      | 35/107 [09:36<18:55, 15.76s/it]\u001b[A\n","train-2:  34%|‚ñà‚ñà‚ñà‚ñé      | 36/107 [09:50<18:05, 15.29s/it]\u001b[A\n","train-2:  35%|‚ñà‚ñà‚ñà‚ñç      | 37/107 [10:09<19:03, 16.34s/it]\u001b[A\n","train-2:  36%|‚ñà‚ñà‚ñà‚ñå      | 38/107 [10:25<18:49, 16.37s/it]\u001b[A\n","train-2:  36%|‚ñà‚ñà‚ñà‚ñã      | 39/107 [10:42<18:38, 16.45s/it]\u001b[A\n","train-2:  37%|‚ñà‚ñà‚ñà‚ñã      | 40/107 [11:01<19:09, 17.16s/it]\u001b[A\n","train-2:  38%|‚ñà‚ñà‚ñà‚ñä      | 41/107 [11:14<17:45, 16.15s/it]\u001b[A\n","train-2:  39%|‚ñà‚ñà‚ñà‚ñâ      | 42/107 [11:31<17:45, 16.39s/it]\u001b[A\n","train-2:  40%|‚ñà‚ñà‚ñà‚ñà      | 43/107 [11:46<16:52, 15.82s/it]\u001b[A\n","train-2:  41%|‚ñà‚ñà‚ñà‚ñà      | 44/107 [12:00<16:02, 15.28s/it]\u001b[A\n","train-2:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 45/107 [12:17<16:16, 15.75s/it]\u001b[A\n","train-2:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 46/107 [12:36<17:14, 16.96s/it]\u001b[A\n","train-2:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 47/107 [12:55<17:18, 17.30s/it]\u001b[A\n","train-2:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 48/107 [13:13<17:19, 17.62s/it]\u001b[A\n","train-2:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 49/107 [13:31<17:03, 17.64s/it]\u001b[A\n","train-2:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 50/107 [13:47<16:16, 17.13s/it]\u001b[A\n","train-2:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 51/107 [14:03<15:48, 16.94s/it]\u001b[A\n","train-2:  49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 52/107 [14:21<15:52, 17.32s/it]\u001b[A\n","train-2:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 53/107 [14:36<14:49, 16.48s/it]\u001b[A\n","train-2:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 54/107 [14:52<14:22, 16.27s/it]\u001b[A\n","train-2:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 55/107 [15:08<14:13, 16.42s/it]\u001b[A\n","train-2:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 56/107 [15:23<13:27, 15.83s/it]\u001b[A\n","train-2:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 57/107 [15:41<13:52, 16.65s/it]\u001b[A\n","train-2:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 58/107 [15:58<13:31, 16.56s/it]\u001b[A\n","train-2:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 59/107 [16:14<13:12, 16.51s/it]\u001b[A\n","train-2:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 60/107 [16:40<15:15, 19.47s/it]\u001b[A\n","train-2:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 61/107 [16:55<13:55, 18.15s/it]\u001b[A\n","train-2:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 62/107 [17:11<12:58, 17.30s/it]\u001b[A\n","train-2:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 63/107 [17:26<12:11, 16.62s/it]\u001b[A\n","train-2:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 64/107 [17:43<11:56, 16.66s/it]\u001b[A\n","train-2:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 65/107 [17:59<11:30, 16.44s/it]\u001b[A\n","train-2:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 66/107 [18:12<10:39, 15.60s/it]\u001b[A\n","train-2:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 67/107 [18:30<10:48, 16.22s/it]\u001b[A\n","train-2:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 68/107 [18:46<10:34, 16.27s/it]\u001b[A\n","train-2:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 69/107 [19:05<10:46, 17.02s/it]\u001b[A\n","train-2:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 70/107 [19:22<10:31, 17.06s/it]\u001b[A\n","train-2:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 71/107 [19:39<10:13, 17.04s/it]\u001b[A\n","train-2:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 72/107 [19:58<10:19, 17.69s/it]\u001b[A\n","train-2:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 73/107 [20:16<09:56, 17.54s/it]\u001b[A\n","train-2:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 74/107 [20:38<10:29, 19.08s/it]\u001b[A\n","train-2:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 75/107 [20:58<10:12, 19.15s/it]\u001b[A\n","train-2:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 76/107 [21:12<09:10, 17.76s/it]\u001b[A\n","train-2:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 77/107 [21:30<08:59, 17.97s/it]\u001b[A\n","train-2:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 78/107 [21:48<08:39, 17.90s/it]\u001b[A\n","train-2:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 79/107 [22:03<07:53, 16.90s/it]\u001b[A\n","train-2:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 80/107 [22:20<07:35, 16.86s/it]\u001b[A\n","train-2:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 81/107 [22:34<07:03, 16.27s/it]\u001b[A\n","train-2:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 82/107 [22:53<07:07, 17.09s/it]\u001b[A\n","train-2:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 83/107 [23:11<06:50, 17.10s/it]\u001b[A\n","train-2:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 84/107 [23:26<06:22, 16.64s/it]\u001b[A\n","train-2:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 85/107 [23:44<06:11, 16.91s/it]\u001b[A\n","train-2:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 86/107 [23:59<05:47, 16.55s/it]\u001b[A\n","train-2:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 87/107 [24:16<05:31, 16.55s/it]\u001b[A\n","train-2:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 88/107 [24:34<05:22, 16.98s/it]\u001b[A\n","train-2:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 89/107 [24:53<05:16, 17.59s/it]\u001b[A\n","train-2:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 90/107 [25:10<04:55, 17.40s/it]\u001b[A\n","train-2:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 91/107 [25:26<04:34, 17.13s/it]\u001b[A\n","train-2:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 92/107 [25:44<04:18, 17.25s/it]\u001b[A\n","train-2:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 93/107 [25:59<03:53, 16.71s/it]\u001b[A\n","train-2:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 94/107 [26:21<03:55, 18.15s/it]\u001b[A\n","train-2:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 95/107 [26:36<03:26, 17.25s/it]\u001b[A\n","train-2:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 96/107 [26:52<03:05, 16.82s/it]\u001b[A\n","train-2:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 97/107 [27:07<02:43, 16.37s/it]\u001b[A\n","train-2:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 98/107 [27:24<02:28, 16.47s/it]\u001b[A\n","train-2:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 99/107 [27:42<02:14, 16.85s/it]\u001b[A\n","train-2:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 100/107 [27:57<01:54, 16.31s/it]\u001b[A\n","train-2:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 101/107 [28:12<01:36, 16.04s/it]\u001b[A\n","train-2:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 102/107 [28:27<01:18, 15.69s/it]\u001b[A\n","train-2:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 103/107 [28:43<01:03, 15.81s/it]\u001b[A\n","train-2:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 104/107 [29:02<00:49, 16.66s/it]\u001b[A\n","train-2:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 105/107 [29:17<00:32, 16.30s/it]\u001b[A\n","train-2:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 106/107 [29:37<00:17, 17.21s/it]\u001b[A\n","train-2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 107/107 [29:50<00:00, 16.73s/it]\n","\n","eval:   0%|          | 0/107 [00:00<?, ?it/s]\u001b[A\n","eval:   1%|          | 1/107 [00:04<07:53,  4.47s/it]\u001b[A\n","eval:   2%|‚ñè         | 2/107 [00:10<09:10,  5.25s/it]\u001b[A\n","eval:   3%|‚ñé         | 3/107 [00:15<09:17,  5.36s/it]\u001b[A\n","eval:   4%|‚ñé         | 4/107 [00:20<08:31,  4.97s/it]\u001b[A\n","eval:   5%|‚ñç         | 5/107 [00:27<09:38,  5.67s/it]\u001b[A\n","eval:   6%|‚ñå         | 6/107 [00:32<09:10,  5.45s/it]\u001b[A\n","eval:   7%|‚ñã         | 7/107 [00:37<08:58,  5.38s/it]\u001b[A\n","eval:   7%|‚ñã         | 8/107 [00:44<09:44,  5.91s/it]\u001b[A\n","eval:   8%|‚ñä         | 9/107 [00:49<09:10,  5.61s/it]\u001b[A\n","eval:   9%|‚ñâ         | 10/107 [00:55<09:33,  5.92s/it]\u001b[A\n","eval:  10%|‚ñà         | 11/107 [01:01<09:10,  5.74s/it]\u001b[A\n","eval:  11%|‚ñà         | 12/107 [01:06<08:40,  5.48s/it]\u001b[A\n","eval:  12%|‚ñà‚ñè        | 13/107 [01:12<08:59,  5.74s/it]\u001b[A\n","eval:  13%|‚ñà‚ñé        | 14/107 [01:16<08:08,  5.26s/it]\u001b[A\n","eval:  14%|‚ñà‚ñç        | 15/107 [01:21<07:52,  5.13s/it]\u001b[A\n","eval:  15%|‚ñà‚ñç        | 16/107 [01:27<08:05,  5.33s/it]\u001b[A\n","eval:  16%|‚ñà‚ñå        | 17/107 [01:34<08:45,  5.83s/it]\u001b[A\n","eval:  17%|‚ñà‚ñã        | 18/107 [01:39<08:33,  5.77s/it]\u001b[A\n","eval:  18%|‚ñà‚ñä        | 19/107 [01:44<08:09,  5.56s/it]\u001b[A\n","eval:  19%|‚ñà‚ñä        | 20/107 [01:49<07:26,  5.13s/it]\u001b[A\n","eval:  20%|‚ñà‚ñâ        | 21/107 [01:55<07:55,  5.53s/it]\u001b[A\n","eval:  21%|‚ñà‚ñà        | 22/107 [02:00<07:46,  5.49s/it]\u001b[A\n","eval:  21%|‚ñà‚ñà‚ñè       | 23/107 [02:05<07:24,  5.29s/it]\u001b[A\n","eval:  22%|‚ñà‚ñà‚ñè       | 24/107 [02:11<07:32,  5.46s/it]\u001b[A\n","eval:  23%|‚ñà‚ñà‚ñé       | 25/107 [02:16<07:15,  5.31s/it]\u001b[A\n","eval:  24%|‚ñà‚ñà‚ñç       | 26/107 [02:22<07:28,  5.54s/it]\u001b[A\n","eval:  25%|‚ñà‚ñà‚ñå       | 27/107 [02:28<07:30,  5.63s/it]\u001b[A\n","eval:  26%|‚ñà‚ñà‚ñå       | 28/107 [02:34<07:43,  5.87s/it]\u001b[A\n","eval:  27%|‚ñà‚ñà‚ñã       | 29/107 [02:41<07:48,  6.01s/it]\u001b[A\n","eval:  28%|‚ñà‚ñà‚ñä       | 30/107 [02:45<07:13,  5.63s/it]\u001b[A\n","eval:  29%|‚ñà‚ñà‚ñâ       | 31/107 [02:51<07:13,  5.70s/it]\u001b[A\n","eval:  30%|‚ñà‚ñà‚ñâ       | 32/107 [02:57<07:01,  5.62s/it]\u001b[A\n","eval:  31%|‚ñà‚ñà‚ñà       | 33/107 [03:01<06:27,  5.24s/it]\u001b[A\n","eval:  32%|‚ñà‚ñà‚ñà‚ñè      | 34/107 [03:07<06:37,  5.45s/it]\u001b[A\n","eval:  33%|‚ñà‚ñà‚ñà‚ñé      | 35/107 [03:12<06:19,  5.28s/it]\u001b[A\n","eval:  34%|‚ñà‚ñà‚ñà‚ñé      | 36/107 [03:17<06:13,  5.26s/it]\u001b[A\n","eval:  35%|‚ñà‚ñà‚ñà‚ñç      | 37/107 [03:23<06:26,  5.52s/it]\u001b[A\n","eval:  36%|‚ñà‚ñà‚ñà‚ñå      | 38/107 [03:31<07:01,  6.11s/it]\u001b[A\n","eval:  36%|‚ñà‚ñà‚ñà‚ñã      | 39/107 [03:38<07:10,  6.33s/it]\u001b[A\n","eval:  37%|‚ñà‚ñà‚ñà‚ñã      | 40/107 [03:43<06:38,  5.95s/it]\u001b[A\n","eval:  38%|‚ñà‚ñà‚ñà‚ñä      | 41/107 [03:47<06:04,  5.52s/it]\u001b[A\n","eval:  39%|‚ñà‚ñà‚ñà‚ñâ      | 42/107 [03:54<06:13,  5.75s/it]\u001b[A\n","eval:  40%|‚ñà‚ñà‚ñà‚ñà      | 43/107 [03:59<05:54,  5.54s/it]\u001b[A\n","eval:  41%|‚ñà‚ñà‚ñà‚ñà      | 44/107 [04:04<05:51,  5.58s/it]\u001b[A\n","eval:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 45/107 [04:10<05:54,  5.72s/it]\u001b[A\n","eval:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 46/107 [04:17<06:10,  6.07s/it]\u001b[A\n","eval:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 47/107 [04:24<06:12,  6.21s/it]\u001b[A\n","eval:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 48/107 [04:28<05:38,  5.73s/it]\u001b[A\n","eval:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 49/107 [04:34<05:35,  5.78s/it]\u001b[A\n","eval:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 50/107 [04:40<05:23,  5.68s/it]\u001b[A\n","eval:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 51/107 [04:44<05:01,  5.39s/it]\u001b[A\n","eval:  49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 52/107 [04:52<05:29,  6.00s/it]\u001b[A\n","eval:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 53/107 [04:57<05:16,  5.87s/it]\u001b[A\n","eval:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 54/107 [05:02<04:54,  5.55s/it]\u001b[A\n","eval:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 55/107 [05:08<04:45,  5.50s/it]\u001b[A\n","eval:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 56/107 [05:12<04:23,  5.18s/it]\u001b[A\n","eval:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 57/107 [05:17<04:15,  5.12s/it]\u001b[A\n","eval:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 58/107 [05:23<04:17,  5.26s/it]\u001b[A\n","eval:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 59/107 [05:27<04:01,  5.04s/it]\u001b[A\n","eval:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 60/107 [05:34<04:22,  5.59s/it]\u001b[A\n","eval:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 61/107 [05:39<04:03,  5.30s/it]\u001b[A\n","eval:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 62/107 [05:44<03:55,  5.23s/it]\u001b[A\n","eval:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 63/107 [05:50<04:11,  5.71s/it]\u001b[A\n","eval:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 64/107 [05:56<04:04,  5.69s/it]\u001b[A\n","eval:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 65/107 [06:02<04:00,  5.74s/it]\u001b[A\n","eval:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 66/107 [06:07<03:52,  5.67s/it]\u001b[A\n","eval:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 67/107 [06:12<03:33,  5.34s/it]\u001b[A\n","eval:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 68/107 [06:19<03:45,  5.77s/it]\u001b[A\n","eval:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 69/107 [06:23<03:27,  5.45s/it]\u001b[A\n","eval:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 70/107 [06:28<03:09,  5.11s/it]\u001b[A\n","eval:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 71/107 [06:35<03:26,  5.75s/it]\u001b[A\n","eval:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 72/107 [06:40<03:14,  5.55s/it]\u001b[A\n","eval:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 73/107 [06:45<03:06,  5.48s/it]\u001b[A\n","eval:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 74/107 [06:51<02:58,  5.41s/it]\u001b[A\n","eval:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 75/107 [06:56<02:50,  5.33s/it]\u001b[A\n","eval:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 76/107 [07:03<03:02,  5.89s/it]\u001b[A\n","eval:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 77/107 [07:08<02:50,  5.68s/it]\u001b[A\n","eval:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 78/107 [07:12<02:32,  5.26s/it]\u001b[A\n","eval:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 79/107 [07:18<02:33,  5.48s/it]\u001b[A\n","eval:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 80/107 [07:23<02:23,  5.31s/it]\u001b[A\n","eval:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 81/107 [07:30<02:26,  5.62s/it]\u001b[A\n","eval:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 82/107 [07:35<02:20,  5.62s/it]\u001b[A\n","eval:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 83/107 [07:40<02:10,  5.44s/it]\u001b[A\n","eval:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 84/107 [07:47<02:15,  5.87s/it]\u001b[A\n","eval:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 85/107 [07:52<02:04,  5.67s/it]\u001b[A\n","eval:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 86/107 [07:59<02:02,  5.82s/it]\u001b[A\n","eval:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 87/107 [08:04<01:56,  5.83s/it]\u001b[A\n","eval:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 88/107 [08:10<01:51,  5.89s/it]\u001b[A\n","eval:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 89/107 [08:18<01:53,  6.32s/it]\u001b[A\n","eval:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 90/107 [08:22<01:37,  5.74s/it]\u001b[A\n","eval:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 91/107 [08:28<01:31,  5.74s/it]\u001b[A\n","eval:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 92/107 [08:34<01:28,  5.87s/it]\u001b[A\n","eval:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 93/107 [08:39<01:18,  5.63s/it]\u001b[A\n","eval:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 94/107 [08:46<01:17,  5.96s/it]\u001b[A\n","eval:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 95/107 [08:52<01:10,  5.87s/it]\u001b[A\n","eval:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 96/107 [09:00<01:13,  6.70s/it]\u001b[A\n","eval:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 97/107 [09:06<01:03,  6.34s/it]\u001b[A\n","eval:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 98/107 [09:10<00:51,  5.73s/it]\u001b[A\n","eval:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 99/107 [09:16<00:47,  5.89s/it]\u001b[A\n","eval:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 100/107 [09:21<00:38,  5.49s/it]\u001b[A\n","eval:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 101/107 [09:27<00:33,  5.56s/it]\u001b[A\n","eval:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 102/107 [09:32<00:28,  5.65s/it]\u001b[A\n","eval:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 103/107 [09:38<00:22,  5.66s/it]\u001b[A\n","eval:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 104/107 [09:44<00:17,  5.84s/it]\u001b[A\n","eval:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 105/107 [09:50<00:11,  5.70s/it]\u001b[A\n","eval:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 106/107 [09:55<00:05,  5.67s/it]\u001b[A\n","eval: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 107/107 [10:01<00:00,  5.62s/it]\n","\n","eval:   0%|          | 0/14 [00:00<?, ?it/s]\u001b[A\n","eval:   7%|‚ñã         | 1/14 [00:04<00:52,  4.07s/it]\u001b[A\n","eval:  14%|‚ñà‚ñç        | 2/14 [00:08<00:53,  4.43s/it]\u001b[A\n","eval:  21%|‚ñà‚ñà‚ñè       | 3/14 [00:14<00:57,  5.19s/it]\u001b[A\n","eval:  29%|‚ñà‚ñà‚ñä       | 4/14 [00:20<00:53,  5.31s/it]\u001b[A\n","eval:  36%|‚ñà‚ñà‚ñà‚ñå      | 5/14 [00:25<00:47,  5.31s/it]\u001b[A\n","eval:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 6/14 [00:31<00:43,  5.38s/it]\u001b[A\n","eval:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 7/14 [00:35<00:36,  5.20s/it]\u001b[A\n","eval:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 8/14 [00:42<00:32,  5.50s/it]\u001b[A\n","eval:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 9/14 [00:46<00:26,  5.26s/it]\u001b[A\n","eval:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 10/14 [00:51<00:20,  5.06s/it]\u001b[A\n","eval:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 11/14 [00:58<00:16,  5.53s/it]\u001b[A\n","eval:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 12/14 [01:02<00:10,  5.26s/it]\u001b[A\n","eval:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 13/14 [01:07<00:05,  5.15s/it]\u001b[A\n","eval: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 14/14 [01:13<00:00,  5.22s/it]\n"," 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 3/5 [2:04:34<1:22:54, 2487.15s/it]"]},{"name":"stdout","output_type":"stream","text":["save the model to finetune-5-2e-05.pt\n","epoch 2: train loss :: 1.558, train acc :: 0.401, dev acc :: 0.361\n"]},{"name":"stderr","output_type":"stream","text":["\n","train-3:   0%|          | 0/107 [00:00<?, ?it/s]\u001b[A\n","train-3:   1%|          | 1/107 [00:17<30:15, 17.12s/it]\u001b[A\n","train-3:   2%|‚ñè         | 2/107 [00:32<28:00, 16.00s/it]\u001b[A\n","train-3:   3%|‚ñé         | 3/107 [00:49<28:57, 16.70s/it]\u001b[A\n","train-3:   4%|‚ñé         | 4/107 [01:07<29:05, 16.95s/it]\u001b[A\n","train-3:   5%|‚ñç         | 5/107 [01:25<29:39, 17.45s/it]\u001b[A\n","train-3:   6%|‚ñå         | 6/107 [01:41<28:34, 16.98s/it]\u001b[A\n","train-3:   7%|‚ñã         | 7/107 [01:55<26:47, 16.08s/it]\u001b[A\n","train-3:   7%|‚ñã         | 8/107 [02:12<26:53, 16.30s/it]\u001b[A\n","train-3:   8%|‚ñä         | 9/107 [02:33<29:07, 17.83s/it]\u001b[A\n","train-3:   9%|‚ñâ         | 10/107 [02:47<26:54, 16.64s/it]\u001b[A\n","train-3:  10%|‚ñà         | 11/107 [03:07<27:58, 17.49s/it]\u001b[A\n","train-3:  11%|‚ñà         | 12/107 [03:21<26:03, 16.46s/it]\u001b[A\n","train-3:  12%|‚ñà‚ñè        | 13/107 [03:39<26:27, 16.89s/it]\u001b[A\n","train-3:  13%|‚ñà‚ñé        | 14/107 [03:55<26:05, 16.83s/it]\u001b[A\n","train-3:  14%|‚ñà‚ñç        | 15/107 [04:15<27:09, 17.71s/it]\u001b[A\n","train-3:  15%|‚ñà‚ñç        | 16/107 [04:30<25:35, 16.87s/it]\u001b[A\n","train-3:  16%|‚ñà‚ñå        | 17/107 [04:49<26:16, 17.52s/it]\u001b[A\n","train-3:  17%|‚ñà‚ñã        | 18/107 [05:04<25:02, 16.88s/it]\u001b[A\n","train-3:  18%|‚ñà‚ñä        | 19/107 [05:21<24:28, 16.69s/it]\u001b[A\n","train-3:  19%|‚ñà‚ñä        | 20/107 [05:38<24:31, 16.92s/it]\u001b[A\n","train-3:  20%|‚ñà‚ñâ        | 21/107 [05:54<23:39, 16.50s/it]\u001b[A\n","train-3:  21%|‚ñà‚ñà        | 22/107 [06:11<23:38, 16.68s/it]\u001b[A\n","train-3:  21%|‚ñà‚ñà‚ñè       | 23/107 [06:26<22:46, 16.27s/it]\u001b[A\n","train-3:  22%|‚ñà‚ñà‚ñè       | 24/107 [06:44<23:07, 16.72s/it]\u001b[A\n","train-3:  23%|‚ñà‚ñà‚ñé       | 25/107 [07:02<23:34, 17.25s/it]\u001b[A\n","train-3:  24%|‚ñà‚ñà‚ñç       | 26/107 [07:22<24:16, 17.98s/it]\u001b[A\n","train-3:  25%|‚ñà‚ñà‚ñå       | 27/107 [07:40<23:49, 17.86s/it]\u001b[A\n","train-3:  26%|‚ñà‚ñà‚ñå       | 28/107 [07:55<22:40, 17.23s/it]\u001b[A\n","train-3:  27%|‚ñà‚ñà‚ñã       | 29/107 [08:10<21:11, 16.31s/it]\u001b[A\n","train-3:  28%|‚ñà‚ñà‚ñä       | 30/107 [08:25<20:25, 15.92s/it]\u001b[A\n","train-3:  29%|‚ñà‚ñà‚ñâ       | 31/107 [08:42<20:40, 16.32s/it]\u001b[A\n","train-3:  30%|‚ñà‚ñà‚ñâ       | 32/107 [08:57<20:01, 16.03s/it]\u001b[A\n","train-3:  31%|‚ñà‚ñà‚ñà       | 33/107 [09:14<20:10, 16.36s/it]\u001b[A\n","train-3:  32%|‚ñà‚ñà‚ñà‚ñè      | 34/107 [09:32<20:28, 16.83s/it]\u001b[A\n","train-3:  33%|‚ñà‚ñà‚ñà‚ñé      | 35/107 [09:50<20:40, 17.23s/it]\u001b[A\n","train-3:  34%|‚ñà‚ñà‚ñà‚ñé      | 36/107 [10:06<19:58, 16.87s/it]\u001b[A\n","train-3:  35%|‚ñà‚ñà‚ñà‚ñç      | 37/107 [10:23<19:39, 16.86s/it]\u001b[A\n","train-3:  36%|‚ñà‚ñà‚ñà‚ñå      | 38/107 [10:40<19:16, 16.77s/it]\u001b[A\n","train-3:  36%|‚ñà‚ñà‚ñà‚ñã      | 39/107 [10:55<18:25, 16.26s/it]\u001b[A\n","train-3:  37%|‚ñà‚ñà‚ñà‚ñã      | 40/107 [11:11<18:01, 16.15s/it]\u001b[A\n","train-3:  38%|‚ñà‚ñà‚ñà‚ñä      | 41/107 [11:28<18:05, 16.45s/it]\u001b[A\n","train-3:  39%|‚ñà‚ñà‚ñà‚ñâ      | 42/107 [11:47<18:39, 17.23s/it]\u001b[A\n","train-3:  40%|‚ñà‚ñà‚ñà‚ñà      | 43/107 [12:02<17:48, 16.69s/it]\u001b[A\n","train-3:  41%|‚ñà‚ñà‚ñà‚ñà      | 44/107 [12:20<17:49, 16.98s/it]\u001b[A\n","train-3:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 45/107 [12:35<16:55, 16.38s/it]\u001b[A\n","train-3:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 46/107 [12:50<16:12, 15.94s/it]\u001b[A\n","train-3:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 47/107 [13:06<15:55, 15.93s/it]\u001b[A\n","train-3:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 48/107 [13:23<16:03, 16.33s/it]\u001b[A\n","train-3:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 49/107 [13:39<15:40, 16.22s/it]\u001b[A\n","train-3:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 50/107 [13:59<16:27, 17.33s/it]\u001b[A\n","train-3:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 51/107 [14:15<15:52, 17.00s/it]\u001b[A\n","train-3:  49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 52/107 [14:33<15:45, 17.19s/it]\u001b[A\n","train-3:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 53/107 [14:49<15:07, 16.81s/it]\u001b[A\n","train-3:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 54/107 [15:03<14:14, 16.13s/it]\u001b[A\n","train-3:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 55/107 [15:22<14:31, 16.76s/it]\u001b[A\n","train-3:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 56/107 [15:38<14:09, 16.66s/it]\u001b[A\n","train-3:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 57/107 [15:57<14:26, 17.33s/it]\u001b[A\n","train-3:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 58/107 [16:13<13:56, 17.06s/it]\u001b[A\n","train-3:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 59/107 [16:32<14:07, 17.66s/it]\u001b[A\n","train-3:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 60/107 [16:49<13:32, 17.28s/it]\u001b[A\n","train-3:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 61/107 [17:06<13:17, 17.34s/it]\u001b[A\n","train-3:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 62/107 [17:23<12:50, 17.12s/it]\u001b[A\n","train-3:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 63/107 [17:40<12:27, 16.98s/it]\u001b[A\n","train-3:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 64/107 [17:55<11:48, 16.47s/it]\u001b[A\n","train-3:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 65/107 [18:12<11:35, 16.55s/it]\u001b[A\n","train-3:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 66/107 [18:27<11:00, 16.10s/it]\u001b[A\n","train-3:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 67/107 [18:42<10:35, 15.90s/it]\u001b[A\n","train-3:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 68/107 [18:57<10:10, 15.65s/it]\u001b[A\n","train-3:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 69/107 [19:13<09:56, 15.70s/it]\u001b[A\n","train-3:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 70/107 [19:26<09:11, 14.90s/it]\u001b[A\n","train-3:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 71/107 [19:43<09:16, 15.46s/it]\u001b[A\n","train-3:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 72/107 [20:09<10:49, 18.56s/it]\u001b[A\n","train-3:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 73/107 [20:24<09:55, 17.51s/it]\u001b[A\n","train-3:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 74/107 [20:40<09:28, 17.22s/it]\u001b[A\n","train-3:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 75/107 [20:57<09:07, 17.11s/it]\u001b[A\n","train-3:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 76/107 [21:14<08:52, 17.16s/it]\u001b[A\n","train-3:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 77/107 [21:34<09:01, 18.04s/it]\u001b[A\n","train-3:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 78/107 [21:51<08:32, 17.68s/it]\u001b[A\n","train-3:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 79/107 [22:09<08:13, 17.61s/it]\u001b[A\n","train-3:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 80/107 [22:24<07:35, 16.86s/it]\u001b[A\n","train-3:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 81/107 [22:39<07:06, 16.42s/it]\u001b[A\n","train-3:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 82/107 [22:55<06:49, 16.37s/it]\u001b[A\n","train-3:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 83/107 [23:15<06:54, 17.28s/it]\u001b[A\n","train-3:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 84/107 [23:32<06:35, 17.21s/it]\u001b[A\n","train-3:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 85/107 [23:48<06:13, 17.00s/it]\u001b[A\n","train-3:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 86/107 [24:05<05:54, 16.90s/it]\u001b[A\n","train-3:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 87/107 [24:20<05:26, 16.35s/it]\u001b[A\n","train-3:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 88/107 [24:37<05:13, 16.51s/it]\u001b[A\n","train-3:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 89/107 [24:53<04:54, 16.36s/it]\u001b[A\n","train-3:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 90/107 [25:13<04:54, 17.35s/it]\u001b[A\n","train-3:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 91/107 [25:29<04:32, 17.01s/it]\u001b[A\n","train-3:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 92/107 [25:42<03:59, 16.00s/it]\u001b[A\n","train-3:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 93/107 [26:00<03:49, 16.37s/it]\u001b[A\n","train-3:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 94/107 [26:18<03:39, 16.90s/it]\u001b[A\n","train-3:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 95/107 [26:35<03:24, 17.04s/it]\u001b[A\n","train-3:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 96/107 [27:01<03:37, 19.78s/it]\u001b[A\n","train-3:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 97/107 [27:15<03:00, 18.02s/it]\u001b[A\n","train-3:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 98/107 [27:31<02:34, 17.21s/it]\u001b[A\n","train-3:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 99/107 [27:43<02:05, 15.73s/it]\u001b[A\n","train-3:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 100/107 [28:00<01:53, 16.21s/it]\u001b[A\n","train-3:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 101/107 [28:16<01:36, 16.11s/it]\u001b[A\n","train-3:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 102/107 [28:33<01:21, 16.33s/it]\u001b[A\n","train-3:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 103/107 [28:50<01:06, 16.61s/it]\u001b[A\n","train-3:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 104/107 [29:04<00:47, 15.82s/it]\u001b[A\n","train-3:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 105/107 [29:21<00:32, 16.10s/it]\u001b[A\n","train-3:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 106/107 [29:39<00:16, 16.68s/it]\u001b[A\n","train-3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 107/107 [29:50<00:00, 16.73s/it]\n","\n","eval:   0%|          | 0/107 [00:00<?, ?it/s]\u001b[A\n","eval:   1%|          | 1/107 [00:06<11:09,  6.31s/it]\u001b[A\n","eval:   2%|‚ñè         | 2/107 [00:10<09:17,  5.31s/it]\u001b[A\n","eval:   3%|‚ñé         | 3/107 [00:19<11:41,  6.74s/it]\u001b[A\n","eval:   4%|‚ñé         | 4/107 [00:24<10:20,  6.02s/it]\u001b[A\n","eval:   5%|‚ñç         | 5/107 [00:29<09:26,  5.55s/it]\u001b[A\n","eval:   6%|‚ñå         | 6/107 [00:35<09:58,  5.93s/it]\u001b[A\n","eval:   7%|‚ñã         | 7/107 [00:40<09:08,  5.48s/it]\u001b[A\n","eval:   7%|‚ñã         | 8/107 [00:49<11:00,  6.67s/it]\u001b[A\n","eval:   8%|‚ñä         | 9/107 [00:54<09:57,  6.10s/it]\u001b[A\n","eval:   9%|‚ñâ         | 10/107 [00:58<09:04,  5.62s/it]\u001b[A\n","eval:  10%|‚ñà         | 11/107 [01:05<09:37,  6.02s/it]\u001b[A\n","eval:  11%|‚ñà         | 12/107 [01:10<09:03,  5.72s/it]\u001b[A\n","eval:  12%|‚ñà‚ñè        | 13/107 [01:17<09:31,  6.08s/it]\u001b[A\n","eval:  13%|‚ñà‚ñé        | 14/107 [01:24<09:38,  6.22s/it]\u001b[A\n","eval:  14%|‚ñà‚ñç        | 15/107 [01:28<08:49,  5.76s/it]\u001b[A\n","eval:  15%|‚ñà‚ñç        | 16/107 [01:35<09:15,  6.10s/it]\u001b[A\n","eval:  16%|‚ñà‚ñå        | 17/107 [01:41<08:52,  5.92s/it]\u001b[A\n","eval:  17%|‚ñà‚ñã        | 18/107 [01:45<08:06,  5.47s/it]\u001b[A\n","eval:  18%|‚ñà‚ñä        | 19/107 [01:52<08:41,  5.93s/it]\u001b[A\n","eval:  19%|‚ñà‚ñä        | 20/107 [01:57<08:08,  5.61s/it]\u001b[A\n","eval:  20%|‚ñà‚ñâ        | 21/107 [02:02<07:53,  5.51s/it]\u001b[A\n","eval:  21%|‚ñà‚ñà        | 22/107 [02:10<08:33,  6.04s/it]\u001b[A\n","eval:  21%|‚ñà‚ñà‚ñè       | 23/107 [02:15<07:56,  5.68s/it]\u001b[A\n","eval:  22%|‚ñà‚ñà‚ñè       | 24/107 [02:21<08:19,  6.02s/it]\u001b[A\n","eval:  23%|‚ñà‚ñà‚ñé       | 25/107 [02:26<07:37,  5.58s/it]\u001b[A\n","eval:  24%|‚ñà‚ñà‚ñç       | 26/107 [02:31<07:13,  5.35s/it]\u001b[A\n","eval:  25%|‚ñà‚ñà‚ñå       | 27/107 [02:37<07:40,  5.76s/it]\u001b[A\n","eval:  26%|‚ñà‚ñà‚ñå       | 28/107 [02:42<07:08,  5.42s/it]\u001b[A\n","eval:  27%|‚ñà‚ñà‚ñã       | 29/107 [02:47<06:46,  5.22s/it]\u001b[A\n","eval:  28%|‚ñà‚ñà‚ñä       | 30/107 [02:53<07:02,  5.49s/it]\u001b[A\n","eval:  29%|‚ñà‚ñà‚ñâ       | 31/107 [02:58<06:47,  5.37s/it]\u001b[A\n","eval:  30%|‚ñà‚ñà‚ñâ       | 32/107 [03:05<07:17,  5.84s/it]\u001b[A\n","eval:  31%|‚ñà‚ñà‚ñà       | 33/107 [03:10<07:00,  5.68s/it]\u001b[A\n","eval:  32%|‚ñà‚ñà‚ñà‚ñè      | 34/107 [03:15<06:30,  5.35s/it]\u001b[A\n","eval:  33%|‚ñà‚ñà‚ñà‚ñé      | 35/107 [03:21<06:43,  5.60s/it]\u001b[A\n","eval:  34%|‚ñà‚ñà‚ñà‚ñé      | 36/107 [03:25<06:07,  5.18s/it]\u001b[A\n","eval:  35%|‚ñà‚ñà‚ñà‚ñç      | 37/107 [03:29<05:38,  4.83s/it]\u001b[A\n","eval:  36%|‚ñà‚ñà‚ñà‚ñå      | 38/107 [03:36<06:11,  5.39s/it]\u001b[A\n","eval:  36%|‚ñà‚ñà‚ñà‚ñã      | 39/107 [03:41<06:05,  5.37s/it]\u001b[A\n","eval:  37%|‚ñà‚ñà‚ñà‚ñã      | 40/107 [03:46<05:55,  5.30s/it]\u001b[A\n","eval:  38%|‚ñà‚ñà‚ñà‚ñä      | 41/107 [03:53<06:07,  5.57s/it]\u001b[A\n","eval:  39%|‚ñà‚ñà‚ñà‚ñâ      | 42/107 [03:58<05:52,  5.42s/it]\u001b[A\n","eval:  40%|‚ñà‚ñà‚ñà‚ñà      | 43/107 [04:04<06:12,  5.83s/it]\u001b[A\n","eval:  41%|‚ñà‚ñà‚ñà‚ñà      | 44/107 [04:09<05:34,  5.31s/it]\u001b[A\n","eval:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 45/107 [04:14<05:26,  5.27s/it]\u001b[A\n","eval:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 46/107 [04:20<05:31,  5.43s/it]\u001b[A\n","eval:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 47/107 [04:25<05:29,  5.48s/it]\u001b[A\n","eval:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 48/107 [04:31<05:30,  5.61s/it]\u001b[A\n","eval:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 49/107 [04:37<05:23,  5.58s/it]\u001b[A\n","eval:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 50/107 [04:41<05:01,  5.29s/it]\u001b[A\n","eval:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 51/107 [04:49<05:31,  5.91s/it]\u001b[A\n","eval:  49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 52/107 [04:53<04:53,  5.34s/it]\u001b[A\n","eval:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 53/107 [04:57<04:35,  5.10s/it]\u001b[A\n","eval:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 54/107 [05:04<04:53,  5.53s/it]\u001b[A\n","eval:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 55/107 [05:09<04:40,  5.39s/it]\u001b[A\n","eval:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 56/107 [05:16<05:04,  5.98s/it]\u001b[A\n","eval:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 57/107 [05:21<04:45,  5.70s/it]\u001b[A\n","eval:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 58/107 [05:26<04:22,  5.36s/it]\u001b[A\n","eval:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 59/107 [05:32<04:29,  5.62s/it]\u001b[A\n","eval:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 60/107 [05:37<04:10,  5.34s/it]\u001b[A\n","eval:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 61/107 [05:43<04:17,  5.59s/it]\u001b[A\n","eval:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 62/107 [05:48<04:13,  5.63s/it]\u001b[A\n","eval:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 63/107 [05:52<03:45,  5.12s/it]\u001b[A\n","eval:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 64/107 [05:58<03:45,  5.23s/it]\u001b[A\n","eval:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 65/107 [06:03<03:38,  5.21s/it]\u001b[A\n","eval:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 66/107 [06:07<03:23,  4.96s/it]\u001b[A\n","eval:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 67/107 [06:14<03:37,  5.45s/it]\u001b[A\n","eval:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 68/107 [06:20<03:42,  5.72s/it]\u001b[A\n","eval:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 69/107 [06:25<03:22,  5.33s/it]\u001b[A\n","eval:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 70/107 [06:29<03:08,  5.10s/it]\u001b[A\n","eval:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 71/107 [06:36<03:21,  5.58s/it]\u001b[A\n","eval:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 72/107 [06:41<03:12,  5.49s/it]\u001b[A\n","eval:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 73/107 [06:48<03:18,  5.85s/it]\u001b[A\n","eval:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 74/107 [06:52<02:58,  5.42s/it]\u001b[A\n","eval:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 75/107 [06:58<02:52,  5.38s/it]\u001b[A\n","eval:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 76/107 [07:05<03:03,  5.92s/it]\u001b[A\n","eval:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 77/107 [07:09<02:45,  5.51s/it]\u001b[A\n","eval:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 78/107 [07:15<02:41,  5.58s/it]\u001b[A\n","eval:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 79/107 [07:21<02:42,  5.80s/it]\u001b[A\n","eval:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 80/107 [07:26<02:30,  5.57s/it]\u001b[A\n","eval:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 81/107 [07:33<02:30,  5.78s/it]\u001b[A\n","eval:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 82/107 [07:38<02:18,  5.55s/it]\u001b[A\n","eval:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 83/107 [07:44<02:16,  5.67s/it]\u001b[A\n","eval:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 84/107 [07:49<02:09,  5.64s/it]\u001b[A\n","eval:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 85/107 [07:53<01:53,  5.15s/it]\u001b[A\n","eval:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 86/107 [08:00<01:55,  5.50s/it]\u001b[A\n","eval:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 87/107 [08:05<01:49,  5.46s/it]\u001b[A\n","eval:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 88/107 [08:10<01:39,  5.22s/it]\u001b[A\n","eval:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 89/107 [08:17<01:43,  5.73s/it]\u001b[A\n","eval:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 90/107 [08:21<01:30,  5.32s/it]\u001b[A\n","eval:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 91/107 [08:26<01:25,  5.32s/it]\u001b[A\n","eval:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 92/107 [08:33<01:24,  5.62s/it]\u001b[A\n","eval:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 93/107 [08:38<01:16,  5.48s/it]\u001b[A\n","eval:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 94/107 [08:44<01:15,  5.84s/it]\u001b[A\n","eval:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 95/107 [08:50<01:08,  5.68s/it]\u001b[A\n","eval:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 96/107 [08:54<00:58,  5.29s/it]\u001b[A\n","eval:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 97/107 [09:00<00:55,  5.53s/it]\u001b[A\n","eval:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 98/107 [09:05<00:47,  5.30s/it]\u001b[A\n","eval:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 99/107 [09:11<00:43,  5.46s/it]\u001b[A\n","eval:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 100/107 [09:16<00:38,  5.52s/it]\u001b[A\n","eval:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 101/107 [09:22<00:33,  5.66s/it]\u001b[A\n","eval:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 102/107 [09:30<00:30,  6.15s/it]\u001b[A\n","eval:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 103/107 [09:34<00:22,  5.62s/it]\u001b[A\n","eval:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 104/107 [09:38<00:15,  5.21s/it]\u001b[A\n","eval:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 105/107 [09:45<00:11,  5.59s/it]\u001b[A\n","eval:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 106/107 [09:50<00:05,  5.42s/it]\u001b[A\n","eval: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 107/107 [09:53<00:00,  5.55s/it]\n","\n","eval:   0%|          | 0/14 [00:00<?, ?it/s]\u001b[A\n","eval:   7%|‚ñã         | 1/14 [00:05<01:15,  5.81s/it]\u001b[A\n","eval:  14%|‚ñà‚ñç        | 2/14 [00:10<01:01,  5.09s/it]\u001b[A\n","eval:  21%|‚ñà‚ñà‚ñè       | 3/14 [00:14<00:52,  4.74s/it]\u001b[A\n","eval:  29%|‚ñà‚ñà‚ñä       | 4/14 [00:21<00:57,  5.72s/it]\u001b[A\n","eval:  36%|‚ñà‚ñà‚ñà‚ñå      | 5/14 [00:26<00:47,  5.25s/it]\u001b[A\n","eval:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 6/14 [00:32<00:43,  5.41s/it]\u001b[A\n","eval:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 7/14 [00:37<00:38,  5.45s/it]\u001b[A\n","eval:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 8/14 [00:42<00:30,  5.16s/it]\u001b[A\n","eval:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 9/14 [00:48<00:27,  5.51s/it]\u001b[A\n","eval:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 10/14 [00:53<00:21,  5.26s/it]\u001b[A\n","eval:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 11/14 [00:57<00:15,  5.12s/it]\u001b[A\n","eval:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 12/14 [01:04<00:11,  5.52s/it]\u001b[A\n","eval:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 13/14 [01:09<00:05,  5.29s/it]\u001b[A\n","eval: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 14/14 [01:12<00:00,  5.21s/it]\n"," 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 4/5 [2:45:41<41:19, 2479.30s/it]  "]},{"name":"stdout","output_type":"stream","text":["save the model to finetune-5-2e-05.pt\n","epoch 3: train loss :: 1.326, train acc :: 0.517, dev acc :: 0.392\n"]},{"name":"stderr","output_type":"stream","text":["\n","train-4:   0%|          | 0/107 [00:00<?, ?it/s]\u001b[A\n","train-4:   1%|          | 1/107 [00:19<34:19, 19.43s/it]\u001b[A\n","train-4:   2%|‚ñè         | 2/107 [00:37<32:27, 18.55s/it]\u001b[A\n","train-4:   3%|‚ñé         | 3/107 [00:52<29:14, 16.87s/it]\u001b[A\n","train-4:   4%|‚ñé         | 4/107 [01:09<29:14, 17.04s/it]\u001b[A\n","train-4:   5%|‚ñç         | 5/107 [01:23<27:06, 15.94s/it]\u001b[A\n","train-4:   6%|‚ñå         | 6/107 [01:41<28:03, 16.67s/it]\u001b[A\n","train-4:   7%|‚ñã         | 7/107 [01:58<27:53, 16.73s/it]\u001b[A\n","train-4:   7%|‚ñã         | 8/107 [02:15<27:56, 16.93s/it]\u001b[A\n","train-4:   8%|‚ñä         | 9/107 [02:31<26:50, 16.43s/it]\u001b[A\n","train-4:   9%|‚ñâ         | 10/107 [02:45<25:34, 15.82s/it]\u001b[A\n","train-4:  10%|‚ñà         | 11/107 [03:01<25:06, 15.69s/it]\u001b[A\n","train-4:  11%|‚ñà         | 12/107 [03:16<24:53, 15.72s/it]\u001b[A\n","train-4:  12%|‚ñà‚ñè        | 13/107 [03:31<24:13, 15.46s/it]\u001b[A\n","train-4:  13%|‚ñà‚ñé        | 14/107 [03:48<24:34, 15.86s/it]\u001b[A\n","train-4:  14%|‚ñà‚ñç        | 15/107 [04:04<24:21, 15.88s/it]\u001b[A\n","train-4:  15%|‚ñà‚ñç        | 16/107 [04:23<25:42, 16.95s/it]\u001b[A\n","train-4:  16%|‚ñà‚ñå        | 17/107 [04:39<24:42, 16.48s/it]\u001b[A\n","train-4:  17%|‚ñà‚ñã        | 18/107 [04:57<25:16, 17.03s/it]\u001b[A\n","train-4:  18%|‚ñà‚ñä        | 19/107 [05:13<24:26, 16.66s/it]\u001b[A\n","train-4:  19%|‚ñà‚ñä        | 20/107 [05:31<24:40, 17.01s/it]\u001b[A\n","train-4:  20%|‚ñà‚ñâ        | 21/107 [05:50<25:12, 17.59s/it]\u001b[A\n","train-4:  21%|‚ñà‚ñà        | 22/107 [06:08<25:05, 17.72s/it]\u001b[A\n","train-4:  21%|‚ñà‚ñà‚ñè       | 23/107 [06:24<24:11, 17.28s/it]\u001b[A\n","train-4:  22%|‚ñà‚ñà‚ñè       | 24/107 [06:37<22:19, 16.14s/it]\u001b[A\n","train-4:  23%|‚ñà‚ñà‚ñé       | 25/107 [06:54<22:18, 16.32s/it]\u001b[A\n","train-4:  24%|‚ñà‚ñà‚ñç       | 26/107 [07:09<21:30, 15.93s/it]\u001b[A\n","train-4:  25%|‚ñà‚ñà‚ñå       | 27/107 [07:23<20:16, 15.20s/it]\u001b[A\n","train-4:  26%|‚ñà‚ñà‚ñå       | 28/107 [07:39<20:27, 15.53s/it]\u001b[A\n","train-4:  27%|‚ñà‚ñà‚ñã       | 29/107 [07:56<20:57, 16.12s/it]\u001b[A\n","train-4:  28%|‚ñà‚ñà‚ñä       | 30/107 [08:13<20:49, 16.22s/it]\u001b[A\n","train-4:  29%|‚ñà‚ñà‚ñâ       | 31/107 [08:27<19:55, 15.73s/it]\u001b[A\n","train-4:  30%|‚ñà‚ñà‚ñâ       | 32/107 [08:42<19:04, 15.26s/it]\u001b[A\n","train-4:  31%|‚ñà‚ñà‚ñà       | 33/107 [08:56<18:28, 14.98s/it]\u001b[A\n","train-4:  32%|‚ñà‚ñà‚ñà‚ñè      | 34/107 [09:15<19:36, 16.12s/it]\u001b[A\n","train-4:  33%|‚ñà‚ñà‚ñà‚ñé      | 35/107 [09:30<18:53, 15.74s/it]\u001b[A\n","train-4:  34%|‚ñà‚ñà‚ñà‚ñé      | 36/107 [09:56<22:15, 18.81s/it]\u001b[A\n","train-4:  35%|‚ñà‚ñà‚ñà‚ñç      | 37/107 [10:09<20:10, 17.29s/it]\u001b[A\n","train-4:  36%|‚ñà‚ñà‚ñà‚ñå      | 38/107 [10:26<19:40, 17.11s/it]\u001b[A\n","train-4:  36%|‚ñà‚ñà‚ñà‚ñã      | 39/107 [10:44<19:41, 17.37s/it]\u001b[A\n","train-4:  37%|‚ñà‚ñà‚ñà‚ñã      | 40/107 [10:57<18:01, 16.14s/it]\u001b[A\n","train-4:  38%|‚ñà‚ñà‚ñà‚ñä      | 41/107 [11:13<17:47, 16.18s/it]\u001b[A\n","train-4:  39%|‚ñà‚ñà‚ñà‚ñâ      | 42/107 [11:34<18:58, 17.51s/it]\u001b[A\n","train-4:  40%|‚ñà‚ñà‚ñà‚ñà      | 43/107 [11:52<18:50, 17.66s/it]\u001b[A\n","train-4:  41%|‚ñà‚ñà‚ñà‚ñà      | 44/107 [12:08<18:02, 17.19s/it]\u001b[A\n","train-4:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 45/107 [12:24<17:12, 16.65s/it]\u001b[A\n","train-4:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 46/107 [12:43<17:46, 17.49s/it]\u001b[A\n","train-4:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 47/107 [13:01<17:39, 17.67s/it]\u001b[A\n","train-4:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 48/107 [13:16<16:33, 16.83s/it]\u001b[A\n","train-4:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 49/107 [13:31<15:47, 16.33s/it]\u001b[A\n","train-4:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 50/107 [13:49<15:57, 16.80s/it]\u001b[A\n","train-4:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 51/107 [14:08<16:09, 17.31s/it]\u001b[A\n","train-4:  49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 52/107 [14:24<15:45, 17.20s/it]\u001b[A\n","train-4:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 53/107 [14:42<15:40, 17.41s/it]\u001b[A\n","train-4:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 54/107 [15:01<15:40, 17.75s/it]\u001b[A\n","train-4:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 55/107 [15:20<15:36, 18.01s/it]\u001b[A\n","train-4:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 56/107 [15:35<14:31, 17.09s/it]\u001b[A\n","train-4:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 57/107 [15:59<16:05, 19.32s/it]\u001b[A\n","train-4:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 58/107 [16:16<15:08, 18.55s/it]\u001b[A\n","train-4:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 59/107 [16:29<13:40, 17.10s/it]\u001b[A\n","train-4:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 60/107 [16:47<13:33, 17.31s/it]\u001b[A\n","train-4:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 61/107 [17:04<13:04, 17.06s/it]\u001b[A\n","train-4:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 62/107 [17:24<13:25, 17.90s/it]\u001b[A\n","train-4:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 63/107 [17:44<13:37, 18.58s/it]\u001b[A\n","train-4:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 64/107 [17:59<12:34, 17.55s/it]\u001b[A\n","train-4:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 65/107 [18:16<12:14, 17.48s/it]\u001b[A\n","train-4:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 66/107 [18:33<11:47, 17.27s/it]\u001b[A\n","train-4:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 67/107 [18:50<11:27, 17.19s/it]\u001b[A\n","train-4:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 68/107 [19:06<10:57, 16.86s/it]\u001b[A\n","train-4:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 69/107 [19:22<10:26, 16.48s/it]\u001b[A\n","train-4:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 70/107 [19:40<10:24, 16.88s/it]\u001b[A\n","train-4:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 71/107 [19:58<10:25, 17.39s/it]\u001b[A\n","train-4:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 72/107 [20:17<10:22, 17.79s/it]\u001b[A\n","train-4:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 73/107 [20:33<09:45, 17.21s/it]\u001b[A\n","train-4:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 74/107 [20:48<09:13, 16.79s/it]\u001b[A\n","train-4:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 75/107 [21:06<09:00, 16.88s/it]\u001b[A\n","train-4:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 76/107 [21:23<08:47, 17.02s/it]\u001b[A\n","train-4:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 77/107 [21:42<08:45, 17.53s/it]\u001b[A\n","train-4:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 78/107 [21:57<08:12, 16.97s/it]\u001b[A\n","train-4:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 79/107 [22:13<07:45, 16.64s/it]\u001b[A\n","train-4:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 80/107 [22:30<07:29, 16.67s/it]\u001b[A\n","train-4:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 81/107 [22:47<07:17, 16.83s/it]\u001b[A\n","train-4:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 82/107 [23:04<06:58, 16.73s/it]\u001b[A\n","train-4:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 83/107 [23:21<06:44, 16.86s/it]\u001b[A\n","train-4:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 84/107 [23:36<06:15, 16.33s/it]\u001b[A\n","train-4:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 85/107 [23:54<06:12, 16.94s/it]\u001b[A\n","train-4:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 86/107 [24:12<06:02, 17.24s/it]\u001b[A\n","train-4:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 87/107 [24:28<05:36, 16.83s/it]\u001b[A\n","train-4:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 88/107 [24:43<05:10, 16.36s/it]\u001b[A\n","train-4:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 89/107 [24:58<04:45, 15.85s/it]\u001b[A\n","train-4:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 90/107 [25:16<04:40, 16.52s/it]\u001b[A\n","train-4:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 91/107 [25:34<04:28, 16.81s/it]\u001b[A\n","train-4:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 92/107 [25:51<04:15, 17.02s/it]\u001b[A\n","train-4:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 93/107 [26:08<03:58, 17.01s/it]\u001b[A\n","train-4:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 94/107 [26:28<03:51, 17.81s/it]\u001b[A\n","train-4:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 95/107 [26:45<03:32, 17.71s/it]\u001b[A\n","train-4:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 96/107 [27:02<03:11, 17.41s/it]\u001b[A\n","train-4:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 97/107 [27:18<02:50, 17.06s/it]\u001b[A\n","train-4:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 98/107 [27:32<02:24, 16.08s/it]\u001b[A\n","train-4:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 99/107 [27:51<02:15, 16.94s/it]\u001b[A\n","train-4:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 100/107 [28:05<01:52, 16.13s/it]\u001b[A\n","train-4:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 101/107 [28:23<01:39, 16.65s/it]\u001b[A\n","train-4:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 102/107 [28:40<01:23, 16.72s/it]\u001b[A\n","train-4:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 103/107 [28:56<01:06, 16.59s/it]\u001b[A\n","train-4:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 104/107 [29:12<00:49, 16.34s/it]\u001b[A\n","train-4:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 105/107 [29:27<00:32, 16.05s/it]\u001b[A\n","train-4:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 106/107 [29:47<00:17, 17.26s/it]\u001b[A\n","train-4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 107/107 [29:58<00:00, 16.81s/it]\n","\n","eval:   0%|          | 0/107 [00:00<?, ?it/s]\u001b[A\n","eval:   1%|          | 1/107 [00:07<12:28,  7.06s/it]\u001b[A\n","eval:   2%|‚ñè         | 2/107 [00:12<10:16,  5.87s/it]\u001b[A\n","eval:   3%|‚ñé         | 3/107 [00:19<11:25,  6.59s/it]\u001b[A\n","eval:   4%|‚ñé         | 4/107 [00:25<10:50,  6.32s/it]\u001b[A\n","eval:   5%|‚ñç         | 5/107 [00:32<11:09,  6.57s/it]\u001b[A\n","eval:   6%|‚ñå         | 6/107 [00:37<10:18,  6.12s/it]\u001b[A\n","eval:   7%|‚ñã         | 7/107 [00:42<09:24,  5.65s/it]\u001b[A\n","eval:   7%|‚ñã         | 8/107 [00:48<09:41,  5.88s/it]\u001b[A\n","eval:   8%|‚ñä         | 9/107 [00:53<09:11,  5.63s/it]\u001b[A\n","eval:   9%|‚ñâ         | 10/107 [01:00<09:33,  5.91s/it]\u001b[A\n","eval:  10%|‚ñà         | 11/107 [01:05<09:10,  5.74s/it]\u001b[A\n","eval:  11%|‚ñà         | 12/107 [01:10<08:38,  5.46s/it]\u001b[A\n","eval:  12%|‚ñà‚ñè        | 13/107 [01:17<09:16,  5.92s/it]\u001b[A\n","eval:  13%|‚ñà‚ñé        | 14/107 [01:22<08:35,  5.55s/it]\u001b[A\n","eval:  14%|‚ñà‚ñç        | 15/107 [01:27<08:23,  5.47s/it]\u001b[A\n","eval:  15%|‚ñà‚ñç        | 16/107 [01:34<08:57,  5.91s/it]\u001b[A\n","eval:  16%|‚ñà‚ñå        | 17/107 [01:38<08:10,  5.45s/it]\u001b[A\n","eval:  17%|‚ñà‚ñã        | 18/107 [01:45<08:28,  5.71s/it]\u001b[A\n","eval:  18%|‚ñà‚ñä        | 19/107 [01:51<08:34,  5.85s/it]\u001b[A\n","eval:  19%|‚ñà‚ñä        | 20/107 [01:56<08:19,  5.74s/it]\u001b[A\n","eval:  20%|‚ñà‚ñâ        | 21/107 [02:02<08:19,  5.81s/it]\u001b[A\n","eval:  21%|‚ñà‚ñà        | 22/107 [02:06<07:30,  5.30s/it]\u001b[A\n","eval:  21%|‚ñà‚ñà‚ñè       | 23/107 [02:10<06:51,  4.90s/it]\u001b[A\n","eval:  22%|‚ñà‚ñà‚ñè       | 24/107 [02:17<07:29,  5.42s/it]\u001b[A\n","eval:  23%|‚ñà‚ñà‚ñé       | 25/107 [02:21<07:02,  5.15s/it]\u001b[A\n","eval:  24%|‚ñà‚ñà‚ñç       | 26/107 [02:28<07:28,  5.53s/it]\u001b[A\n","eval:  25%|‚ñà‚ñà‚ñå       | 27/107 [02:33<07:17,  5.47s/it]\u001b[A\n","eval:  26%|‚ñà‚ñà‚ñå       | 28/107 [02:37<06:38,  5.05s/it]\u001b[A\n","eval:  27%|‚ñà‚ñà‚ñã       | 29/107 [02:43<06:50,  5.26s/it]\u001b[A\n","eval:  28%|‚ñà‚ñà‚ñä       | 30/107 [02:48<06:41,  5.22s/it]\u001b[A\n","eval:  29%|‚ñà‚ñà‚ñâ       | 31/107 [02:57<07:51,  6.20s/it]\u001b[A\n","eval:  30%|‚ñà‚ñà‚ñâ       | 32/107 [03:03<07:47,  6.23s/it]\u001b[A\n","eval:  31%|‚ñà‚ñà‚ñà       | 33/107 [03:11<08:16,  6.71s/it]\u001b[A\n","eval:  32%|‚ñà‚ñà‚ñà‚ñè      | 34/107 [03:17<08:01,  6.59s/it]\u001b[A\n","eval:  33%|‚ñà‚ñà‚ñà‚ñé      | 35/107 [03:22<07:13,  6.03s/it]\u001b[A\n","eval:  34%|‚ñà‚ñà‚ñà‚ñé      | 36/107 [03:28<07:09,  6.06s/it]\u001b[A\n","eval:  35%|‚ñà‚ñà‚ñà‚ñç      | 37/107 [03:34<06:54,  5.92s/it]\u001b[A\n","eval:  36%|‚ñà‚ñà‚ñà‚ñå      | 38/107 [03:38<06:17,  5.47s/it]\u001b[A\n","eval:  36%|‚ñà‚ñà‚ñà‚ñã      | 39/107 [03:46<06:57,  6.13s/it]\u001b[A\n","eval:  37%|‚ñà‚ñà‚ñà‚ñã      | 40/107 [03:51<06:34,  5.89s/it]\u001b[A\n","eval:  38%|‚ñà‚ñà‚ñà‚ñä      | 41/107 [03:58<06:48,  6.19s/it]\u001b[A\n","eval:  39%|‚ñà‚ñà‚ñà‚ñâ      | 42/107 [04:03<06:16,  5.80s/it]\u001b[A\n","eval:  40%|‚ñà‚ñà‚ñà‚ñà      | 43/107 [04:06<05:31,  5.19s/it]\u001b[A\n","eval:  41%|‚ñà‚ñà‚ñà‚ñà      | 44/107 [04:13<05:42,  5.44s/it]\u001b[A\n","eval:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 45/107 [04:18<05:39,  5.48s/it]\u001b[A\n","eval:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 46/107 [04:23<05:22,  5.29s/it]\u001b[A\n","eval:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 47/107 [04:30<05:41,  5.69s/it]\u001b[A\n","eval:  45%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 48/107 [04:35<05:22,  5.47s/it]\u001b[A\n","eval:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 49/107 [04:40<05:19,  5.51s/it]\u001b[A\n","eval:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 50/107 [04:46<05:20,  5.63s/it]\u001b[A\n","eval:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 51/107 [04:51<04:56,  5.29s/it]\u001b[A\n","eval:  49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 52/107 [04:57<05:16,  5.76s/it]\u001b[A\n","eval:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 53/107 [05:02<04:51,  5.40s/it]\u001b[A\n","eval:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 54/107 [05:07<04:44,  5.38s/it]\u001b[A\n","eval:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 55/107 [05:14<04:57,  5.73s/it]\u001b[A\n","eval:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 56/107 [05:19<04:46,  5.61s/it]\u001b[A\n","eval:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 57/107 [05:27<05:07,  6.15s/it]\u001b[A\n","eval:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 58/107 [05:33<05:03,  6.20s/it]\u001b[A\n","eval:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 59/107 [05:40<05:06,  6.39s/it]\u001b[A\n","eval:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 60/107 [05:44<04:34,  5.84s/it]\u001b[A\n","eval:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 61/107 [05:49<04:19,  5.64s/it]\u001b[A\n","eval:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 62/107 [05:56<04:25,  5.89s/it]\u001b[A\n","eval:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 63/107 [06:01<04:04,  5.55s/it]\u001b[A\n","eval:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 64/107 [06:06<03:49,  5.35s/it]\u001b[A\n","eval:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 65/107 [06:12<03:55,  5.61s/it]\u001b[A\n","eval:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 66/107 [06:16<03:39,  5.34s/it]\u001b[A\n","eval:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 67/107 [06:21<03:27,  5.18s/it]\u001b[A\n","eval:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 68/107 [06:28<03:39,  5.63s/it]\u001b[A\n","eval:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 69/107 [06:33<03:27,  5.46s/it]\u001b[A\n","eval:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 70/107 [06:40<03:33,  5.78s/it]\u001b[A\n","eval:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 71/107 [06:45<03:20,  5.57s/it]\u001b[A\n","eval:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 72/107 [06:49<03:01,  5.19s/it]\u001b[A\n","eval:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 73/107 [06:56<03:12,  5.65s/it]\u001b[A\n","eval:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 74/107 [07:01<03:00,  5.46s/it]\u001b[A\n","eval:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 75/107 [07:08<03:08,  5.88s/it]\u001b[A\n","eval:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 76/107 [07:13<02:56,  5.69s/it]\u001b[A\n","eval:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 77/107 [07:17<02:39,  5.30s/it]\u001b[A\n","eval:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 78/107 [07:23<02:42,  5.59s/it]\u001b[A\n","eval:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 79/107 [07:29<02:36,  5.57s/it]\u001b[A\n","eval:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 80/107 [07:34<02:25,  5.37s/it]\u001b[A\n","eval:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 81/107 [07:41<02:35,  5.96s/it]\u001b[A\n","eval:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 82/107 [07:46<02:22,  5.69s/it]\u001b[A\n","eval:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 83/107 [07:53<02:25,  6.07s/it]\u001b[A\n","eval:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 84/107 [07:58<02:12,  5.74s/it]\u001b[A\n","eval:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 85/107 [08:03<02:00,  5.48s/it]\u001b[A\n","eval:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 86/107 [08:10<02:06,  6.00s/it]\u001b[A\n","eval:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 87/107 [08:15<01:49,  5.47s/it]\u001b[A\n","eval:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 88/107 [08:21<01:50,  5.80s/it]\u001b[A\n","eval:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 89/107 [08:27<01:42,  5.69s/it]\u001b[A\n","eval:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 90/107 [08:31<01:32,  5.47s/it]\u001b[A\n","eval:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 91/107 [08:38<01:33,  5.87s/it]\u001b[A\n","eval:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 92/107 [08:43<01:21,  5.42s/it]\u001b[A\n","eval:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 93/107 [08:47<01:13,  5.22s/it]\u001b[A\n","eval:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 94/107 [08:54<01:11,  5.51s/it]\u001b[A\n","eval:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 95/107 [08:59<01:04,  5.36s/it]\u001b[A\n","eval:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 96/107 [09:04<00:59,  5.40s/it]\u001b[A\n","eval:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 97/107 [09:10<00:55,  5.53s/it]\u001b[A\n","eval:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 98/107 [09:15<00:48,  5.43s/it]\u001b[A\n","eval:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 99/107 [09:22<00:46,  5.82s/it]\u001b[A\n","eval:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 100/107 [09:27<00:39,  5.67s/it]\u001b[A\n","eval:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 101/107 [09:33<00:35,  5.84s/it]\u001b[A\n","eval:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 102/107 [09:40<00:30,  6.07s/it]\u001b[A\n","eval:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 103/107 [09:45<00:22,  5.73s/it]\u001b[A\n","eval:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 104/107 [09:51<00:17,  5.92s/it]\u001b[A\n","eval:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 105/107 [09:56<00:11,  5.66s/it]\u001b[A\n","eval:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 106/107 [10:02<00:05,  5.58s/it]\u001b[A\n","eval: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 107/107 [10:07<00:00,  5.68s/it]\n","\n","eval:   0%|          | 0/14 [00:00<?, ?it/s]\u001b[A\n","eval:   7%|‚ñã         | 1/14 [00:04<00:53,  4.09s/it]\u001b[A\n","eval:  14%|‚ñà‚ñç        | 2/14 [00:08<00:54,  4.54s/it]\u001b[A\n","eval:  21%|‚ñà‚ñà‚ñè       | 3/14 [00:14<00:56,  5.15s/it]\u001b[A\n","eval:  29%|‚ñà‚ñà‚ñä       | 4/14 [00:20<00:52,  5.29s/it]\u001b[A\n","eval:  36%|‚ñà‚ñà‚ñà‚ñå      | 5/14 [00:25<00:48,  5.36s/it]\u001b[A\n","eval:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 6/14 [00:33<00:48,  6.00s/it]\u001b[A\n","eval:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 7/14 [00:37<00:39,  5.62s/it]\u001b[A\n","eval:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 8/14 [00:44<00:35,  5.85s/it]\u001b[A\n","eval:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 9/14 [00:48<00:27,  5.44s/it]\u001b[A\n","eval:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 10/14 [00:53<00:20,  5.19s/it]\u001b[A\n","eval:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 11/14 [00:59<00:16,  5.61s/it]\u001b[A\n","eval:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 12/14 [01:04<00:10,  5.31s/it]\u001b[A\n","eval:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 13/14 [01:09<00:05,  5.30s/it]\u001b[A\n","eval: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 14/14 [01:15<00:00,  5.36s/it]\n","100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [3:27:10<00:00, 2486.02s/it]"]},{"name":"stdout","output_type":"stream","text":["save the model to finetune-5-2e-05.pt\n","epoch 4: train loss :: 1.074, train acc :: 0.688, dev acc :: 0.414\n"]},{"name":"stderr","output_type":"stream","text":["\n","/tmp/ipykernel_7362/3169216680.py:279: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  saved = torch.load(args.filepath)\n","/tmp/ipykernel_7362/2561774016.py:332: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  checkpoint_dict = torch.load(checkpoint, map_location=device)\n"]},{"name":"stdout","output_type":"stream","text":["load model from finetune-5-2e-05.pt\n","load 1101 data from data/sst-dev.txt\n","load 2210 data from data/sst-test.txt\n"]},{"name":"stderr","output_type":"stream","text":["eval: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 14/14 [01:15<00:00,  5.37s/it]\n","eval: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 28/28 [02:30<00:00,  5.36s/it]"]},{"name":"stdout","output_type":"stream","text":["dev acc :: 0.414\n","test acc :: 0.418\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["def get_args():\n","    # Instead of using argparse, we define a simple class to hold our parameters\n","    class Args:\n","        def __init__(self):\n","            self.train = \"data/sst-train.txt\"\n","            self.dev = \"data/sst-dev.txt\"\n","            self.test = \"data/sst-test.txt\"\n","            self.label_names = \"data/sst-label-mapping.json\"\n","            self.pretrained_model_path = \"stories42M.pt\"\n","            self.max_sentence_len = None\n","            self.seed = 1337\n","            self.epochs = 5\n","            self.option = \"finetune\"  # ('generate', 'prompt', 'finetune')'prompt: the Llama parameters are frozen; finetune: Llama parameters are updated',\n","            self.use_gpu = False  # Set to True if you want to use GPU\n","            self.generated_sentence_low_temp_out = \"generated-sentence-temp-0.txt\"\n","            self.generated_sentence_high_temp_out = \"generated-sentence-temp-1.txt\"\n","            self.dev_out = \"sst-dev-finetuning-output.txt\"\n","            self.test_out = \"sst-test-finetuning-output.txt\"\n","            self.batch_size = 80 # sst: 64, cfimdb: 8 can fit a 12GB GPU\n","            self.hidden_dropout_prob = 0.3\n","            self.lr = 2e-5 # default lr for 'pretrain': 1e-3, 'finetune': 1e-5\", default=2e-5\n","\n","    args = Args()\n","    print(f\"args: {vars(args)}\")\n","    return args\n","\n","if __name__ == \"__main__\":\n","    args = get_args()\n","    args.filepath = f'{args.option}-{args.epochs}-{args.lr}.pt'  # save path\n","    seed_everything(args.seed)  # fix the seed for reproducibility\n","\n","    if args.option == \"generate\":\n","        # Step 1\n","        # Complete this sentence to test your implementation!\n","        prefix = \"I have wanted to see this thriller for a while, and it didn't disappoint. Keanu Reeves, playing the hero John Wick, is\"\n","        generate_sentence(args, prefix, args.generated_sentence_low_temp_out, max_new_tokens=75, temperature=0.0)\n","        generate_sentence(args, prefix, args.generated_sentence_high_temp_out, max_new_tokens=75, temperature=1.0)\n","    elif args.option == \"prompt\":\n","        # Step 2\n","        # Solve this task with prompted language modeling\n","        test_with_prompting(args)\n","    elif args.option == \"finetune\":\n","        # Step 3\n","        # Finetune a classification model\n","        train(args)\n","\n","        # Step 4\n","        # Evaluate your model on the dev and test sets\n","        test(args)\n","    else:\n","        raise ValueError(f\"Invalid option: {args.option}\")"]},{"cell_type":"markdown","metadata":{"id":"FhUWXm__yzBB"},"source":["# Finetuning for CFIMDB"]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":381342,"status":"ok","timestamp":1732189193942,"user":{"displayName":"Chaoran Zhou","userId":"09162553986537566448"},"user_tz":-480},"id":"w6o85cz3yy39","outputId":"9f5506e6-075d-41a7-8867-7de282673f33"},"outputs":[{"name":"stdout","output_type":"stream","text":["args: {'train': 'data/cfimdb-train.txt', 'dev': 'data/cfimdb-dev.txt', 'test': 'data/cfimdb-test.txt', 'label_names': 'data/cfimdb-label-mapping.json', 'pretrained_model_path': 'stories42M.pt', 'max_sentence_len': None, 'seed': 1337, 'epochs': 5, 'option': 'finetune', 'use_gpu': True, 'generated_sentence_low_temp_out': 'generated-sentence-temp-0.txt', 'generated_sentence_high_temp_out': 'generated-sentence-temp-1.txt', 'dev_out': 'cfimdb-dev-finetuning-output.txt', 'test_out': 'cfimdb-test-finetuning-output.txt', 'batch_size': 10, 'hidden_dropout_prob': 0.3, 'lr': 2e-05}\n","load 1707 data from data/cfimdb-train.txt\n","load 245 data from data/cfimdb-dev.txt\n"]},{"name":"stderr","output_type":"stream","text":["<ipython-input-5-6906fe08d67a>:332: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  checkpoint_dict = torch.load(checkpoint, map_location=device)\n","  0%|          | 0/5 [00:00<?, ?it/s]\n","train-0:   0%|          | 0/171 [00:00<?, ?it/s]\u001b[A\n","train-0:   1%|          | 1/171 [00:01<04:45,  1.68s/it]\u001b[A\n","train-0:   1%|          | 2/171 [00:01<02:24,  1.17it/s]\u001b[A\n","train-0:   2%|‚ñè         | 3/171 [00:02<01:39,  1.69it/s]\u001b[A\n","train-0:   2%|‚ñè         | 4/171 [00:02<01:21,  2.04it/s]\u001b[A\n","train-0:   3%|‚ñé         | 5/171 [00:02<01:05,  2.52it/s]\u001b[A\n","train-0:   4%|‚ñé         | 6/171 [00:03<00:56,  2.92it/s]\u001b[A\n","train-0:   4%|‚ñç         | 7/171 [00:03<00:51,  3.19it/s]\u001b[A\n","train-0:   5%|‚ñç         | 8/171 [00:03<00:47,  3.40it/s]\u001b[A\n","train-0:   5%|‚ñå         | 9/171 [00:03<00:45,  3.57it/s]\u001b[A\n","train-0:   6%|‚ñå         | 10/171 [00:04<00:42,  3.76it/s]\u001b[A\n","train-0:   6%|‚ñã         | 11/171 [00:04<00:45,  3.49it/s]\u001b[A\n","train-0:   7%|‚ñã         | 12/171 [00:04<00:45,  3.52it/s]\u001b[A\n","train-0:   8%|‚ñä         | 13/171 [00:05<00:49,  3.21it/s]\u001b[A\n","train-0:   8%|‚ñä         | 14/171 [00:05<00:47,  3.30it/s]\u001b[A\n","train-0:   9%|‚ñâ         | 15/171 [00:05<00:44,  3.51it/s]\u001b[A\n","train-0:   9%|‚ñâ         | 16/171 [00:05<00:45,  3.42it/s]\u001b[A\n","train-0:  10%|‚ñâ         | 17/171 [00:06<00:41,  3.70it/s]\u001b[A\n","train-0:  11%|‚ñà         | 18/171 [00:06<00:40,  3.78it/s]\u001b[A\n","train-0:  11%|‚ñà         | 19/171 [00:06<00:41,  3.66it/s]\u001b[A\n","train-0:  12%|‚ñà‚ñè        | 20/171 [00:06<00:40,  3.72it/s]\u001b[A\n","train-0:  12%|‚ñà‚ñè        | 21/171 [00:07<00:39,  3.77it/s]\u001b[A\n","train-0:  13%|‚ñà‚ñé        | 22/171 [00:07<00:39,  3.80it/s]\u001b[A\n","train-0:  13%|‚ñà‚ñé        | 23/171 [00:07<00:39,  3.74it/s]\u001b[A\n","train-0:  14%|‚ñà‚ñç        | 24/171 [00:08<00:44,  3.32it/s]\u001b[A\n","train-0:  15%|‚ñà‚ñç        | 25/171 [00:08<00:44,  3.25it/s]\u001b[A\n","train-0:  15%|‚ñà‚ñå        | 26/171 [00:08<00:44,  3.28it/s]\u001b[A\n","train-0:  16%|‚ñà‚ñå        | 27/171 [00:08<00:42,  3.38it/s]\u001b[A\n","train-0:  16%|‚ñà‚ñã        | 28/171 [00:09<00:41,  3.44it/s]\u001b[A\n","train-0:  17%|‚ñà‚ñã        | 29/171 [00:09<00:42,  3.32it/s]\u001b[A\n","train-0:  18%|‚ñà‚ñä        | 30/171 [00:09<00:42,  3.33it/s]\u001b[A\n","train-0:  18%|‚ñà‚ñä        | 31/171 [00:10<00:43,  3.22it/s]\u001b[A\n","train-0:  19%|‚ñà‚ñä        | 32/171 [00:10<00:42,  3.26it/s]\u001b[A\n","train-0:  19%|‚ñà‚ñâ        | 33/171 [00:10<00:40,  3.43it/s]\u001b[A\n","train-0:  20%|‚ñà‚ñâ        | 34/171 [00:11<00:41,  3.28it/s]\u001b[A\n","train-0:  20%|‚ñà‚ñà        | 35/171 [00:11<00:43,  3.13it/s]\u001b[A\n","train-0:  21%|‚ñà‚ñà        | 36/171 [00:11<00:45,  2.99it/s]\u001b[A\n","train-0:  22%|‚ñà‚ñà‚ñè       | 37/171 [00:12<00:39,  3.37it/s]\u001b[A\n","train-0:  22%|‚ñà‚ñà‚ñè       | 38/171 [00:12<00:37,  3.55it/s]\u001b[A\n","train-0:  23%|‚ñà‚ñà‚ñé       | 39/171 [00:12<00:37,  3.52it/s]\u001b[A\n","train-0:  23%|‚ñà‚ñà‚ñé       | 40/171 [00:12<00:35,  3.70it/s]\u001b[A\n","train-0:  24%|‚ñà‚ñà‚ñç       | 41/171 [00:13<00:36,  3.58it/s]\u001b[A\n","train-0:  25%|‚ñà‚ñà‚ñç       | 42/171 [00:13<00:34,  3.75it/s]\u001b[A\n","train-0:  25%|‚ñà‚ñà‚ñå       | 43/171 [00:13<00:36,  3.55it/s]\u001b[A\n","train-0:  26%|‚ñà‚ñà‚ñå       | 44/171 [00:13<00:36,  3.46it/s]\u001b[A\n","train-0:  26%|‚ñà‚ñà‚ñã       | 45/171 [00:14<00:34,  3.62it/s]\u001b[A\n","train-0:  27%|‚ñà‚ñà‚ñã       | 46/171 [00:14<00:35,  3.54it/s]\u001b[A\n","train-0:  27%|‚ñà‚ñà‚ñã       | 47/171 [00:14<00:34,  3.55it/s]\u001b[A\n","train-0:  28%|‚ñà‚ñà‚ñä       | 48/171 [00:15<00:33,  3.63it/s]\u001b[A\n","train-0:  29%|‚ñà‚ñà‚ñä       | 49/171 [00:15<00:39,  3.12it/s]\u001b[A\n","train-0:  29%|‚ñà‚ñà‚ñâ       | 50/171 [00:15<00:37,  3.21it/s]\u001b[A\n","train-0:  30%|‚ñà‚ñà‚ñâ       | 51/171 [00:16<00:37,  3.16it/s]\u001b[A\n","train-0:  30%|‚ñà‚ñà‚ñà       | 52/171 [00:16<00:36,  3.27it/s]\u001b[A\n","train-0:  31%|‚ñà‚ñà‚ñà       | 53/171 [00:16<00:36,  3.28it/s]\u001b[A\n","train-0:  32%|‚ñà‚ñà‚ñà‚ñè      | 54/171 [00:16<00:35,  3.31it/s]\u001b[A\n","train-0:  32%|‚ñà‚ñà‚ñà‚ñè      | 55/171 [00:17<00:37,  3.13it/s]\u001b[A\n","train-0:  33%|‚ñà‚ñà‚ñà‚ñé      | 56/171 [00:17<00:35,  3.24it/s]\u001b[A\n","train-0:  33%|‚ñà‚ñà‚ñà‚ñé      | 57/171 [00:17<00:35,  3.22it/s]\u001b[A\n","train-0:  34%|‚ñà‚ñà‚ñà‚ñç      | 58/171 [00:18<00:34,  3.32it/s]\u001b[A\n","train-0:  35%|‚ñà‚ñà‚ñà‚ñç      | 59/171 [00:18<00:35,  3.19it/s]\u001b[A\n","train-0:  35%|‚ñà‚ñà‚ñà‚ñå      | 60/171 [00:18<00:32,  3.39it/s]\u001b[A\n","train-0:  36%|‚ñà‚ñà‚ñà‚ñå      | 61/171 [00:19<00:32,  3.41it/s]\u001b[A\n","train-0:  36%|‚ñà‚ñà‚ñà‚ñã      | 62/171 [00:19<00:31,  3.46it/s]\u001b[A\n","train-0:  37%|‚ñà‚ñà‚ñà‚ñã      | 63/171 [00:19<00:29,  3.61it/s]\u001b[A\n","train-0:  37%|‚ñà‚ñà‚ñà‚ñã      | 64/171 [00:19<00:29,  3.60it/s]\u001b[A\n","train-0:  38%|‚ñà‚ñà‚ñà‚ñä      | 65/171 [00:20<00:28,  3.70it/s]\u001b[A\n","train-0:  39%|‚ñà‚ñà‚ñà‚ñä      | 66/171 [00:20<00:28,  3.65it/s]\u001b[A\n","train-0:  39%|‚ñà‚ñà‚ñà‚ñâ      | 67/171 [00:20<00:27,  3.77it/s]\u001b[A\n","train-0:  40%|‚ñà‚ñà‚ñà‚ñâ      | 68/171 [00:20<00:28,  3.66it/s]\u001b[A\n","train-0:  40%|‚ñà‚ñà‚ñà‚ñà      | 69/171 [00:21<00:28,  3.53it/s]\u001b[A\n","train-0:  41%|‚ñà‚ñà‚ñà‚ñà      | 70/171 [00:21<00:28,  3.58it/s]\u001b[A\n","train-0:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 71/171 [00:21<00:25,  3.93it/s]\u001b[A\n","train-0:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 72/171 [00:22<00:27,  3.65it/s]\u001b[A\n","train-0:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 73/171 [00:22<00:28,  3.41it/s]\u001b[A\n","train-0:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 74/171 [00:22<00:26,  3.60it/s]\u001b[A\n","train-0:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 75/171 [00:22<00:26,  3.59it/s]\u001b[A\n","train-0:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 76/171 [00:23<00:26,  3.57it/s]\u001b[A\n","train-0:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 77/171 [00:23<00:26,  3.49it/s]\u001b[A\n","train-0:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 78/171 [00:23<00:26,  3.49it/s]\u001b[A\n","train-0:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 79/171 [00:24<00:26,  3.45it/s]\u001b[A\n","train-0:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 80/171 [00:24<00:26,  3.37it/s]\u001b[A\n","train-0:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 81/171 [00:24<00:27,  3.30it/s]\u001b[A\n","train-0:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 82/171 [00:25<00:28,  3.17it/s]\u001b[A\n","train-0:  49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 83/171 [00:25<00:27,  3.21it/s]\u001b[A\n","train-0:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 84/171 [00:25<00:27,  3.18it/s]\u001b[A\n","train-0:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 85/171 [00:25<00:27,  3.13it/s]\u001b[A\n","train-0:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 86/171 [00:26<00:26,  3.19it/s]\u001b[A\n","train-0:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 87/171 [00:26<00:25,  3.24it/s]\u001b[A\n","train-0:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 88/171 [00:26<00:24,  3.37it/s]\u001b[A\n","train-0:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 89/171 [00:27<00:23,  3.51it/s]\u001b[A\n","train-0:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 90/171 [00:27<00:22,  3.57it/s]\u001b[A\n","train-0:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 91/171 [00:27<00:22,  3.51it/s]\u001b[A\n","train-0:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 92/171 [00:27<00:21,  3.61it/s]\u001b[A\n","train-0:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 93/171 [00:28<00:22,  3.45it/s]\u001b[A\n","train-0:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 94/171 [00:28<00:23,  3.26it/s]\u001b[A\n","train-0:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 95/171 [00:28<00:23,  3.27it/s]\u001b[A\n","train-0:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 96/171 [00:29<00:22,  3.32it/s]\u001b[A\n","train-0:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 97/171 [00:29<00:23,  3.12it/s]\u001b[A\n","train-0:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 98/171 [00:29<00:23,  3.06it/s]\u001b[A\n","train-0:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 99/171 [00:30<00:21,  3.37it/s]\u001b[A\n","train-0:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 100/171 [00:30<00:22,  3.18it/s]\u001b[A\n","train-0:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 101/171 [00:30<00:20,  3.34it/s]\u001b[A\n","train-0:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 102/171 [00:31<00:20,  3.45it/s]\u001b[A\n","train-0:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 103/171 [00:31<00:19,  3.44it/s]\u001b[A\n","train-0:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 104/171 [00:31<00:21,  3.15it/s]\u001b[A\n","train-0:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 105/171 [00:31<00:20,  3.24it/s]\u001b[A\n","train-0:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 106/171 [00:32<00:20,  3.23it/s]\u001b[A\n","train-0:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 107/171 [00:32<00:19,  3.22it/s]\u001b[A\n","train-0:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 108/171 [00:32<00:19,  3.19it/s]\u001b[A\n","train-0:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 109/171 [00:33<00:20,  3.05it/s]\u001b[A\n","train-0:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 110/171 [00:33<00:18,  3.25it/s]\u001b[A\n","train-0:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 111/171 [00:33<00:17,  3.41it/s]\u001b[A\n","train-0:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 112/171 [00:34<00:17,  3.37it/s]\u001b[A\n","train-0:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 113/171 [00:34<00:16,  3.53it/s]\u001b[A\n","train-0:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 114/171 [00:34<00:16,  3.51it/s]\u001b[A\n","train-0:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 115/171 [00:34<00:15,  3.67it/s]\u001b[A\n","train-0:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 116/171 [00:35<00:15,  3.46it/s]\u001b[A\n","train-0:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 117/171 [00:35<00:13,  3.96it/s]\u001b[A\n","train-0:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 118/171 [00:35<00:15,  3.39it/s]\u001b[A\n","train-0:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 119/171 [00:36<00:16,  3.24it/s]\u001b[A\n","train-0:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 120/171 [00:36<00:15,  3.34it/s]\u001b[A\n","train-0:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 121/171 [00:36<00:15,  3.21it/s]\u001b[A\n","train-0:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 122/171 [00:37<00:15,  3.14it/s]\u001b[A\n","train-0:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 123/171 [00:37<00:16,  2.93it/s]\u001b[A\n","train-0:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 124/171 [00:37<00:15,  3.13it/s]\u001b[A\n","train-0:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 125/171 [00:38<00:14,  3.07it/s]\u001b[A\n","train-0:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 126/171 [00:38<00:13,  3.40it/s]\u001b[A\n","train-0:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 127/171 [00:38<00:13,  3.31it/s]\u001b[A\n","train-0:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 128/171 [00:38<00:12,  3.32it/s]\u001b[A\n","train-0:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 129/171 [00:39<00:11,  3.55it/s]\u001b[A\n","train-0:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 130/171 [00:39<00:12,  3.28it/s]\u001b[A\n","train-0:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 131/171 [00:39<00:10,  3.73it/s]\u001b[A\n","train-0:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 132/171 [00:40<00:11,  3.55it/s]\u001b[A\n","train-0:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 133/171 [00:40<00:11,  3.26it/s]\u001b[A\n","train-0:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 134/171 [00:40<00:11,  3.15it/s]\u001b[A\n","train-0:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 135/171 [00:41<00:11,  3.12it/s]\u001b[A\n","train-0:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 136/171 [00:41<00:11,  3.17it/s]\u001b[A\n","train-0:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 137/171 [00:41<00:10,  3.12it/s]\u001b[A\n","train-0:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 138/171 [00:41<00:09,  3.46it/s]\u001b[A\n","train-0:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 139/171 [00:42<00:09,  3.52it/s]\u001b[A\n","train-0:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 140/171 [00:42<00:08,  3.64it/s]\u001b[A\n","train-0:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 141/171 [00:42<00:08,  3.63it/s]\u001b[A\n","train-0:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 142/171 [00:43<00:08,  3.56it/s]\u001b[A\n","train-0:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 143/171 [00:43<00:08,  3.38it/s]\u001b[A\n","train-0:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 144/171 [00:43<00:07,  3.43it/s]\u001b[A\n","train-0:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 145/171 [00:43<00:07,  3.38it/s]\u001b[A\n","train-0:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 146/171 [00:44<00:08,  3.11it/s]\u001b[A\n","train-0:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 147/171 [00:44<00:07,  3.21it/s]\u001b[A\n","train-0:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 148/171 [00:44<00:07,  3.06it/s]\u001b[A\n","train-0:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 149/171 [00:45<00:06,  3.18it/s]\u001b[A\n","train-0:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 150/171 [00:45<00:06,  3.04it/s]\u001b[A\n","train-0:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 151/171 [00:45<00:06,  3.13it/s]\u001b[A\n","train-0:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 152/171 [00:46<00:05,  3.22it/s]\u001b[A\n","train-0:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 153/171 [00:46<00:06,  2.88it/s]\u001b[A\n","train-0:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 154/171 [00:47<00:06,  2.79it/s]\u001b[A\n","train-0:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 155/171 [00:47<00:05,  2.89it/s]\u001b[A\n","train-0:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 156/171 [00:47<00:04,  3.05it/s]\u001b[A\n","train-0:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 157/171 [00:47<00:04,  3.32it/s]\u001b[A\n","train-0:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 158/171 [00:48<00:03,  3.66it/s]\u001b[A\n","train-0:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 159/171 [00:48<00:03,  3.60it/s]\u001b[A\n","train-0:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 160/171 [00:48<00:03,  3.38it/s]\u001b[A\n","train-0:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 161/171 [00:49<00:03,  3.09it/s]\u001b[A\n","train-0:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 162/171 [00:49<00:02,  3.35it/s]\u001b[A\n","train-0:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 163/171 [00:49<00:02,  3.34it/s]\u001b[A\n","train-0:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 164/171 [00:49<00:02,  3.44it/s]\u001b[A\n","train-0:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 165/171 [00:50<00:01,  3.37it/s]\u001b[A\n","train-0:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 166/171 [00:50<00:01,  3.38it/s]\u001b[A\n","train-0:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 167/171 [00:50<00:01,  3.31it/s]\u001b[A\n","train-0:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 168/171 [00:51<00:00,  3.30it/s]\u001b[A\n","train-0:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 169/171 [00:51<00:00,  3.46it/s]\u001b[A\n","train-0:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 170/171 [00:51<00:00,  3.27it/s]\u001b[A\n","train-0: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 171/171 [00:51<00:00,  3.29it/s]\n","\n","eval:   0%|          | 0/171 [00:00<?, ?it/s]\u001b[A\n","eval:   1%|          | 1/171 [00:00<00:17,  9.71it/s]\u001b[A\n","eval:   1%|          | 2/171 [00:00<00:17,  9.77it/s]\u001b[A\n","eval:   2%|‚ñè         | 3/171 [00:00<00:17,  9.47it/s]\u001b[A\n","eval:   3%|‚ñé         | 5/171 [00:00<00:16, 10.13it/s]\u001b[A\n","eval:   4%|‚ñç         | 7/171 [00:00<00:15, 10.91it/s]\u001b[A\n","eval:   5%|‚ñå         | 9/171 [00:00<00:15, 10.64it/s]\u001b[A\n","eval:   6%|‚ñã         | 11/171 [00:01<00:15, 10.19it/s]\u001b[A\n","eval:   8%|‚ñä         | 13/171 [00:01<00:15, 10.37it/s]\u001b[A\n","eval:   9%|‚ñâ         | 15/171 [00:01<00:15,  9.95it/s]\u001b[A\n","eval:  10%|‚ñâ         | 17/171 [00:01<00:15,  9.76it/s]\u001b[A\n","eval:  11%|‚ñà         | 18/171 [00:01<00:15,  9.72it/s]\u001b[A\n","eval:  11%|‚ñà         | 19/171 [00:01<00:16,  9.01it/s]\u001b[A\n","eval:  12%|‚ñà‚ñè        | 20/171 [00:02<00:18,  8.37it/s]\u001b[A\n","eval:  12%|‚ñà‚ñè        | 21/171 [00:02<00:17,  8.48it/s]\u001b[A\n","eval:  13%|‚ñà‚ñé        | 22/171 [00:02<00:17,  8.63it/s]\u001b[A\n","eval:  13%|‚ñà‚ñé        | 23/171 [00:02<00:17,  8.34it/s]\u001b[A\n","eval:  14%|‚ñà‚ñç        | 24/171 [00:02<00:17,  8.30it/s]\u001b[A\n","eval:  15%|‚ñà‚ñç        | 25/171 [00:02<00:17,  8.57it/s]\u001b[A\n","eval:  15%|‚ñà‚ñå        | 26/171 [00:02<00:16,  8.55it/s]\u001b[A\n","eval:  16%|‚ñà‚ñå        | 27/171 [00:02<00:16,  8.89it/s]\u001b[A\n","eval:  16%|‚ñà‚ñã        | 28/171 [00:02<00:15,  8.98it/s]\u001b[A\n","eval:  17%|‚ñà‚ñã        | 29/171 [00:03<00:17,  8.05it/s]\u001b[A\n","eval:  18%|‚ñà‚ñä        | 31/171 [00:03<00:15,  9.09it/s]\u001b[A\n","eval:  19%|‚ñà‚ñä        | 32/171 [00:03<00:15,  9.19it/s]\u001b[A\n","eval:  20%|‚ñà‚ñâ        | 34/171 [00:03<00:14,  9.32it/s]\u001b[A\n","eval:  20%|‚ñà‚ñà        | 35/171 [00:03<00:14,  9.23it/s]\u001b[A\n","eval:  22%|‚ñà‚ñà‚ñè       | 37/171 [00:03<00:14,  9.55it/s]\u001b[A\n","eval:  22%|‚ñà‚ñà‚ñè       | 38/171 [00:04<00:14,  9.25it/s]\u001b[A\n","eval:  23%|‚ñà‚ñà‚ñé       | 40/171 [00:04<00:14,  9.34it/s]\u001b[A\n","eval:  25%|‚ñà‚ñà‚ñç       | 42/171 [00:04<00:13,  9.26it/s]\u001b[A\n","eval:  25%|‚ñà‚ñà‚ñå       | 43/171 [00:04<00:14,  9.12it/s]\u001b[A\n","eval:  26%|‚ñà‚ñà‚ñå       | 44/171 [00:04<00:14,  8.74it/s]\u001b[A\n","eval:  26%|‚ñà‚ñà‚ñã       | 45/171 [00:04<00:14,  8.58it/s]\u001b[A\n","eval:  27%|‚ñà‚ñà‚ñã       | 46/171 [00:04<00:14,  8.69it/s]\u001b[A\n","eval:  27%|‚ñà‚ñà‚ñã       | 47/171 [00:05<00:14,  8.78it/s]\u001b[A\n","eval:  29%|‚ñà‚ñà‚ñä       | 49/171 [00:05<00:13,  9.07it/s]\u001b[A\n","eval:  29%|‚ñà‚ñà‚ñâ       | 50/171 [00:05<00:13,  8.84it/s]\u001b[A\n","eval:  30%|‚ñà‚ñà‚ñâ       | 51/171 [00:05<00:13,  8.70it/s]\u001b[A\n","eval:  30%|‚ñà‚ñà‚ñà       | 52/171 [00:05<00:13,  8.77it/s]\u001b[A\n","eval:  32%|‚ñà‚ñà‚ñà‚ñè      | 54/171 [00:05<00:12,  9.44it/s]\u001b[A\n","eval:  32%|‚ñà‚ñà‚ñà‚ñè      | 55/171 [00:05<00:12,  9.40it/s]\u001b[A\n","eval:  33%|‚ñà‚ñà‚ñà‚ñé      | 57/171 [00:06<00:11,  9.71it/s]\u001b[A\n","eval:  34%|‚ñà‚ñà‚ñà‚ñç      | 58/171 [00:06<00:12,  9.21it/s]\u001b[A\n","eval:  35%|‚ñà‚ñà‚ñà‚ñç      | 59/171 [00:06<00:12,  8.89it/s]\u001b[A\n","eval:  36%|‚ñà‚ñà‚ñà‚ñå      | 61/171 [00:06<00:11,  9.18it/s]\u001b[A\n","eval:  36%|‚ñà‚ñà‚ñà‚ñã      | 62/171 [00:06<00:11,  9.09it/s]\u001b[A\n","eval:  37%|‚ñà‚ñà‚ñà‚ñã      | 63/171 [00:06<00:11,  9.24it/s]\u001b[A\n","eval:  38%|‚ñà‚ñà‚ñà‚ñä      | 65/171 [00:07<00:10,  9.65it/s]\u001b[A\n","eval:  39%|‚ñà‚ñà‚ñà‚ñä      | 66/171 [00:07<00:11,  9.41it/s]\u001b[A\n","eval:  40%|‚ñà‚ñà‚ñà‚ñâ      | 68/171 [00:07<00:10,  9.83it/s]\u001b[A\n","eval:  40%|‚ñà‚ñà‚ñà‚ñà      | 69/171 [00:07<00:10,  9.40it/s]\u001b[A\n","eval:  41%|‚ñà‚ñà‚ñà‚ñà      | 70/171 [00:07<00:10,  9.35it/s]\u001b[A\n","eval:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 72/171 [00:07<00:10,  9.78it/s]\u001b[A\n","eval:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 74/171 [00:07<00:08, 10.86it/s]\u001b[A\n","eval:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 76/171 [00:08<00:09,  9.99it/s]\u001b[A\n","eval:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 78/171 [00:08<00:09, 10.13it/s]\u001b[A\n","eval:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 80/171 [00:08<00:09,  9.99it/s]\u001b[A\n","eval:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 82/171 [00:08<00:09,  9.58it/s]\u001b[A\n","eval:  49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 83/171 [00:08<00:09,  9.64it/s]\u001b[A\n","eval:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 85/171 [00:09<00:08, 10.00it/s]\u001b[A\n","eval:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 87/171 [00:09<00:08,  9.78it/s]\u001b[A\n","eval:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 89/171 [00:09<00:08, 10.02it/s]\u001b[A\n","eval:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 91/171 [00:09<00:08,  9.66it/s]\u001b[A\n","eval:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 93/171 [00:09<00:07, 10.03it/s]\u001b[A\n","eval:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 95/171 [00:10<00:07, 10.23it/s]\u001b[A\n","eval:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 97/171 [00:10<00:06, 10.73it/s]\u001b[A\n","eval:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 99/171 [00:10<00:06, 10.69it/s]\u001b[A\n","eval:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 101/171 [00:10<00:07,  9.64it/s]\u001b[A\n","eval:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 102/171 [00:10<00:07,  9.26it/s]\u001b[A\n","eval:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 103/171 [00:10<00:07,  8.83it/s]\u001b[A\n","eval:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 104/171 [00:11<00:07,  8.58it/s]\u001b[A\n","eval:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 105/171 [00:11<00:07,  8.82it/s]\u001b[A\n","eval:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 106/171 [00:11<00:07,  8.91it/s]\u001b[A\n","eval:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 107/171 [00:11<00:07,  8.79it/s]\u001b[A\n","eval:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 109/171 [00:11<00:06,  9.48it/s]\u001b[A\n","eval:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 111/171 [00:11<00:05, 10.79it/s]\u001b[A\n","eval:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 113/171 [00:11<00:04, 11.85it/s]\u001b[A\n","eval:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 115/171 [00:12<00:05, 10.56it/s]\u001b[A\n","eval:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 117/171 [00:12<00:05, 10.69it/s]\u001b[A\n","eval:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 119/171 [00:12<00:04, 10.88it/s]\u001b[A\n","eval:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 121/171 [00:12<00:04, 10.54it/s]\u001b[A\n","eval:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 123/171 [00:12<00:04,  9.96it/s]\u001b[A\n","eval:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 125/171 [00:13<00:04,  9.91it/s]\u001b[A\n","eval:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 127/171 [00:13<00:04,  9.47it/s]\u001b[A\n","eval:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 128/171 [00:13<00:04,  9.48it/s]\u001b[A\n","eval:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 129/171 [00:13<00:04,  9.45it/s]\u001b[A\n","eval:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 131/171 [00:13<00:04,  9.78it/s]\u001b[A\n","eval:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 132/171 [00:13<00:04,  9.36it/s]\u001b[A\n","eval:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 133/171 [00:13<00:04,  9.16it/s]\u001b[A\n","eval:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 134/171 [00:14<00:04,  9.21it/s]\u001b[A\n","eval:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 135/171 [00:14<00:03,  9.10it/s]\u001b[A\n","eval:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 137/171 [00:14<00:03, 10.66it/s]\u001b[A\n","eval:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 139/171 [00:14<00:02, 10.87it/s]\u001b[A\n","eval:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 141/171 [00:14<00:02, 10.54it/s]\u001b[A\n","eval:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 143/171 [00:14<00:02, 10.66it/s]\u001b[A\n","eval:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 145/171 [00:15<00:02, 10.48it/s]\u001b[A\n","eval:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 147/171 [00:15<00:02,  9.56it/s]\u001b[A\n","eval:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 148/171 [00:15<00:02,  9.04it/s]\u001b[A\n","eval:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 150/171 [00:15<00:02,  9.72it/s]\u001b[A\n","eval:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 151/171 [00:15<00:02,  9.64it/s]\u001b[A\n","eval:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 153/171 [00:15<00:01,  9.52it/s]\u001b[A\n","eval:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 154/171 [00:16<00:01,  9.38it/s]\u001b[A\n","eval:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 155/171 [00:16<00:01,  9.36it/s]\u001b[A\n","eval:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 156/171 [00:16<00:01,  9.20it/s]\u001b[A\n","eval:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 157/171 [00:16<00:01,  8.88it/s]\u001b[A\n","eval:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 159/171 [00:16<00:01,  9.33it/s]\u001b[A\n","eval:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 160/171 [00:16<00:01,  9.21it/s]\u001b[A\n","eval:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 161/171 [00:16<00:01,  8.89it/s]\u001b[A\n","eval:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 162/171 [00:16<00:01,  8.96it/s]\u001b[A\n","eval:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 163/171 [00:17<00:00,  9.14it/s]\u001b[A\n","eval:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 164/171 [00:17<00:00,  9.17it/s]\u001b[A\n","eval:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 166/171 [00:17<00:00, 10.19it/s]\u001b[A\n","eval:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 167/171 [00:17<00:00,  9.61it/s]\u001b[A\n","eval:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 168/171 [00:17<00:00,  9.27it/s]\u001b[A\n","eval: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 171/171 [00:17<00:00,  9.59it/s]\n","\n","eval:   0%|          | 0/25 [00:00<?, ?it/s]\u001b[A\n","eval:   4%|‚ñç         | 1/25 [00:00<00:02,  8.35it/s]\u001b[A\n","eval:   8%|‚ñä         | 2/25 [00:00<00:02,  8.06it/s]\u001b[A\n","eval:  12%|‚ñà‚ñè        | 3/25 [00:00<00:02,  8.78it/s]\u001b[A\n","eval:  20%|‚ñà‚ñà        | 5/25 [00:00<00:02,  9.65it/s]\u001b[A\n","eval:  24%|‚ñà‚ñà‚ñç       | 6/25 [00:00<00:02,  9.10it/s]\u001b[A\n","eval:  28%|‚ñà‚ñà‚ñä       | 7/25 [00:00<00:01,  9.05it/s]\u001b[A\n","eval:  32%|‚ñà‚ñà‚ñà‚ñè      | 8/25 [00:00<00:01,  8.73it/s]\u001b[A\n","eval:  36%|‚ñà‚ñà‚ñà‚ñå      | 9/25 [00:01<00:01,  8.70it/s]\u001b[A\n","eval:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 11/25 [00:01<00:01, 10.04it/s]\u001b[A\n","eval:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 13/25 [00:01<00:01, 10.92it/s]\u001b[A\n","eval:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 15/25 [00:01<00:00, 10.02it/s]\u001b[A\n","eval:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 17/25 [00:01<00:00, 10.02it/s]\u001b[A\n","eval:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 19/25 [00:01<00:00, 10.77it/s]\u001b[A\n","eval:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 21/25 [00:02<00:00, 10.08it/s]\u001b[A\n","eval:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 23/25 [00:02<00:00,  9.53it/s]\u001b[A\n","eval: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25/25 [00:02<00:00,  9.78it/s]\n"," 20%|‚ñà‚ñà        | 1/5 [01:20<05:20, 80.01s/it]"]},{"name":"stdout","output_type":"stream","text":["save the model to finetune-5-2e-05.pt\n","epoch 0: train loss :: 0.972, train acc :: 0.501, dev acc :: 0.502\n"]},{"name":"stderr","output_type":"stream","text":["\n","train-1:   0%|          | 0/171 [00:00<?, ?it/s]\u001b[A\n","train-1:   1%|          | 1/171 [00:00<00:56,  2.98it/s]\u001b[A\n","train-1:   1%|          | 2/171 [00:00<00:52,  3.22it/s]\u001b[A\n","train-1:   2%|‚ñè         | 3/171 [00:00<00:51,  3.24it/s]\u001b[A\n","train-1:   2%|‚ñè         | 4/171 [00:01<00:45,  3.69it/s]\u001b[A\n","train-1:   3%|‚ñé         | 5/171 [00:01<00:45,  3.62it/s]\u001b[A\n","train-1:   4%|‚ñé         | 6/171 [00:01<00:44,  3.69it/s]\u001b[A\n","train-1:   4%|‚ñç         | 7/171 [00:02<00:46,  3.52it/s]\u001b[A\n","train-1:   5%|‚ñç         | 8/171 [00:02<00:47,  3.45it/s]\u001b[A\n","train-1:   5%|‚ñå         | 9/171 [00:02<00:54,  2.99it/s]\u001b[A\n","train-1:   6%|‚ñå         | 10/171 [00:03<00:55,  2.91it/s]\u001b[A\n","train-1:   6%|‚ñã         | 11/171 [00:03<00:50,  3.18it/s]\u001b[A\n","train-1:   7%|‚ñã         | 12/171 [00:03<00:47,  3.36it/s]\u001b[A\n","train-1:   8%|‚ñä         | 13/171 [00:03<00:46,  3.39it/s]\u001b[A\n","train-1:   8%|‚ñä         | 14/171 [00:04<00:46,  3.41it/s]\u001b[A\n","train-1:   9%|‚ñâ         | 15/171 [00:04<00:48,  3.24it/s]\u001b[A\n","train-1:   9%|‚ñâ         | 16/171 [00:04<00:45,  3.44it/s]\u001b[A\n","train-1:  10%|‚ñâ         | 17/171 [00:05<00:46,  3.30it/s]\u001b[A\n","train-1:  11%|‚ñà         | 18/171 [00:05<00:46,  3.29it/s]\u001b[A\n","train-1:  11%|‚ñà         | 19/171 [00:05<00:45,  3.37it/s]\u001b[A\n","train-1:  12%|‚ñà‚ñè        | 20/171 [00:05<00:43,  3.51it/s]\u001b[A\n","train-1:  12%|‚ñà‚ñè        | 21/171 [00:06<00:41,  3.61it/s]\u001b[A\n","train-1:  13%|‚ñà‚ñé        | 22/171 [00:06<00:40,  3.67it/s]\u001b[A\n","train-1:  13%|‚ñà‚ñé        | 23/171 [00:06<00:42,  3.48it/s]\u001b[A\n","train-1:  14%|‚ñà‚ñç        | 24/171 [00:06<00:38,  3.85it/s]\u001b[A\n","train-1:  15%|‚ñà‚ñç        | 25/171 [00:07<00:35,  4.11it/s]\u001b[A\n","train-1:  15%|‚ñà‚ñå        | 26/171 [00:07<00:39,  3.65it/s]\u001b[A\n","train-1:  16%|‚ñà‚ñå        | 27/171 [00:07<00:40,  3.55it/s]\u001b[A\n","train-1:  16%|‚ñà‚ñã        | 28/171 [00:08<00:39,  3.61it/s]\u001b[A\n","train-1:  17%|‚ñà‚ñã        | 29/171 [00:08<00:42,  3.36it/s]\u001b[A\n","train-1:  18%|‚ñà‚ñä        | 30/171 [00:08<00:37,  3.74it/s]\u001b[A\n","train-1:  18%|‚ñà‚ñä        | 31/171 [00:08<00:39,  3.51it/s]\u001b[A\n","train-1:  19%|‚ñà‚ñä        | 32/171 [00:09<00:42,  3.29it/s]\u001b[A\n","train-1:  19%|‚ñà‚ñâ        | 33/171 [00:09<00:41,  3.31it/s]\u001b[A\n","train-1:  20%|‚ñà‚ñâ        | 34/171 [00:09<00:38,  3.57it/s]\u001b[A\n","train-1:  20%|‚ñà‚ñà        | 35/171 [00:10<00:42,  3.23it/s]\u001b[A\n","train-1:  21%|‚ñà‚ñà        | 36/171 [00:10<00:42,  3.15it/s]\u001b[A\n","train-1:  22%|‚ñà‚ñà‚ñè       | 37/171 [00:10<00:41,  3.24it/s]\u001b[A\n","train-1:  22%|‚ñà‚ñà‚ñè       | 38/171 [00:11<00:42,  3.15it/s]\u001b[A\n","train-1:  23%|‚ñà‚ñà‚ñé       | 39/171 [00:11<00:40,  3.23it/s]\u001b[A\n","train-1:  23%|‚ñà‚ñà‚ñé       | 40/171 [00:11<00:43,  3.01it/s]\u001b[A\n","train-1:  24%|‚ñà‚ñà‚ñç       | 41/171 [00:12<00:42,  3.06it/s]\u001b[A\n","train-1:  25%|‚ñà‚ñà‚ñç       | 42/171 [00:12<00:42,  3.02it/s]\u001b[A\n","train-1:  25%|‚ñà‚ñà‚ñå       | 43/171 [00:12<00:42,  3.01it/s]\u001b[A\n","train-1:  26%|‚ñà‚ñà‚ñå       | 44/171 [00:13<00:40,  3.11it/s]\u001b[A\n","train-1:  26%|‚ñà‚ñà‚ñã       | 45/171 [00:13<00:37,  3.35it/s]\u001b[A\n","train-1:  27%|‚ñà‚ñà‚ñã       | 46/171 [00:13<00:34,  3.61it/s]\u001b[A\n","train-1:  27%|‚ñà‚ñà‚ñã       | 47/171 [00:13<00:35,  3.49it/s]\u001b[A\n","train-1:  28%|‚ñà‚ñà‚ñä       | 48/171 [00:14<00:33,  3.64it/s]\u001b[A\n","train-1:  29%|‚ñà‚ñà‚ñä       | 49/171 [00:14<00:35,  3.48it/s]\u001b[A\n","train-1:  29%|‚ñà‚ñà‚ñâ       | 50/171 [00:14<00:33,  3.63it/s]\u001b[A\n","train-1:  30%|‚ñà‚ñà‚ñâ       | 51/171 [00:15<00:35,  3.37it/s]\u001b[A\n","train-1:  30%|‚ñà‚ñà‚ñà       | 52/171 [00:15<00:36,  3.30it/s]\u001b[A\n","train-1:  31%|‚ñà‚ñà‚ñà       | 53/171 [00:15<00:34,  3.44it/s]\u001b[A\n","train-1:  32%|‚ñà‚ñà‚ñà‚ñè      | 54/171 [00:15<00:29,  3.94it/s]\u001b[A\n","train-1:  32%|‚ñà‚ñà‚ñà‚ñè      | 55/171 [00:16<00:30,  3.82it/s]\u001b[A\n","train-1:  33%|‚ñà‚ñà‚ñà‚ñé      | 56/171 [00:16<00:35,  3.27it/s]\u001b[A\n","train-1:  33%|‚ñà‚ñà‚ñà‚ñé      | 57/171 [00:16<00:35,  3.24it/s]\u001b[A\n","train-1:  34%|‚ñà‚ñà‚ñà‚ñç      | 58/171 [00:17<00:37,  3.03it/s]\u001b[A\n","train-1:  35%|‚ñà‚ñà‚ñà‚ñç      | 59/171 [00:17<00:35,  3.12it/s]\u001b[A\n","train-1:  35%|‚ñà‚ñà‚ñà‚ñå      | 60/171 [00:17<00:37,  2.97it/s]\u001b[A\n","train-1:  36%|‚ñà‚ñà‚ñà‚ñå      | 61/171 [00:18<00:35,  3.06it/s]\u001b[A\n","train-1:  36%|‚ñà‚ñà‚ñà‚ñã      | 62/171 [00:18<00:36,  2.95it/s]\u001b[A\n","train-1:  37%|‚ñà‚ñà‚ñà‚ñã      | 63/171 [00:18<00:35,  3.08it/s]\u001b[A\n","train-1:  37%|‚ñà‚ñà‚ñà‚ñã      | 64/171 [00:19<00:36,  2.94it/s]\u001b[A\n","train-1:  38%|‚ñà‚ñà‚ñà‚ñä      | 65/171 [00:19<00:33,  3.16it/s]\u001b[A\n","train-1:  39%|‚ñà‚ñà‚ñà‚ñä      | 66/171 [00:19<00:33,  3.14it/s]\u001b[A\n","train-1:  39%|‚ñà‚ñà‚ñà‚ñâ      | 67/171 [00:20<00:33,  3.15it/s]\u001b[A\n","train-1:  40%|‚ñà‚ñà‚ñà‚ñâ      | 68/171 [00:20<00:30,  3.36it/s]\u001b[A\n","train-1:  40%|‚ñà‚ñà‚ñà‚ñà      | 69/171 [00:20<00:31,  3.27it/s]\u001b[A\n","train-1:  41%|‚ñà‚ñà‚ñà‚ñà      | 70/171 [00:20<00:28,  3.60it/s]\u001b[A\n","train-1:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 71/171 [00:21<00:26,  3.70it/s]\u001b[A\n","train-1:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 72/171 [00:21<00:26,  3.76it/s]\u001b[A\n","train-1:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 73/171 [00:21<00:24,  4.01it/s]\u001b[A\n","train-1:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 74/171 [00:22<00:28,  3.42it/s]\u001b[A\n","train-1:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 75/171 [00:22<00:30,  3.12it/s]\u001b[A\n","train-1:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 76/171 [00:22<00:29,  3.23it/s]\u001b[A\n","train-1:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 77/171 [00:22<00:27,  3.40it/s]\u001b[A\n","train-1:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 78/171 [00:23<00:29,  3.17it/s]\u001b[A\n","train-1:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 79/171 [00:23<00:28,  3.22it/s]\u001b[A\n","train-1:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 80/171 [00:23<00:25,  3.53it/s]\u001b[A\n","train-1:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 81/171 [00:24<00:23,  3.76it/s]\u001b[A\n","train-1:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 82/171 [00:24<00:23,  3.86it/s]\u001b[A\n","train-1:  49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 83/171 [00:24<00:24,  3.66it/s]\u001b[A\n","train-1:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 84/171 [00:24<00:24,  3.49it/s]\u001b[A\n","train-1:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 85/171 [00:25<00:24,  3.56it/s]\u001b[A\n","train-1:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 86/171 [00:25<00:24,  3.49it/s]\u001b[A\n","train-1:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 87/171 [00:25<00:23,  3.57it/s]\u001b[A\n","train-1:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 88/171 [00:26<00:23,  3.60it/s]\u001b[A\n","train-1:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 89/171 [00:26<00:23,  3.48it/s]\u001b[A\n","train-1:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 90/171 [00:26<00:23,  3.48it/s]\u001b[A\n","train-1:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 91/171 [00:26<00:22,  3.63it/s]\u001b[A\n","train-1:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 92/171 [00:27<00:21,  3.60it/s]\u001b[A\n","train-1:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 93/171 [00:27<00:22,  3.52it/s]\u001b[A\n","train-1:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 94/171 [00:27<00:21,  3.57it/s]\u001b[A\n","train-1:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 95/171 [00:28<00:21,  3.46it/s]\u001b[A\n","train-1:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 96/171 [00:28<00:19,  3.77it/s]\u001b[A\n","train-1:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 97/171 [00:28<00:21,  3.48it/s]\u001b[A\n","train-1:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 98/171 [00:28<00:19,  3.65it/s]\u001b[A\n","train-1:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 99/171 [00:29<00:20,  3.55it/s]\u001b[A\n","train-1:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 100/171 [00:29<00:20,  3.52it/s]\u001b[A\n","train-1:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 101/171 [00:29<00:19,  3.68it/s]\u001b[A\n","train-1:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 102/171 [00:30<00:20,  3.35it/s]\u001b[A\n","train-1:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 103/171 [00:30<00:21,  3.20it/s]\u001b[A\n","train-1:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 104/171 [00:30<00:21,  3.10it/s]\u001b[A\n","train-1:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 105/171 [00:31<00:20,  3.27it/s]\u001b[A\n","train-1:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 106/171 [00:31<00:20,  3.15it/s]\u001b[A\n","train-1:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 107/171 [00:31<00:20,  3.18it/s]\u001b[A\n","train-1:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 108/171 [00:31<00:19,  3.31it/s]\u001b[A\n","train-1:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 109/171 [00:32<00:19,  3.20it/s]\u001b[A\n","train-1:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 110/171 [00:32<00:18,  3.21it/s]\u001b[A\n","train-1:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 111/171 [00:32<00:16,  3.54it/s]\u001b[A\n","train-1:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 112/171 [00:33<00:16,  3.58it/s]\u001b[A\n","train-1:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 113/171 [00:33<00:17,  3.39it/s]\u001b[A\n","train-1:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 114/171 [00:33<00:17,  3.28it/s]\u001b[A\n","train-1:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 115/171 [00:34<00:16,  3.34it/s]\u001b[A\n","train-1:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 116/171 [00:34<00:16,  3.38it/s]\u001b[A\n","train-1:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 117/171 [00:34<00:16,  3.35it/s]\u001b[A\n","train-1:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 118/171 [00:34<00:15,  3.40it/s]\u001b[A\n","train-1:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 119/171 [00:35<00:14,  3.51it/s]\u001b[A\n","train-1:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 120/171 [00:35<00:15,  3.30it/s]\u001b[A\n","train-1:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 121/171 [00:35<00:15,  3.18it/s]\u001b[A\n","train-1:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 122/171 [00:36<00:14,  3.42it/s]\u001b[A\n","train-1:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 123/171 [00:36<00:15,  3.20it/s]\u001b[A\n","train-1:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 124/171 [00:36<00:14,  3.28it/s]\u001b[A\n","train-1:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 125/171 [00:37<00:14,  3.24it/s]\u001b[A\n","train-1:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 126/171 [00:37<00:13,  3.30it/s]\u001b[A\n","train-1:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 127/171 [00:37<00:13,  3.30it/s]\u001b[A\n","train-1:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 128/171 [00:37<00:12,  3.38it/s]\u001b[A\n","train-1:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 129/171 [00:38<00:12,  3.42it/s]\u001b[A\n","train-1:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 130/171 [00:38<00:12,  3.32it/s]\u001b[A\n","train-1:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 131/171 [00:38<00:11,  3.43it/s]\u001b[A\n","train-1:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 132/171 [00:39<00:10,  3.60it/s]\u001b[A\n","train-1:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 133/171 [00:39<00:10,  3.72it/s]\u001b[A\n","train-1:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 134/171 [00:39<00:11,  3.32it/s]\u001b[A\n","train-1:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 135/171 [00:40<00:11,  3.17it/s]\u001b[A\n","train-1:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 136/171 [00:40<00:10,  3.41it/s]\u001b[A\n","train-1:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 137/171 [00:40<00:09,  3.55it/s]\u001b[A\n","train-1:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 138/171 [00:40<00:09,  3.53it/s]\u001b[A\n","train-1:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 139/171 [00:41<00:09,  3.37it/s]\u001b[A\n","train-1:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 140/171 [00:41<00:08,  3.53it/s]\u001b[A\n","train-1:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 141/171 [00:41<00:09,  3.25it/s]\u001b[A\n","train-1:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 142/171 [00:42<00:09,  3.17it/s]\u001b[A\n","train-1:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 143/171 [00:42<00:09,  2.97it/s]\u001b[A\n","train-1:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 144/171 [00:42<00:08,  3.18it/s]\u001b[A\n","train-1:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 145/171 [00:42<00:07,  3.32it/s]\u001b[A\n","train-1:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 146/171 [00:43<00:06,  3.60it/s]\u001b[A\n","train-1:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 147/171 [00:43<00:06,  3.44it/s]\u001b[A\n","train-1:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 148/171 [00:43<00:06,  3.44it/s]\u001b[A\n","train-1:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 149/171 [00:44<00:05,  3.70it/s]\u001b[A\n","train-1:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 150/171 [00:44<00:05,  3.63it/s]\u001b[A\n","train-1:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 151/171 [00:44<00:05,  3.51it/s]\u001b[A\n","train-1:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 152/171 [00:44<00:05,  3.36it/s]\u001b[A\n","train-1:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 153/171 [00:45<00:05,  3.26it/s]\u001b[A\n","train-1:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 154/171 [00:45<00:05,  3.31it/s]\u001b[A\n","train-1:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 155/171 [00:45<00:04,  3.44it/s]\u001b[A\n","train-1:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 156/171 [00:46<00:04,  3.40it/s]\u001b[A\n","train-1:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 157/171 [00:46<00:03,  3.56it/s]\u001b[A\n","train-1:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 158/171 [00:46<00:03,  3.35it/s]\u001b[A\n","train-1:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 159/171 [00:47<00:03,  3.41it/s]\u001b[A\n","train-1:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 160/171 [00:47<00:03,  3.60it/s]\u001b[A\n","train-1:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 161/171 [00:47<00:02,  3.73it/s]\u001b[A\n","train-1:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 162/171 [00:47<00:02,  3.60it/s]\u001b[A\n","train-1:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 163/171 [00:48<00:02,  3.54it/s]\u001b[A\n","train-1:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 164/171 [00:48<00:02,  3.49it/s]\u001b[A\n","train-1:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 165/171 [00:48<00:01,  3.43it/s]\u001b[A\n","train-1:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 166/171 [00:49<00:01,  3.21it/s]\u001b[A\n","train-1:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 167/171 [00:49<00:01,  3.55it/s]\u001b[A\n","train-1:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 168/171 [00:49<00:00,  3.78it/s]\u001b[A\n","train-1:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 169/171 [00:49<00:00,  3.70it/s]\u001b[A\n","train-1:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 170/171 [00:50<00:00,  3.53it/s]\u001b[A\n","train-1: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 171/171 [00:50<00:00,  3.39it/s]\n","\n","eval:   0%|          | 0/171 [00:00<?, ?it/s]\u001b[A\n","eval:   1%|          | 2/171 [00:00<00:13, 12.08it/s]\u001b[A\n","eval:   2%|‚ñè         | 4/171 [00:00<00:16,  9.98it/s]\u001b[A\n","eval:   4%|‚ñé         | 6/171 [00:00<00:14, 11.02it/s]\u001b[A\n","eval:   5%|‚ñç         | 8/171 [00:00<00:16,  9.67it/s]\u001b[A\n","eval:   6%|‚ñå         | 10/171 [00:01<00:17,  9.38it/s]\u001b[A\n","eval:   6%|‚ñã         | 11/171 [00:01<00:17,  9.36it/s]\u001b[A\n","eval:   7%|‚ñã         | 12/171 [00:01<00:16,  9.44it/s]\u001b[A\n","eval:   8%|‚ñä         | 13/171 [00:01<00:16,  9.52it/s]\u001b[A\n","eval:   9%|‚ñâ         | 15/171 [00:01<00:15, 10.27it/s]\u001b[A\n","eval:  10%|‚ñâ         | 17/171 [00:01<00:15,  9.68it/s]\u001b[A\n","eval:  11%|‚ñà         | 19/171 [00:01<00:15, 10.10it/s]\u001b[A\n","eval:  12%|‚ñà‚ñè        | 21/171 [00:02<00:13, 10.81it/s]\u001b[A\n","eval:  13%|‚ñà‚ñé        | 23/171 [00:02<00:15,  9.66it/s]\u001b[A\n","eval:  15%|‚ñà‚ñç        | 25/171 [00:02<00:13, 10.48it/s]\u001b[A\n","eval:  16%|‚ñà‚ñå        | 27/171 [00:02<00:14,  9.86it/s]\u001b[A\n","eval:  17%|‚ñà‚ñã        | 29/171 [00:02<00:14,  9.68it/s]\u001b[A\n","eval:  18%|‚ñà‚ñä        | 30/171 [00:03<00:14,  9.57it/s]\u001b[A\n","eval:  18%|‚ñà‚ñä        | 31/171 [00:03<00:14,  9.54it/s]\u001b[A\n","eval:  19%|‚ñà‚ñä        | 32/171 [00:03<00:14,  9.48it/s]\u001b[A\n","eval:  19%|‚ñà‚ñâ        | 33/171 [00:03<00:15,  9.16it/s]\u001b[A\n","eval:  20%|‚ñà‚ñà        | 35/171 [00:03<00:13,  9.90it/s]\u001b[A\n","eval:  22%|‚ñà‚ñà‚ñè       | 37/171 [00:03<00:13, 10.01it/s]\u001b[A\n","eval:  22%|‚ñà‚ñà‚ñè       | 38/171 [00:03<00:13,  9.97it/s]\u001b[A\n","eval:  23%|‚ñà‚ñà‚ñé       | 40/171 [00:03<00:11, 11.20it/s]\u001b[A\n","eval:  25%|‚ñà‚ñà‚ñç       | 42/171 [00:04<00:11, 11.31it/s]\u001b[A\n","eval:  26%|‚ñà‚ñà‚ñå       | 44/171 [00:04<00:11, 11.50it/s]\u001b[A\n","eval:  27%|‚ñà‚ñà‚ñã       | 46/171 [00:04<00:10, 11.66it/s]\u001b[A\n","eval:  28%|‚ñà‚ñà‚ñä       | 48/171 [00:04<00:10, 11.80it/s]\u001b[A\n","eval:  29%|‚ñà‚ñà‚ñâ       | 50/171 [00:04<00:10, 11.25it/s]\u001b[A\n","eval:  30%|‚ñà‚ñà‚ñà       | 52/171 [00:05<00:11, 10.72it/s]\u001b[A\n","eval:  32%|‚ñà‚ñà‚ñà‚ñè      | 54/171 [00:05<00:10, 10.72it/s]\u001b[A\n","eval:  33%|‚ñà‚ñà‚ñà‚ñé      | 56/171 [00:05<00:10, 11.00it/s]\u001b[A\n","eval:  34%|‚ñà‚ñà‚ñà‚ñç      | 58/171 [00:05<00:10, 11.07it/s]\u001b[A\n","eval:  35%|‚ñà‚ñà‚ñà‚ñå      | 60/171 [00:05<00:10, 10.88it/s]\u001b[A\n","eval:  36%|‚ñà‚ñà‚ñà‚ñã      | 62/171 [00:06<00:10, 10.33it/s]\u001b[A\n","eval:  37%|‚ñà‚ñà‚ñà‚ñã      | 64/171 [00:06<00:10, 10.51it/s]\u001b[A\n","eval:  39%|‚ñà‚ñà‚ñà‚ñä      | 66/171 [00:06<00:10, 10.33it/s]\u001b[A\n","eval:  40%|‚ñà‚ñà‚ñà‚ñâ      | 68/171 [00:06<00:10, 10.26it/s]\u001b[A\n","eval:  41%|‚ñà‚ñà‚ñà‚ñà      | 70/171 [00:06<00:09, 10.59it/s]\u001b[A\n","eval:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 72/171 [00:07<00:10,  9.73it/s]\u001b[A\n","eval:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 73/171 [00:07<00:10,  9.71it/s]\u001b[A\n","eval:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 75/171 [00:07<00:09,  9.96it/s]\u001b[A\n","eval:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 77/171 [00:07<00:09,  9.46it/s]\u001b[A\n","eval:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 78/171 [00:07<00:09,  9.47it/s]\u001b[A\n","eval:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 80/171 [00:07<00:09,  9.72it/s]\u001b[A\n","eval:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 81/171 [00:07<00:09,  9.53it/s]\u001b[A\n","eval:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 82/171 [00:08<00:09,  9.10it/s]\u001b[A\n","eval:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 84/171 [00:08<00:08,  9.79it/s]\u001b[A\n","eval:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 85/171 [00:08<00:09,  9.23it/s]\u001b[A\n","eval:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 86/171 [00:08<00:09,  8.86it/s]\u001b[A\n","eval:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 88/171 [00:08<00:08,  9.48it/s]\u001b[A\n","eval:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 89/171 [00:08<00:08,  9.40it/s]\u001b[A\n","eval:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 90/171 [00:08<00:08,  9.02it/s]\u001b[A\n","eval:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 91/171 [00:09<00:08,  9.16it/s]\u001b[A\n","eval:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 92/171 [00:09<00:08,  9.20it/s]\u001b[A\n","eval:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 93/171 [00:09<00:08,  9.07it/s]\u001b[A\n","eval:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 95/171 [00:09<00:07,  9.79it/s]\u001b[A\n","eval:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 96/171 [00:09<00:08,  9.17it/s]\u001b[A\n","eval:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 97/171 [00:09<00:08,  8.73it/s]\u001b[A\n","eval:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 99/171 [00:09<00:07,  9.76it/s]\u001b[A\n","eval:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 100/171 [00:10<00:07,  9.09it/s]\u001b[A\n","eval:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 101/171 [00:10<00:07,  8.98it/s]\u001b[A\n","eval:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 103/171 [00:10<00:06,  9.81it/s]\u001b[A\n","eval:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 104/171 [00:10<00:07,  9.47it/s]\u001b[A\n","eval:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 106/171 [00:10<00:06,  9.32it/s]\u001b[A\n","eval:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 108/171 [00:10<00:06,  9.67it/s]\u001b[A\n","eval:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 110/171 [00:11<00:06,  9.74it/s]\u001b[A\n","eval:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 112/171 [00:11<00:05, 10.30it/s]\u001b[A\n","eval:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 114/171 [00:11<00:05,  9.95it/s]\u001b[A\n","eval:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 115/171 [00:11<00:05,  9.80it/s]\u001b[A\n","eval:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 116/171 [00:11<00:06,  8.86it/s]\u001b[A\n","eval:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 118/171 [00:11<00:05,  9.19it/s]\u001b[A\n","eval:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 119/171 [00:12<00:05,  8.95it/s]\u001b[A\n","eval:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 121/171 [00:12<00:05,  9.71it/s]\u001b[A\n","eval:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 122/171 [00:12<00:05,  9.56it/s]\u001b[A\n","eval:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 123/171 [00:12<00:05,  9.13it/s]\u001b[A\n","eval:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 124/171 [00:12<00:05,  8.84it/s]\u001b[A\n","eval:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 125/171 [00:12<00:05,  8.35it/s]\u001b[A\n","eval:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 127/171 [00:12<00:04,  9.75it/s]\u001b[A\n","eval:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 128/171 [00:12<00:04,  9.64it/s]\u001b[A\n","eval:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 130/171 [00:13<00:04,  9.66it/s]\u001b[A\n","eval:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 131/171 [00:13<00:04,  9.32it/s]\u001b[A\n","eval:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 132/171 [00:13<00:04,  8.93it/s]\u001b[A\n","eval:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 133/171 [00:13<00:04,  8.79it/s]\u001b[A\n","eval:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 134/171 [00:13<00:04,  8.83it/s]\u001b[A\n","eval:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 135/171 [00:13<00:04,  8.99it/s]\u001b[A\n","eval:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 136/171 [00:13<00:03,  9.01it/s]\u001b[A\n","eval:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 137/171 [00:13<00:03,  8.77it/s]\u001b[A\n","eval:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 138/171 [00:14<00:03,  8.34it/s]\u001b[A\n","eval:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 139/171 [00:14<00:03,  8.28it/s]\u001b[A\n","eval:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 141/171 [00:14<00:03,  9.48it/s]\u001b[A\n","eval:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 143/171 [00:14<00:02,  9.75it/s]\u001b[A\n","eval:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 144/171 [00:14<00:02,  9.78it/s]\u001b[A\n","eval:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 145/171 [00:14<00:02,  9.76it/s]\u001b[A\n","eval:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 146/171 [00:14<00:02,  9.76it/s]\u001b[A\n","eval:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 148/171 [00:15<00:02, 10.35it/s]\u001b[A\n","eval:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 150/171 [00:15<00:02, 10.29it/s]\u001b[A\n","eval:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 152/171 [00:15<00:01,  9.72it/s]\u001b[A\n","eval:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 154/171 [00:15<00:01, 10.09it/s]\u001b[A\n","eval:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 156/171 [00:15<00:01, 10.87it/s]\u001b[A\n","eval:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 158/171 [00:16<00:01, 10.92it/s]\u001b[A\n","eval:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 160/171 [00:16<00:00, 11.38it/s]\u001b[A\n","eval:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 162/171 [00:16<00:00, 10.81it/s]\u001b[A\n","eval:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 164/171 [00:16<00:00, 10.69it/s]\u001b[A\n","eval:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 166/171 [00:16<00:00, 11.16it/s]\u001b[A\n","eval:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 168/171 [00:16<00:00, 11.08it/s]\u001b[A\n","eval: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 171/171 [00:17<00:00,  9.94it/s]\n","\n","eval:   0%|          | 0/25 [00:00<?, ?it/s]\u001b[A\n","eval:   4%|‚ñç         | 1/25 [00:00<00:02,  8.91it/s]\u001b[A\n","eval:   8%|‚ñä         | 2/25 [00:00<00:02,  8.48it/s]\u001b[A\n","eval:  16%|‚ñà‚ñå        | 4/25 [00:00<00:02, 10.00it/s]\u001b[A\n","eval:  20%|‚ñà‚ñà        | 5/25 [00:00<00:02,  9.78it/s]\u001b[A\n","eval:  24%|‚ñà‚ñà‚ñç       | 6/25 [00:00<00:02,  9.25it/s]\u001b[A\n","eval:  28%|‚ñà‚ñà‚ñä       | 7/25 [00:00<00:01,  9.21it/s]\u001b[A\n","eval:  32%|‚ñà‚ñà‚ñà‚ñè      | 8/25 [00:00<00:01,  9.01it/s]\u001b[A\n","eval:  36%|‚ñà‚ñà‚ñà‚ñå      | 9/25 [00:00<00:01,  9.00it/s]\u001b[A\n","eval:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 11/25 [00:01<00:01, 10.37it/s]\u001b[A\n","eval:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 13/25 [00:01<00:01, 11.06it/s]\u001b[A\n","eval:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 15/25 [00:01<00:01,  9.96it/s]\u001b[A\n","eval:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 17/25 [00:01<00:00,  9.85it/s]\u001b[A\n","eval:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 19/25 [00:01<00:00, 10.68it/s]\u001b[A\n","eval:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 21/25 [00:02<00:00, 10.04it/s]\u001b[A\n","eval:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 23/25 [00:02<00:00,  9.51it/s]\u001b[A\n","eval: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25/25 [00:02<00:00,  9.87it/s]\n"," 40%|‚ñà‚ñà‚ñà‚ñà      | 2/5 [02:30<03:42, 74.26s/it]"]},{"name":"stdout","output_type":"stream","text":["epoch 1: train loss :: 0.816, train acc :: 0.503, dev acc :: 0.502\n"]},{"name":"stderr","output_type":"stream","text":["\n","train-2:   0%|          | 0/171 [00:00<?, ?it/s]\u001b[A\n","train-2:   1%|          | 1/171 [00:00<00:50,  3.40it/s]\u001b[A\n","train-2:   1%|          | 2/171 [00:00<00:42,  3.96it/s]\u001b[A\n","train-2:   2%|‚ñè         | 3/171 [00:00<00:40,  4.17it/s]\u001b[A\n","train-2:   2%|‚ñè         | 4/171 [00:01<00:44,  3.76it/s]\u001b[A\n","train-2:   3%|‚ñé         | 5/171 [00:01<00:41,  4.03it/s]\u001b[A\n","train-2:   4%|‚ñé         | 6/171 [00:01<00:42,  3.91it/s]\u001b[A\n","train-2:   4%|‚ñç         | 7/171 [00:01<00:46,  3.52it/s]\u001b[A\n","train-2:   5%|‚ñç         | 8/171 [00:02<00:49,  3.30it/s]\u001b[A\n","train-2:   5%|‚ñå         | 9/171 [00:02<00:52,  3.10it/s]\u001b[A\n","train-2:   6%|‚ñå         | 10/171 [00:02<00:52,  3.08it/s]\u001b[A\n","train-2:   6%|‚ñã         | 11/171 [00:03<00:50,  3.18it/s]\u001b[A\n","train-2:   7%|‚ñã         | 12/171 [00:03<00:53,  2.99it/s]\u001b[A\n","train-2:   8%|‚ñä         | 13/171 [00:03<00:55,  2.85it/s]\u001b[A\n","train-2:   8%|‚ñä         | 14/171 [00:04<00:53,  2.92it/s]\u001b[A\n","train-2:   9%|‚ñâ         | 15/171 [00:04<00:51,  3.02it/s]\u001b[A\n","train-2:   9%|‚ñâ         | 16/171 [00:04<00:51,  3.03it/s]\u001b[A\n","train-2:  10%|‚ñâ         | 17/171 [00:05<00:49,  3.13it/s]\u001b[A\n","train-2:  11%|‚ñà         | 18/171 [00:05<00:43,  3.50it/s]\u001b[A\n","train-2:  11%|‚ñà         | 19/171 [00:05<00:45,  3.37it/s]\u001b[A\n","train-2:  12%|‚ñà‚ñè        | 20/171 [00:06<00:43,  3.48it/s]\u001b[A\n","train-2:  12%|‚ñà‚ñè        | 21/171 [00:06<00:46,  3.23it/s]\u001b[A\n","train-2:  13%|‚ñà‚ñé        | 22/171 [00:06<00:46,  3.19it/s]\u001b[A\n","train-2:  13%|‚ñà‚ñé        | 23/171 [00:07<00:46,  3.17it/s]\u001b[A\n","train-2:  14%|‚ñà‚ñç        | 24/171 [00:07<00:45,  3.23it/s]\u001b[A\n","train-2:  15%|‚ñà‚ñç        | 25/171 [00:07<00:45,  3.18it/s]\u001b[A\n","train-2:  15%|‚ñà‚ñå        | 26/171 [00:07<00:44,  3.28it/s]\u001b[A\n","train-2:  16%|‚ñà‚ñå        | 27/171 [00:08<00:41,  3.46it/s]\u001b[A\n","train-2:  16%|‚ñà‚ñã        | 28/171 [00:08<00:37,  3.80it/s]\u001b[A\n","train-2:  17%|‚ñà‚ñã        | 29/171 [00:08<00:38,  3.71it/s]\u001b[A\n","train-2:  18%|‚ñà‚ñä        | 30/171 [00:08<00:40,  3.51it/s]\u001b[A\n","train-2:  18%|‚ñà‚ñä        | 31/171 [00:09<00:41,  3.37it/s]\u001b[A\n","train-2:  19%|‚ñà‚ñä        | 32/171 [00:09<00:39,  3.49it/s]\u001b[A\n","train-2:  19%|‚ñà‚ñâ        | 33/171 [00:09<00:40,  3.40it/s]\u001b[A\n","train-2:  20%|‚ñà‚ñâ        | 34/171 [00:10<00:38,  3.52it/s]\u001b[A\n","train-2:  20%|‚ñà‚ñà        | 35/171 [00:10<00:35,  3.86it/s]\u001b[A\n","train-2:  21%|‚ñà‚ñà        | 36/171 [00:10<00:39,  3.41it/s]\u001b[A\n","train-2:  22%|‚ñà‚ñà‚ñè       | 37/171 [00:11<00:42,  3.14it/s]\u001b[A\n","train-2:  22%|‚ñà‚ñà‚ñè       | 38/171 [00:11<00:43,  3.04it/s]\u001b[A\n","train-2:  23%|‚ñà‚ñà‚ñé       | 39/171 [00:11<00:40,  3.30it/s]\u001b[A\n","train-2:  23%|‚ñà‚ñà‚ñé       | 40/171 [00:11<00:38,  3.41it/s]\u001b[A\n","train-2:  24%|‚ñà‚ñà‚ñç       | 41/171 [00:12<00:40,  3.25it/s]\u001b[A\n","train-2:  25%|‚ñà‚ñà‚ñç       | 42/171 [00:12<00:37,  3.42it/s]\u001b[A\n","train-2:  25%|‚ñà‚ñà‚ñå       | 43/171 [00:12<00:39,  3.21it/s]\u001b[A\n","train-2:  26%|‚ñà‚ñà‚ñå       | 44/171 [00:13<00:42,  3.00it/s]\u001b[A\n","train-2:  26%|‚ñà‚ñà‚ñã       | 45/171 [00:13<00:38,  3.23it/s]\u001b[A\n","train-2:  27%|‚ñà‚ñà‚ñã       | 46/171 [00:13<00:39,  3.18it/s]\u001b[A\n","train-2:  27%|‚ñà‚ñà‚ñã       | 47/171 [00:14<00:37,  3.27it/s]\u001b[A\n","train-2:  28%|‚ñà‚ñà‚ñä       | 48/171 [00:14<00:36,  3.33it/s]\u001b[A\n","train-2:  29%|‚ñà‚ñà‚ñä       | 49/171 [00:14<00:37,  3.28it/s]\u001b[A\n","train-2:  29%|‚ñà‚ñà‚ñâ       | 50/171 [00:15<00:38,  3.17it/s]\u001b[A\n","train-2:  30%|‚ñà‚ñà‚ñâ       | 51/171 [00:15<00:35,  3.37it/s]\u001b[A\n","train-2:  30%|‚ñà‚ñà‚ñà       | 52/171 [00:15<00:34,  3.42it/s]\u001b[A\n","train-2:  31%|‚ñà‚ñà‚ñà       | 53/171 [00:15<00:33,  3.56it/s]\u001b[A\n","train-2:  32%|‚ñà‚ñà‚ñà‚ñè      | 54/171 [00:16<00:29,  3.97it/s]\u001b[A\n","train-2:  32%|‚ñà‚ñà‚ñà‚ñè      | 55/171 [00:16<00:29,  3.96it/s]\u001b[A\n","train-2:  33%|‚ñà‚ñà‚ñà‚ñé      | 56/171 [00:16<00:29,  3.87it/s]\u001b[A\n","train-2:  33%|‚ñà‚ñà‚ñà‚ñé      | 57/171 [00:16<00:29,  3.83it/s]\u001b[A\n","train-2:  34%|‚ñà‚ñà‚ñà‚ñç      | 58/171 [00:17<00:29,  3.83it/s]\u001b[A\n","train-2:  35%|‚ñà‚ñà‚ñà‚ñç      | 59/171 [00:17<00:31,  3.51it/s]\u001b[A\n","train-2:  35%|‚ñà‚ñà‚ñà‚ñå      | 60/171 [00:17<00:30,  3.65it/s]\u001b[A\n","train-2:  36%|‚ñà‚ñà‚ñà‚ñå      | 61/171 [00:17<00:29,  3.69it/s]\u001b[A\n","train-2:  36%|‚ñà‚ñà‚ñà‚ñã      | 62/171 [00:18<00:30,  3.58it/s]\u001b[A\n","train-2:  37%|‚ñà‚ñà‚ñà‚ñã      | 63/171 [00:18<00:29,  3.63it/s]\u001b[A\n","train-2:  37%|‚ñà‚ñà‚ñà‚ñã      | 64/171 [00:18<00:30,  3.49it/s]\u001b[A\n","train-2:  38%|‚ñà‚ñà‚ñà‚ñä      | 65/171 [00:19<00:30,  3.43it/s]\u001b[A\n","train-2:  39%|‚ñà‚ñà‚ñà‚ñä      | 66/171 [00:19<00:31,  3.36it/s]\u001b[A\n","train-2:  39%|‚ñà‚ñà‚ñà‚ñâ      | 67/171 [00:19<00:31,  3.35it/s]\u001b[A\n","train-2:  40%|‚ñà‚ñà‚ñà‚ñâ      | 68/171 [00:20<00:31,  3.29it/s]\u001b[A\n","train-2:  40%|‚ñà‚ñà‚ñà‚ñà      | 69/171 [00:20<00:28,  3.52it/s]\u001b[A\n","train-2:  41%|‚ñà‚ñà‚ñà‚ñà      | 70/171 [00:20<00:28,  3.55it/s]\u001b[A\n","train-2:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 71/171 [00:20<00:28,  3.48it/s]\u001b[A\n","train-2:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 72/171 [00:21<00:28,  3.50it/s]\u001b[A\n","train-2:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 73/171 [00:21<00:27,  3.61it/s]\u001b[A\n","train-2:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 74/171 [00:21<00:28,  3.40it/s]\u001b[A\n","train-2:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 75/171 [00:22<00:28,  3.33it/s]\u001b[A\n","train-2:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 76/171 [00:22<00:28,  3.35it/s]\u001b[A\n","train-2:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 77/171 [00:22<00:27,  3.40it/s]\u001b[A\n","train-2:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 78/171 [00:22<00:27,  3.40it/s]\u001b[A\n","train-2:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 79/171 [00:23<00:28,  3.18it/s]\u001b[A\n","train-2:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 80/171 [00:23<00:28,  3.21it/s]\u001b[A\n","train-2:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 81/171 [00:23<00:25,  3.57it/s]\u001b[A\n","train-2:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 82/171 [00:24<00:25,  3.45it/s]\u001b[A\n","train-2:  49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 83/171 [00:24<00:27,  3.15it/s]\u001b[A\n","train-2:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 84/171 [00:24<00:26,  3.23it/s]\u001b[A\n","train-2:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 85/171 [00:25<00:26,  3.23it/s]\u001b[A\n","train-2:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 86/171 [00:25<00:25,  3.37it/s]\u001b[A\n","train-2:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 87/171 [00:25<00:25,  3.33it/s]\u001b[A\n","train-2:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 88/171 [00:26<00:26,  3.16it/s]\u001b[A\n","train-2:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 89/171 [00:26<00:25,  3.19it/s]\u001b[A\n","train-2:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 90/171 [00:26<00:25,  3.23it/s]\u001b[A\n","train-2:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 91/171 [00:26<00:24,  3.31it/s]\u001b[A\n","train-2:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 92/171 [00:27<00:23,  3.34it/s]\u001b[A\n","train-2:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 93/171 [00:27<00:23,  3.29it/s]\u001b[A\n","train-2:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 94/171 [00:27<00:23,  3.22it/s]\u001b[A\n","train-2:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 95/171 [00:28<00:22,  3.41it/s]\u001b[A\n","train-2:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 96/171 [00:28<00:21,  3.43it/s]\u001b[A\n","train-2:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 97/171 [00:28<00:21,  3.37it/s]\u001b[A\n","train-2:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 98/171 [00:29<00:23,  3.15it/s]\u001b[A\n","train-2:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 99/171 [00:29<00:21,  3.34it/s]\u001b[A\n","train-2:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 100/171 [00:29<00:20,  3.44it/s]\u001b[A\n","train-2:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 101/171 [00:29<00:20,  3.41it/s]\u001b[A\n","train-2:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 102/171 [00:30<00:22,  3.08it/s]\u001b[A\n","train-2:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 103/171 [00:30<00:22,  3.08it/s]\u001b[A\n","train-2:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 104/171 [00:31<00:22,  2.97it/s]\u001b[A\n","train-2:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 105/171 [00:31<00:20,  3.16it/s]\u001b[A\n","train-2:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 106/171 [00:31<00:20,  3.12it/s]\u001b[A\n","train-2:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 107/171 [00:31<00:20,  3.16it/s]\u001b[A\n","train-2:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 108/171 [00:32<00:19,  3.21it/s]\u001b[A\n","train-2:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 109/171 [00:32<00:19,  3.20it/s]\u001b[A\n","train-2:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 110/171 [00:32<00:17,  3.52it/s]\u001b[A\n","train-2:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 111/171 [00:33<00:18,  3.26it/s]\u001b[A\n","train-2:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 112/171 [00:33<00:18,  3.21it/s]\u001b[A\n","train-2:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 113/171 [00:33<00:18,  3.20it/s]\u001b[A\n","train-2:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 114/171 [00:34<00:18,  3.10it/s]\u001b[A\n","train-2:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 115/171 [00:34<00:19,  2.93it/s]\u001b[A\n","train-2:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 116/171 [00:34<00:17,  3.15it/s]\u001b[A\n","train-2:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 117/171 [00:35<00:17,  3.09it/s]\u001b[A\n","train-2:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 118/171 [00:35<00:16,  3.18it/s]\u001b[A\n","train-2:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 119/171 [00:35<00:15,  3.38it/s]\u001b[A\n","train-2:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 120/171 [00:35<00:13,  3.73it/s]\u001b[A\n","train-2:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 121/171 [00:36<00:12,  3.93it/s]\u001b[A\n","train-2:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 122/171 [00:36<00:12,  3.88it/s]\u001b[A\n","train-2:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 123/171 [00:36<00:13,  3.56it/s]\u001b[A\n","train-2:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 124/171 [00:36<00:13,  3.61it/s]\u001b[A\n","train-2:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 125/171 [00:37<00:12,  3.59it/s]\u001b[A\n","train-2:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 126/171 [00:37<00:12,  3.54it/s]\u001b[A\n","train-2:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 127/171 [00:37<00:14,  3.07it/s]\u001b[A\n","train-2:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 128/171 [00:38<00:13,  3.19it/s]\u001b[A\n","train-2:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 129/171 [00:38<00:13,  3.23it/s]\u001b[A\n","train-2:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 130/171 [00:38<00:11,  3.63it/s]\u001b[A\n","train-2:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 131/171 [00:38<00:10,  3.66it/s]\u001b[A\n","train-2:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 132/171 [00:39<00:11,  3.44it/s]\u001b[A\n","train-2:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 133/171 [00:39<00:11,  3.40it/s]\u001b[A\n","train-2:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 134/171 [00:39<00:10,  3.52it/s]\u001b[A\n","train-2:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 135/171 [00:40<00:10,  3.59it/s]\u001b[A\n","train-2:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 136/171 [00:40<00:10,  3.50it/s]\u001b[A\n","train-2:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 137/171 [00:40<00:09,  3.46it/s]\u001b[A\n","train-2:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 138/171 [00:40<00:08,  3.84it/s]\u001b[A\n","train-2:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 139/171 [00:41<00:09,  3.43it/s]\u001b[A\n","train-2:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 140/171 [00:41<00:08,  3.51it/s]\u001b[A\n","train-2:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 141/171 [00:41<00:07,  3.82it/s]\u001b[A\n","train-2:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 142/171 [00:42<00:07,  3.86it/s]\u001b[A\n","train-2:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 143/171 [00:42<00:07,  3.60it/s]\u001b[A\n","train-2:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 144/171 [00:42<00:07,  3.72it/s]\u001b[A\n","train-2:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 145/171 [00:42<00:07,  3.57it/s]\u001b[A\n","train-2:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 146/171 [00:43<00:06,  3.83it/s]\u001b[A\n","train-2:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 147/171 [00:43<00:06,  3.49it/s]\u001b[A\n","train-2:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 148/171 [00:43<00:06,  3.51it/s]\u001b[A\n","train-2:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 149/171 [00:44<00:06,  3.31it/s]\u001b[A\n","train-2:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 150/171 [00:44<00:06,  3.47it/s]\u001b[A\n","train-2:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 151/171 [00:44<00:05,  3.44it/s]\u001b[A\n","train-2:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 152/171 [00:44<00:05,  3.46it/s]\u001b[A\n","train-2:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 153/171 [00:45<00:04,  3.62it/s]\u001b[A\n","train-2:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 154/171 [00:45<00:04,  3.52it/s]\u001b[A\n","train-2:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 155/171 [00:45<00:04,  3.54it/s]\u001b[A\n","train-2:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 156/171 [00:46<00:04,  3.39it/s]\u001b[A\n","train-2:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 157/171 [00:46<00:04,  3.26it/s]\u001b[A\n","train-2:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 158/171 [00:46<00:03,  3.25it/s]\u001b[A\n","train-2:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 159/171 [00:47<00:03,  3.29it/s]\u001b[A\n","train-2:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 160/171 [00:47<00:03,  3.35it/s]\u001b[A\n","train-2:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 161/171 [00:47<00:03,  3.21it/s]\u001b[A\n","train-2:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 162/171 [00:47<00:02,  3.36it/s]\u001b[A\n","train-2:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 163/171 [00:48<00:02,  3.65it/s]\u001b[A\n","train-2:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 164/171 [00:48<00:01,  3.54it/s]\u001b[A\n","train-2:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 165/171 [00:48<00:01,  3.68it/s]\u001b[A\n","train-2:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 166/171 [00:48<00:01,  3.64it/s]\u001b[A\n","train-2:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 167/171 [00:49<00:01,  3.40it/s]\u001b[A\n","train-2:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 168/171 [00:49<00:00,  3.43it/s]\u001b[A\n","train-2:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 169/171 [00:49<00:00,  3.90it/s]\u001b[A\n","train-2:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 170/171 [00:50<00:00,  3.84it/s]\u001b[A\n","train-2: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 171/171 [00:50<00:00,  3.40it/s]\n","\n","eval:   0%|          | 0/171 [00:00<?, ?it/s]\u001b[A\n","eval:   1%|          | 1/171 [00:00<00:18,  9.00it/s]\u001b[A\n","eval:   1%|          | 2/171 [00:00<00:17,  9.42it/s]\u001b[A\n","eval:   2%|‚ñè         | 3/171 [00:00<00:19,  8.77it/s]\u001b[A\n","eval:   3%|‚ñé         | 5/171 [00:00<00:17,  9.73it/s]\u001b[A\n","eval:   4%|‚ñé         | 6/171 [00:00<00:17,  9.43it/s]\u001b[A\n","eval:   4%|‚ñç         | 7/171 [00:00<00:18,  9.07it/s]\u001b[A\n","eval:   5%|‚ñå         | 9/171 [00:00<00:16,  9.98it/s]\u001b[A\n","eval:   6%|‚ñã         | 11/171 [00:01<00:15, 10.29it/s]\u001b[A\n","eval:   8%|‚ñä         | 13/171 [00:01<00:14, 10.73it/s]\u001b[A\n","eval:   9%|‚ñâ         | 15/171 [00:01<00:13, 11.16it/s]\u001b[A\n","eval:  10%|‚ñâ         | 17/171 [00:01<00:13, 11.07it/s]\u001b[A\n","eval:  11%|‚ñà         | 19/171 [00:01<00:13, 11.03it/s]\u001b[A\n","eval:  12%|‚ñà‚ñè        | 21/171 [00:02<00:14, 10.56it/s]\u001b[A\n","eval:  13%|‚ñà‚ñé        | 23/171 [00:02<00:13, 11.37it/s]\u001b[A\n","eval:  15%|‚ñà‚ñç        | 25/171 [00:02<00:13, 10.48it/s]\u001b[A\n","eval:  16%|‚ñà‚ñå        | 27/171 [00:02<00:13, 10.41it/s]\u001b[A\n","eval:  17%|‚ñà‚ñã        | 29/171 [00:02<00:13, 10.21it/s]\u001b[A\n","eval:  18%|‚ñà‚ñä        | 31/171 [00:03<00:14,  9.97it/s]\u001b[A\n","eval:  19%|‚ñà‚ñâ        | 33/171 [00:03<00:13,  9.94it/s]\u001b[A\n","eval:  20%|‚ñà‚ñà        | 35/171 [00:03<00:13, 10.14it/s]\u001b[A\n","eval:  22%|‚ñà‚ñà‚ñè       | 37/171 [00:03<00:12, 10.61it/s]\u001b[A\n","eval:  23%|‚ñà‚ñà‚ñé       | 39/171 [00:03<00:13,  9.74it/s]\u001b[A\n","eval:  24%|‚ñà‚ñà‚ñç       | 41/171 [00:03<00:12, 10.30it/s]\u001b[A\n","eval:  25%|‚ñà‚ñà‚ñå       | 43/171 [00:04<00:12, 10.44it/s]\u001b[A\n","eval:  26%|‚ñà‚ñà‚ñã       | 45/171 [00:04<00:12, 10.03it/s]\u001b[A\n","eval:  27%|‚ñà‚ñà‚ñã       | 47/171 [00:04<00:13,  9.19it/s]\u001b[A\n","eval:  28%|‚ñà‚ñà‚ñä       | 48/171 [00:04<00:13,  9.18it/s]\u001b[A\n","eval:  29%|‚ñà‚ñà‚ñä       | 49/171 [00:04<00:13,  9.09it/s]\u001b[A\n","eval:  29%|‚ñà‚ñà‚ñâ       | 50/171 [00:04<00:13,  9.07it/s]\u001b[A\n","eval:  30%|‚ñà‚ñà‚ñâ       | 51/171 [00:05<00:13,  9.21it/s]\u001b[A\n","eval:  30%|‚ñà‚ñà‚ñà       | 52/171 [00:05<00:13,  8.97it/s]\u001b[A\n","eval:  32%|‚ñà‚ñà‚ñà‚ñè      | 54/171 [00:05<00:12,  9.39it/s]\u001b[A\n","eval:  32%|‚ñà‚ñà‚ñà‚ñè      | 55/171 [00:05<00:12,  9.45it/s]\u001b[A\n","eval:  33%|‚ñà‚ñà‚ñà‚ñé      | 56/171 [00:05<00:12,  9.40it/s]\u001b[A\n","eval:  33%|‚ñà‚ñà‚ñà‚ñé      | 57/171 [00:05<00:12,  8.93it/s]\u001b[A\n","eval:  35%|‚ñà‚ñà‚ñà‚ñç      | 59/171 [00:05<00:11,  9.38it/s]\u001b[A\n","eval:  35%|‚ñà‚ñà‚ñà‚ñå      | 60/171 [00:06<00:11,  9.39it/s]\u001b[A\n","eval:  36%|‚ñà‚ñà‚ñà‚ñå      | 61/171 [00:06<00:11,  9.42it/s]\u001b[A\n","eval:  37%|‚ñà‚ñà‚ñà‚ñã      | 63/171 [00:06<00:11,  9.49it/s]\u001b[A\n","eval:  37%|‚ñà‚ñà‚ñà‚ñã      | 64/171 [00:06<00:11,  9.45it/s]\u001b[A\n","eval:  38%|‚ñà‚ñà‚ñà‚ñä      | 65/171 [00:06<00:11,  8.91it/s]\u001b[A\n","eval:  39%|‚ñà‚ñà‚ñà‚ñä      | 66/171 [00:06<00:11,  9.03it/s]\u001b[A\n","eval:  39%|‚ñà‚ñà‚ñà‚ñâ      | 67/171 [00:06<00:11,  9.18it/s]\u001b[A\n","eval:  40%|‚ñà‚ñà‚ñà‚ñâ      | 68/171 [00:06<00:11,  8.99it/s]\u001b[A\n","eval:  41%|‚ñà‚ñà‚ñà‚ñà      | 70/171 [00:07<00:11,  9.12it/s]\u001b[A\n","eval:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 71/171 [00:07<00:10,  9.30it/s]\u001b[A\n","eval:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 72/171 [00:07<00:10,  9.17it/s]\u001b[A\n","eval:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 73/171 [00:07<00:10,  9.22it/s]\u001b[A\n","eval:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 74/171 [00:07<00:10,  8.93it/s]\u001b[A\n","eval:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 76/171 [00:07<00:10,  9.20it/s]\u001b[A\n","eval:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 77/171 [00:07<00:10,  9.12it/s]\u001b[A\n","eval:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 78/171 [00:08<00:10,  9.02it/s]\u001b[A\n","eval:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 79/171 [00:08<00:10,  9.03it/s]\u001b[A\n","eval:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 81/171 [00:08<00:09,  9.83it/s]\u001b[A\n","eval:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 82/171 [00:08<00:09,  9.82it/s]\u001b[A\n","eval:  49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 83/171 [00:08<00:09,  9.66it/s]\u001b[A\n","eval:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 84/171 [00:08<00:09,  9.38it/s]\u001b[A\n","eval:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 85/171 [00:08<00:09,  9.48it/s]\u001b[A\n","eval:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 87/171 [00:08<00:08, 10.07it/s]\u001b[A\n","eval:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 89/171 [00:09<00:07, 10.37it/s]\u001b[A\n","eval:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 91/171 [00:09<00:07, 10.25it/s]\u001b[A\n","eval:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 93/171 [00:09<00:07, 10.69it/s]\u001b[A\n","eval:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 95/171 [00:09<00:07, 10.13it/s]\u001b[A\n","eval:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 97/171 [00:09<00:07,  9.90it/s]\u001b[A\n","eval:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 98/171 [00:10<00:07,  9.55it/s]\u001b[A\n","eval:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 100/171 [00:10<00:07,  9.65it/s]\u001b[A\n","eval:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 102/171 [00:10<00:06, 10.04it/s]\u001b[A\n","eval:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 103/171 [00:10<00:06,  9.80it/s]\u001b[A\n","eval:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 104/171 [00:10<00:06,  9.77it/s]\u001b[A\n","eval:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 105/171 [00:10<00:06,  9.64it/s]\u001b[A\n","eval:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 107/171 [00:10<00:06,  9.53it/s]\u001b[A\n","eval:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 108/171 [00:11<00:06,  9.18it/s]\u001b[A\n","eval:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 110/171 [00:11<00:06,  8.96it/s]\u001b[A\n","eval:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 112/171 [00:11<00:05,  9.87it/s]\u001b[A\n","eval:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 114/171 [00:11<00:05,  9.75it/s]\u001b[A\n","eval:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 116/171 [00:11<00:05, 10.62it/s]\u001b[A\n","eval:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 118/171 [00:12<00:05, 10.36it/s]\u001b[A\n","eval:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 120/171 [00:12<00:04, 10.83it/s]\u001b[A\n","eval:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 122/171 [00:12<00:04, 11.34it/s]\u001b[A\n","eval:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 124/171 [00:12<00:04, 10.70it/s]\u001b[A\n","eval:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 126/171 [00:12<00:04,  9.63it/s]\u001b[A\n","eval:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 127/171 [00:12<00:04,  9.50it/s]\u001b[A\n","eval:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 129/171 [00:13<00:04, 10.36it/s]\u001b[A\n","eval:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 131/171 [00:13<00:03, 10.36it/s]\u001b[A\n","eval:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 133/171 [00:13<00:03, 10.65it/s]\u001b[A\n","eval:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 135/171 [00:13<00:03, 10.63it/s]\u001b[A\n","eval:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 137/171 [00:13<00:03, 10.32it/s]\u001b[A\n","eval:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 139/171 [00:14<00:02, 10.78it/s]\u001b[A\n","eval:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 141/171 [00:14<00:02, 10.15it/s]\u001b[A\n","eval:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 143/171 [00:14<00:02, 10.28it/s]\u001b[A\n","eval:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 145/171 [00:14<00:02,  9.82it/s]\u001b[A\n","eval:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 146/171 [00:14<00:02,  9.71it/s]\u001b[A\n","eval:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 147/171 [00:14<00:02,  9.70it/s]\u001b[A\n","eval:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 149/171 [00:15<00:02,  9.68it/s]\u001b[A\n","eval:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 151/171 [00:15<00:01, 10.24it/s]\u001b[A\n","eval:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 153/171 [00:15<00:01,  9.27it/s]\u001b[A\n","eval:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 154/171 [00:15<00:01,  8.77it/s]\u001b[A\n","eval:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 155/171 [00:15<00:01,  8.94it/s]\u001b[A\n","eval:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 156/171 [00:15<00:01,  8.84it/s]\u001b[A\n","eval:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 158/171 [00:16<00:01,  9.45it/s]\u001b[A\n","eval:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 159/171 [00:16<00:01,  9.37it/s]\u001b[A\n","eval:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 160/171 [00:16<00:01,  9.33it/s]\u001b[A\n","eval:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 162/171 [00:16<00:00, 10.07it/s]\u001b[A\n","eval:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 163/171 [00:16<00:00,  9.17it/s]\u001b[A\n","eval:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 164/171 [00:16<00:00,  9.12it/s]\u001b[A\n","eval:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 165/171 [00:16<00:00,  8.64it/s]\u001b[A\n","eval:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 166/171 [00:16<00:00,  8.79it/s]\u001b[A\n","eval:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 167/171 [00:17<00:00,  8.48it/s]\u001b[A\n","eval:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 168/171 [00:17<00:00,  8.58it/s]\u001b[A\n","eval:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 169/171 [00:17<00:00,  8.38it/s]\u001b[A\n","eval: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 171/171 [00:17<00:00,  9.76it/s]\n","\n","eval:   0%|          | 0/25 [00:00<?, ?it/s]\u001b[A\n","eval:   4%|‚ñç         | 1/25 [00:00<00:02,  8.75it/s]\u001b[A\n","eval:   8%|‚ñä         | 2/25 [00:00<00:02,  8.40it/s]\u001b[A\n","eval:  12%|‚ñà‚ñè        | 3/25 [00:00<00:02,  9.02it/s]\u001b[A\n","eval:  20%|‚ñà‚ñà        | 5/25 [00:00<00:02,  9.85it/s]\u001b[A\n","eval:  24%|‚ñà‚ñà‚ñç       | 6/25 [00:00<00:02,  9.01it/s]\u001b[A\n","eval:  28%|‚ñà‚ñà‚ñä       | 7/25 [00:00<00:02,  8.97it/s]\u001b[A\n","eval:  32%|‚ñà‚ñà‚ñà‚ñè      | 8/25 [00:00<00:01,  8.82it/s]\u001b[A\n","eval:  36%|‚ñà‚ñà‚ñà‚ñå      | 9/25 [00:01<00:01,  8.76it/s]\u001b[A\n","eval:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 11/25 [00:01<00:01, 10.05it/s]\u001b[A\n","eval:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 13/25 [00:01<00:01, 10.86it/s]\u001b[A\n","eval:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 15/25 [00:01<00:01,  9.90it/s]\u001b[A\n","eval:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 16/25 [00:01<00:00,  9.60it/s]\u001b[A\n","eval:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 17/25 [00:01<00:00,  9.55it/s]\u001b[A\n","eval:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 19/25 [00:01<00:00, 10.41it/s]\u001b[A\n","eval:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 21/25 [00:02<00:00,  9.68it/s]\u001b[A\n","eval:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 22/25 [00:02<00:00,  8.91it/s]\u001b[A\n","eval:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 23/25 [00:02<00:00,  9.03it/s]\u001b[A\n","eval: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25/25 [00:02<00:00,  9.55it/s]\n"," 60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 3/5 [03:43<02:27, 73.59s/it]"]},{"name":"stdout","output_type":"stream","text":["save the model to finetune-5-2e-05.pt\n","epoch 2: train loss :: 0.747, train acc :: 0.543, dev acc :: 0.522\n"]},{"name":"stderr","output_type":"stream","text":["\n","train-3:   0%|          | 0/171 [00:00<?, ?it/s]\u001b[A\n","train-3:   1%|          | 1/171 [00:00<01:04,  2.63it/s]\u001b[A\n","train-3:   1%|          | 2/171 [00:00<01:01,  2.73it/s]\u001b[A\n","train-3:   2%|‚ñè         | 3/171 [00:00<00:49,  3.40it/s]\u001b[A\n","train-3:   2%|‚ñè         | 4/171 [00:01<00:49,  3.35it/s]\u001b[A\n","train-3:   3%|‚ñé         | 5/171 [00:01<00:49,  3.37it/s]\u001b[A\n","train-3:   4%|‚ñé         | 6/171 [00:01<00:51,  3.22it/s]\u001b[A\n","train-3:   4%|‚ñç         | 7/171 [00:02<00:50,  3.22it/s]\u001b[A\n","train-3:   5%|‚ñç         | 8/171 [00:02<00:50,  3.22it/s]\u001b[A\n","train-3:   5%|‚ñå         | 9/171 [00:02<00:46,  3.45it/s]\u001b[A\n","train-3:   6%|‚ñå         | 10/171 [00:03<00:45,  3.55it/s]\u001b[A\n","train-3:   6%|‚ñã         | 11/171 [00:03<00:46,  3.44it/s]\u001b[A\n","train-3:   7%|‚ñã         | 12/171 [00:03<00:45,  3.47it/s]\u001b[A\n","train-3:   8%|‚ñä         | 13/171 [00:03<00:46,  3.37it/s]\u001b[A\n","train-3:   8%|‚ñä         | 14/171 [00:04<00:44,  3.50it/s]\u001b[A\n","train-3:   9%|‚ñâ         | 15/171 [00:04<00:43,  3.62it/s]\u001b[A\n","train-3:   9%|‚ñâ         | 16/171 [00:04<00:41,  3.76it/s]\u001b[A\n","train-3:  10%|‚ñâ         | 17/171 [00:04<00:38,  3.97it/s]\u001b[A\n","train-3:  11%|‚ñà         | 18/171 [00:05<00:39,  3.90it/s]\u001b[A\n","train-3:  11%|‚ñà         | 19/171 [00:05<00:39,  3.85it/s]\u001b[A\n","train-3:  12%|‚ñà‚ñè        | 20/171 [00:05<00:43,  3.50it/s]\u001b[A\n","train-3:  12%|‚ñà‚ñè        | 21/171 [00:06<00:44,  3.40it/s]\u001b[A\n","train-3:  13%|‚ñà‚ñé        | 22/171 [00:06<00:44,  3.32it/s]\u001b[A\n","train-3:  13%|‚ñà‚ñé        | 23/171 [00:06<00:45,  3.24it/s]\u001b[A\n","train-3:  14%|‚ñà‚ñç        | 24/171 [00:07<00:45,  3.23it/s]\u001b[A\n","train-3:  15%|‚ñà‚ñç        | 25/171 [00:07<00:42,  3.43it/s]\u001b[A\n","train-3:  15%|‚ñà‚ñå        | 26/171 [00:07<00:44,  3.28it/s]\u001b[A\n","train-3:  16%|‚ñà‚ñå        | 27/171 [00:07<00:43,  3.35it/s]\u001b[A\n","train-3:  16%|‚ñà‚ñã        | 28/171 [00:08<00:43,  3.29it/s]\u001b[A\n","train-3:  17%|‚ñà‚ñã        | 29/171 [00:08<00:41,  3.46it/s]\u001b[A\n","train-3:  18%|‚ñà‚ñä        | 30/171 [00:08<00:42,  3.29it/s]\u001b[A\n","train-3:  18%|‚ñà‚ñä        | 31/171 [00:09<00:43,  3.18it/s]\u001b[A\n","train-3:  19%|‚ñà‚ñä        | 32/171 [00:09<00:45,  3.09it/s]\u001b[A\n","train-3:  19%|‚ñà‚ñâ        | 33/171 [00:09<00:43,  3.19it/s]\u001b[A\n","train-3:  20%|‚ñà‚ñâ        | 34/171 [00:10<00:41,  3.32it/s]\u001b[A\n","train-3:  20%|‚ñà‚ñà        | 35/171 [00:10<00:44,  3.04it/s]\u001b[A\n","train-3:  21%|‚ñà‚ñà        | 36/171 [00:10<00:40,  3.35it/s]\u001b[A\n","train-3:  22%|‚ñà‚ñà‚ñè       | 37/171 [00:10<00:36,  3.65it/s]\u001b[A\n","train-3:  22%|‚ñà‚ñà‚ñè       | 38/171 [00:11<00:37,  3.51it/s]\u001b[A\n","train-3:  23%|‚ñà‚ñà‚ñé       | 39/171 [00:11<00:39,  3.33it/s]\u001b[A\n","train-3:  23%|‚ñà‚ñà‚ñé       | 40/171 [00:11<00:37,  3.53it/s]\u001b[A\n","train-3:  24%|‚ñà‚ñà‚ñç       | 41/171 [00:12<00:38,  3.36it/s]\u001b[A\n","train-3:  25%|‚ñà‚ñà‚ñç       | 42/171 [00:12<00:39,  3.29it/s]\u001b[A\n","train-3:  25%|‚ñà‚ñà‚ñå       | 43/171 [00:12<00:35,  3.64it/s]\u001b[A\n","train-3:  26%|‚ñà‚ñà‚ñå       | 44/171 [00:12<00:35,  3.54it/s]\u001b[A\n","train-3:  26%|‚ñà‚ñà‚ñã       | 45/171 [00:13<00:35,  3.56it/s]\u001b[A\n","train-3:  27%|‚ñà‚ñà‚ñã       | 46/171 [00:13<00:37,  3.33it/s]\u001b[A\n","train-3:  27%|‚ñà‚ñà‚ñã       | 47/171 [00:13<00:32,  3.81it/s]\u001b[A\n","train-3:  28%|‚ñà‚ñà‚ñä       | 48/171 [00:14<00:32,  3.80it/s]\u001b[A\n","train-3:  29%|‚ñà‚ñà‚ñä       | 49/171 [00:14<00:33,  3.66it/s]\u001b[A\n","train-3:  29%|‚ñà‚ñà‚ñâ       | 50/171 [00:14<00:34,  3.53it/s]\u001b[A\n","train-3:  30%|‚ñà‚ñà‚ñâ       | 51/171 [00:15<00:37,  3.19it/s]\u001b[A\n","train-3:  30%|‚ñà‚ñà‚ñà       | 52/171 [00:15<00:34,  3.49it/s]\u001b[A\n","train-3:  31%|‚ñà‚ñà‚ñà       | 53/171 [00:15<00:35,  3.37it/s]\u001b[A\n","train-3:  32%|‚ñà‚ñà‚ñà‚ñè      | 54/171 [00:15<00:33,  3.49it/s]\u001b[A\n","train-3:  32%|‚ñà‚ñà‚ñà‚ñè      | 55/171 [00:16<00:33,  3.44it/s]\u001b[A\n","train-3:  33%|‚ñà‚ñà‚ñà‚ñé      | 56/171 [00:16<00:32,  3.52it/s]\u001b[A\n","train-3:  33%|‚ñà‚ñà‚ñà‚ñé      | 57/171 [00:16<00:33,  3.37it/s]\u001b[A\n","train-3:  34%|‚ñà‚ñà‚ñà‚ñç      | 58/171 [00:16<00:31,  3.54it/s]\u001b[A\n","train-3:  35%|‚ñà‚ñà‚ñà‚ñç      | 59/171 [00:17<00:32,  3.44it/s]\u001b[A\n","train-3:  35%|‚ñà‚ñà‚ñà‚ñå      | 60/171 [00:17<00:32,  3.39it/s]\u001b[A\n","train-3:  36%|‚ñà‚ñà‚ñà‚ñå      | 61/171 [00:17<00:31,  3.52it/s]\u001b[A\n","train-3:  36%|‚ñà‚ñà‚ñà‚ñã      | 62/171 [00:18<00:29,  3.67it/s]\u001b[A\n","train-3:  37%|‚ñà‚ñà‚ñà‚ñã      | 63/171 [00:18<00:30,  3.55it/s]\u001b[A\n","train-3:  37%|‚ñà‚ñà‚ñà‚ñã      | 64/171 [00:18<00:32,  3.34it/s]\u001b[A\n","train-3:  38%|‚ñà‚ñà‚ñà‚ñä      | 65/171 [00:18<00:30,  3.52it/s]\u001b[A\n","train-3:  39%|‚ñà‚ñà‚ñà‚ñä      | 66/171 [00:19<00:28,  3.70it/s]\u001b[A\n","train-3:  39%|‚ñà‚ñà‚ñà‚ñâ      | 67/171 [00:19<00:28,  3.71it/s]\u001b[A\n","train-3:  40%|‚ñà‚ñà‚ñà‚ñâ      | 68/171 [00:19<00:27,  3.73it/s]\u001b[A\n","train-3:  40%|‚ñà‚ñà‚ñà‚ñà      | 69/171 [00:20<00:27,  3.65it/s]\u001b[A\n","train-3:  41%|‚ñà‚ñà‚ñà‚ñà      | 70/171 [00:20<00:29,  3.40it/s]\u001b[A\n","train-3:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 71/171 [00:20<00:28,  3.55it/s]\u001b[A\n","train-3:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 72/171 [00:20<00:28,  3.52it/s]\u001b[A\n","train-3:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 73/171 [00:21<00:26,  3.65it/s]\u001b[A\n","train-3:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 74/171 [00:21<00:26,  3.67it/s]\u001b[A\n","train-3:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 75/171 [00:21<00:28,  3.35it/s]\u001b[A\n","train-3:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 76/171 [00:22<00:30,  3.14it/s]\u001b[A\n","train-3:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 77/171 [00:22<00:26,  3.52it/s]\u001b[A\n","train-3:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 78/171 [00:22<00:26,  3.53it/s]\u001b[A\n","train-3:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 79/171 [00:22<00:23,  3.85it/s]\u001b[A\n","train-3:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 80/171 [00:23<00:24,  3.73it/s]\u001b[A\n","train-3:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 81/171 [00:23<00:23,  3.77it/s]\u001b[A\n","train-3:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 82/171 [00:23<00:24,  3.57it/s]\u001b[A\n","train-3:  49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 83/171 [00:24<00:25,  3.42it/s]\u001b[A\n","train-3:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 84/171 [00:24<00:24,  3.57it/s]\u001b[A\n","train-3:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 85/171 [00:24<00:24,  3.55it/s]\u001b[A\n","train-3:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 86/171 [00:24<00:25,  3.31it/s]\u001b[A\n","train-3:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 87/171 [00:25<00:26,  3.21it/s]\u001b[A\n","train-3:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 88/171 [00:25<00:27,  3.06it/s]\u001b[A\n","train-3:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 89/171 [00:25<00:23,  3.42it/s]\u001b[A\n","train-3:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 90/171 [00:26<00:23,  3.51it/s]\u001b[A\n","train-3:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 91/171 [00:26<00:23,  3.39it/s]\u001b[A\n","train-3:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 92/171 [00:26<00:23,  3.37it/s]\u001b[A\n","train-3:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 93/171 [00:27<00:23,  3.26it/s]\u001b[A\n","train-3:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 94/171 [00:27<00:22,  3.49it/s]\u001b[A\n","train-3:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 95/171 [00:27<00:22,  3.39it/s]\u001b[A\n","train-3:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 96/171 [00:27<00:22,  3.27it/s]\u001b[A\n","train-3:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 97/171 [00:28<00:23,  3.08it/s]\u001b[A\n","train-3:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 98/171 [00:28<00:22,  3.21it/s]\u001b[A\n","train-3:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 99/171 [00:28<00:20,  3.48it/s]\u001b[A\n","train-3:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 100/171 [00:29<00:20,  3.41it/s]\u001b[A\n","train-3:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 101/171 [00:29<00:19,  3.51it/s]\u001b[A\n","train-3:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 102/171 [00:29<00:18,  3.69it/s]\u001b[A\n","train-3:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 103/171 [00:30<00:20,  3.27it/s]\u001b[A\n","train-3:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 104/171 [00:30<00:20,  3.35it/s]\u001b[A\n","train-3:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 105/171 [00:30<00:20,  3.30it/s]\u001b[A\n","train-3:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 106/171 [00:31<00:22,  2.93it/s]\u001b[A\n","train-3:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 107/171 [00:31<00:21,  2.95it/s]\u001b[A\n","train-3:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 108/171 [00:31<00:20,  3.05it/s]\u001b[A\n","train-3:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 109/171 [00:31<00:19,  3.25it/s]\u001b[A\n","train-3:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 110/171 [00:32<00:18,  3.23it/s]\u001b[A\n","train-3:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 111/171 [00:32<00:16,  3.57it/s]\u001b[A\n","train-3:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 112/171 [00:32<00:16,  3.56it/s]\u001b[A\n","train-3:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 113/171 [00:32<00:16,  3.62it/s]\u001b[A\n","train-3:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 114/171 [00:33<00:17,  3.33it/s]\u001b[A\n","train-3:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 115/171 [00:33<00:15,  3.54it/s]\u001b[A\n","train-3:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 116/171 [00:33<00:15,  3.60it/s]\u001b[A\n","train-3:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 117/171 [00:34<00:14,  3.75it/s]\u001b[A\n","train-3:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 118/171 [00:34<00:14,  3.68it/s]\u001b[A\n","train-3:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 119/171 [00:34<00:13,  3.72it/s]\u001b[A\n","train-3:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 120/171 [00:34<00:14,  3.44it/s]\u001b[A\n","train-3:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 121/171 [00:35<00:14,  3.40it/s]\u001b[A\n","train-3:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 122/171 [00:35<00:14,  3.44it/s]\u001b[A\n","train-3:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 123/171 [00:35<00:13,  3.58it/s]\u001b[A\n","train-3:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 124/171 [00:36<00:13,  3.51it/s]\u001b[A\n","train-3:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 125/171 [00:36<00:12,  3.56it/s]\u001b[A\n","train-3:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 126/171 [00:36<00:13,  3.36it/s]\u001b[A\n","train-3:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 127/171 [00:37<00:12,  3.46it/s]\u001b[A\n","train-3:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 128/171 [00:37<00:12,  3.54it/s]\u001b[A\n","train-3:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 129/171 [00:37<00:11,  3.54it/s]\u001b[A\n","train-3:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 130/171 [00:37<00:12,  3.37it/s]\u001b[A\n","train-3:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 131/171 [00:38<00:11,  3.37it/s]\u001b[A\n","train-3:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 132/171 [00:38<00:11,  3.52it/s]\u001b[A\n","train-3:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 133/171 [00:38<00:10,  3.48it/s]\u001b[A\n","train-3:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 134/171 [00:39<00:11,  3.34it/s]\u001b[A\n","train-3:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 135/171 [00:39<00:10,  3.45it/s]\u001b[A\n","train-3:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 136/171 [00:39<00:10,  3.26it/s]\u001b[A\n","train-3:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 137/171 [00:39<00:10,  3.38it/s]\u001b[A\n","train-3:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 138/171 [00:40<00:09,  3.38it/s]\u001b[A\n","train-3:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 139/171 [00:40<00:10,  3.17it/s]\u001b[A\n","train-3:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 140/171 [00:40<00:09,  3.35it/s]\u001b[A\n","train-3:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 141/171 [00:41<00:08,  3.45it/s]\u001b[A\n","train-3:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 142/171 [00:41<00:09,  3.13it/s]\u001b[A\n","train-3:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 143/171 [00:41<00:09,  3.07it/s]\u001b[A\n","train-3:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 144/171 [00:42<00:08,  3.34it/s]\u001b[A\n","train-3:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 145/171 [00:42<00:08,  3.18it/s]\u001b[A\n","train-3:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 146/171 [00:42<00:07,  3.17it/s]\u001b[A\n","train-3:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 147/171 [00:43<00:07,  3.17it/s]\u001b[A\n","train-3:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 148/171 [00:43<00:06,  3.41it/s]\u001b[A\n","train-3:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 149/171 [00:43<00:07,  3.14it/s]\u001b[A\n","train-3:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 150/171 [00:44<00:07,  2.95it/s]\u001b[A\n","train-3:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 151/171 [00:44<00:06,  3.10it/s]\u001b[A\n","train-3:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 152/171 [00:44<00:06,  3.06it/s]\u001b[A\n","train-3:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 153/171 [00:45<00:06,  3.00it/s]\u001b[A\n","train-3:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 154/171 [00:45<00:05,  3.18it/s]\u001b[A\n","train-3:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 155/171 [00:45<00:04,  3.21it/s]\u001b[A\n","train-3:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 156/171 [00:45<00:04,  3.25it/s]\u001b[A\n","train-3:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 157/171 [00:46<00:04,  3.02it/s]\u001b[A\n","train-3:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 158/171 [00:46<00:04,  3.24it/s]\u001b[A\n","train-3:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 159/171 [00:46<00:03,  3.32it/s]\u001b[A\n","train-3:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 160/171 [00:47<00:03,  3.41it/s]\u001b[A\n","train-3:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 161/171 [00:47<00:02,  3.35it/s]\u001b[A\n","train-3:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 162/171 [00:47<00:02,  3.25it/s]\u001b[A\n","train-3:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 163/171 [00:48<00:02,  3.03it/s]\u001b[A\n","train-3:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 164/171 [00:48<00:02,  3.46it/s]\u001b[A\n","train-3:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 165/171 [00:48<00:01,  3.38it/s]\u001b[A\n","train-3:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 166/171 [00:48<00:01,  3.42it/s]\u001b[A\n","train-3:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 167/171 [00:49<00:01,  3.49it/s]\u001b[A\n","train-3:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 168/171 [00:49<00:00,  3.40it/s]\u001b[A\n","train-3:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 169/171 [00:49<00:00,  3.39it/s]\u001b[A\n","train-3:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 170/171 [00:50<00:00,  3.50it/s]\u001b[A\n","train-3: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 171/171 [00:50<00:00,  3.39it/s]\n","\n","eval:   0%|          | 0/171 [00:00<?, ?it/s]\u001b[A\n","eval:   1%|          | 2/171 [00:00<00:15, 10.57it/s]\u001b[A\n","eval:   2%|‚ñè         | 4/171 [00:00<00:17,  9.71it/s]\u001b[A\n","eval:   3%|‚ñé         | 5/171 [00:00<00:18,  8.76it/s]\u001b[A\n","eval:   4%|‚ñç         | 7/171 [00:00<00:16,  9.92it/s]\u001b[A\n","eval:   5%|‚ñç         | 8/171 [00:00<00:17,  9.31it/s]\u001b[A\n","eval:   6%|‚ñå         | 10/171 [00:01<00:16,  9.57it/s]\u001b[A\n","eval:   6%|‚ñã         | 11/171 [00:01<00:17,  9.06it/s]\u001b[A\n","eval:   8%|‚ñä         | 13/171 [00:01<00:16,  9.67it/s]\u001b[A\n","eval:   8%|‚ñä         | 14/171 [00:01<00:17,  9.00it/s]\u001b[A\n","eval:   9%|‚ñâ         | 15/171 [00:01<00:17,  9.15it/s]\u001b[A\n","eval:   9%|‚ñâ         | 16/171 [00:01<00:17,  9.11it/s]\u001b[A\n","eval:  10%|‚ñâ         | 17/171 [00:01<00:16,  9.16it/s]\u001b[A\n","eval:  11%|‚ñà         | 18/171 [00:01<00:17,  8.85it/s]\u001b[A\n","eval:  11%|‚ñà         | 19/171 [00:02<00:18,  8.24it/s]\u001b[A\n","eval:  12%|‚ñà‚ñè        | 20/171 [00:02<00:18,  8.21it/s]\u001b[A\n","eval:  12%|‚ñà‚ñè        | 21/171 [00:02<00:17,  8.36it/s]\u001b[A\n","eval:  13%|‚ñà‚ñé        | 22/171 [00:02<00:17,  8.71it/s]\u001b[A\n","eval:  13%|‚ñà‚ñé        | 23/171 [00:02<00:17,  8.50it/s]\u001b[A\n","eval:  14%|‚ñà‚ñç        | 24/171 [00:02<00:16,  8.78it/s]\u001b[A\n","eval:  15%|‚ñà‚ñå        | 26/171 [00:02<00:13, 10.52it/s]\u001b[A\n","eval:  16%|‚ñà‚ñã        | 28/171 [00:03<00:14, 10.02it/s]\u001b[A\n","eval:  18%|‚ñà‚ñä        | 30/171 [00:03<00:15,  9.29it/s]\u001b[A\n","eval:  19%|‚ñà‚ñä        | 32/171 [00:03<00:14,  9.82it/s]\u001b[A\n","eval:  19%|‚ñà‚ñâ        | 33/171 [00:03<00:14,  9.75it/s]\u001b[A\n","eval:  20%|‚ñà‚ñâ        | 34/171 [00:03<00:14,  9.61it/s]\u001b[A\n","eval:  20%|‚ñà‚ñà        | 35/171 [00:03<00:14,  9.56it/s]\u001b[A\n","eval:  21%|‚ñà‚ñà        | 36/171 [00:03<00:14,  9.23it/s]\u001b[A\n","eval:  22%|‚ñà‚ñà‚ñè       | 37/171 [00:04<00:14,  8.94it/s]\u001b[A\n","eval:  22%|‚ñà‚ñà‚ñè       | 38/171 [00:04<00:14,  9.00it/s]\u001b[A\n","eval:  23%|‚ñà‚ñà‚ñé       | 39/171 [00:04<00:14,  9.06it/s]\u001b[A\n","eval:  23%|‚ñà‚ñà‚ñé       | 40/171 [00:04<00:14,  8.92it/s]\u001b[A\n","eval:  24%|‚ñà‚ñà‚ñç       | 41/171 [00:04<00:14,  8.67it/s]\u001b[A\n","eval:  25%|‚ñà‚ñà‚ñç       | 42/171 [00:04<00:14,  8.70it/s]\u001b[A\n","eval:  25%|‚ñà‚ñà‚ñå       | 43/171 [00:04<00:14,  8.60it/s]\u001b[A\n","eval:  26%|‚ñà‚ñà‚ñã       | 45/171 [00:04<00:13,  9.68it/s]\u001b[A\n","eval:  27%|‚ñà‚ñà‚ñã       | 47/171 [00:05<00:11, 10.60it/s]\u001b[A\n","eval:  29%|‚ñà‚ñà‚ñä       | 49/171 [00:05<00:10, 11.39it/s]\u001b[A\n","eval:  30%|‚ñà‚ñà‚ñâ       | 51/171 [00:05<00:11, 10.13it/s]\u001b[A\n","eval:  31%|‚ñà‚ñà‚ñà       | 53/171 [00:05<00:12,  9.71it/s]\u001b[A\n","eval:  32%|‚ñà‚ñà‚ñà‚ñè      | 54/171 [00:05<00:12,  9.69it/s]\u001b[A\n","eval:  32%|‚ñà‚ñà‚ñà‚ñè      | 55/171 [00:05<00:11,  9.68it/s]\u001b[A\n","eval:  33%|‚ñà‚ñà‚ñà‚ñé      | 57/171 [00:06<00:10, 10.53it/s]\u001b[A\n","eval:  35%|‚ñà‚ñà‚ñà‚ñç      | 59/171 [00:06<00:11,  9.38it/s]\u001b[A\n","eval:  35%|‚ñà‚ñà‚ñà‚ñå      | 60/171 [00:06<00:12,  9.07it/s]\u001b[A\n","eval:  36%|‚ñà‚ñà‚ñà‚ñå      | 61/171 [00:06<00:12,  9.04it/s]\u001b[A\n","eval:  36%|‚ñà‚ñà‚ñà‚ñã      | 62/171 [00:06<00:12,  8.56it/s]\u001b[A\n","eval:  37%|‚ñà‚ñà‚ñà‚ñã      | 64/171 [00:06<00:11,  9.05it/s]\u001b[A\n","eval:  38%|‚ñà‚ñà‚ñà‚ñä      | 65/171 [00:06<00:11,  9.06it/s]\u001b[A\n","eval:  39%|‚ñà‚ñà‚ñà‚ñä      | 66/171 [00:07<00:11,  9.22it/s]\u001b[A\n","eval:  40%|‚ñà‚ñà‚ñà‚ñâ      | 68/171 [00:07<00:09, 10.45it/s]\u001b[A\n","eval:  41%|‚ñà‚ñà‚ñà‚ñà      | 70/171 [00:07<00:09, 10.71it/s]\u001b[A\n","eval:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 72/171 [00:07<00:09, 10.51it/s]\u001b[A\n","eval:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 74/171 [00:07<00:08, 10.80it/s]\u001b[A\n","eval:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 76/171 [00:07<00:08, 11.11it/s]\u001b[A\n","eval:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 78/171 [00:08<00:09, 10.12it/s]\u001b[A\n","eval:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 80/171 [00:08<00:08, 10.18it/s]\u001b[A\n","eval:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 82/171 [00:08<00:09,  9.88it/s]\u001b[A\n","eval:  49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 83/171 [00:08<00:09,  9.66it/s]\u001b[A\n","eval:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 84/171 [00:08<00:09,  9.62it/s]\u001b[A\n","eval:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 85/171 [00:08<00:09,  9.26it/s]\u001b[A\n","eval:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 87/171 [00:09<00:08,  9.68it/s]\u001b[A\n","eval:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 89/171 [00:09<00:07, 10.30it/s]\u001b[A\n","eval:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 91/171 [00:09<00:08,  9.93it/s]\u001b[A\n","eval:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 92/171 [00:09<00:08,  9.74it/s]\u001b[A\n","eval:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 93/171 [00:09<00:07,  9.76it/s]\u001b[A\n","eval:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 94/171 [00:09<00:08,  9.37it/s]\u001b[A\n","eval:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 95/171 [00:09<00:08,  9.48it/s]\u001b[A\n","eval:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 97/171 [00:10<00:07,  9.52it/s]\u001b[A\n","eval:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 98/171 [00:10<00:07,  9.40it/s]\u001b[A\n","eval:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 99/171 [00:10<00:07,  9.31it/s]\u001b[A\n","eval:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 101/171 [00:10<00:06, 10.38it/s]\u001b[A\n","eval:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 103/171 [00:10<00:06,  9.94it/s]\u001b[A\n","eval:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 105/171 [00:10<00:06, 10.31it/s]\u001b[A\n","eval:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 107/171 [00:11<00:06, 10.66it/s]\u001b[A\n","eval:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 109/171 [00:11<00:06, 10.12it/s]\u001b[A\n","eval:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 111/171 [00:11<00:06,  9.76it/s]\u001b[A\n","eval:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 113/171 [00:11<00:05,  9.82it/s]\u001b[A\n","eval:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 114/171 [00:11<00:05,  9.64it/s]\u001b[A\n","eval:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 116/171 [00:12<00:05,  9.99it/s]\u001b[A\n","eval:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 118/171 [00:12<00:05, 10.43it/s]\u001b[A\n","eval:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 120/171 [00:12<00:05, 10.14it/s]\u001b[A\n","eval:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 122/171 [00:12<00:05,  9.78it/s]\u001b[A\n","eval:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 124/171 [00:12<00:04,  9.74it/s]\u001b[A\n","eval:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 126/171 [00:13<00:04, 10.08it/s]\u001b[A\n","eval:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 128/171 [00:13<00:04,  9.38it/s]\u001b[A\n","eval:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 129/171 [00:13<00:04,  9.10it/s]\u001b[A\n","eval:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 131/171 [00:13<00:04,  9.47it/s]\u001b[A\n","eval:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 132/171 [00:13<00:04,  9.39it/s]\u001b[A\n","eval:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 133/171 [00:13<00:04,  9.48it/s]\u001b[A\n","eval:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 135/171 [00:13<00:03, 10.11it/s]\u001b[A\n","eval:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 137/171 [00:14<00:03, 10.85it/s]\u001b[A\n","eval:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 139/171 [00:14<00:02, 11.23it/s]\u001b[A\n","eval:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 141/171 [00:14<00:02, 10.76it/s]\u001b[A\n","eval:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 143/171 [00:14<00:02, 10.21it/s]\u001b[A\n","eval:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 145/171 [00:14<00:02, 10.15it/s]\u001b[A\n","eval:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 147/171 [00:15<00:02,  9.59it/s]\u001b[A\n","eval:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 148/171 [00:15<00:02,  9.47it/s]\u001b[A\n","eval:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 149/171 [00:15<00:02,  9.56it/s]\u001b[A\n","eval:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 151/171 [00:15<00:01, 10.17it/s]\u001b[A\n","eval:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 153/171 [00:15<00:01,  9.44it/s]\u001b[A\n","eval:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 154/171 [00:15<00:01,  9.42it/s]\u001b[A\n","eval:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 156/171 [00:16<00:01,  9.52it/s]\u001b[A\n","eval:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 157/171 [00:16<00:01,  9.51it/s]\u001b[A\n","eval:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 158/171 [00:16<00:01,  9.18it/s]\u001b[A\n","eval:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 159/171 [00:16<00:01,  9.28it/s]\u001b[A\n","eval:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 161/171 [00:16<00:00, 10.04it/s]\u001b[A\n","eval:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 162/171 [00:16<00:00,  9.82it/s]\u001b[A\n","eval:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 163/171 [00:16<00:00,  9.19it/s]\u001b[A\n","eval:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 164/171 [00:16<00:00,  8.86it/s]\u001b[A\n","eval:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 165/171 [00:17<00:00,  9.01it/s]\u001b[A\n","eval:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 166/171 [00:17<00:00,  8.90it/s]\u001b[A\n","eval:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 168/171 [00:17<00:00,  9.27it/s]\u001b[A\n","eval:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 169/171 [00:17<00:00,  8.73it/s]\u001b[A\n","eval: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 171/171 [00:17<00:00,  9.63it/s]\n","\n","eval:   0%|          | 0/25 [00:00<?, ?it/s]\u001b[A\n","eval:   4%|‚ñç         | 1/25 [00:00<00:02,  8.59it/s]\u001b[A\n","eval:   8%|‚ñä         | 2/25 [00:00<00:02,  8.20it/s]\u001b[A\n","eval:  12%|‚ñà‚ñè        | 3/25 [00:00<00:02,  8.85it/s]\u001b[A\n","eval:  20%|‚ñà‚ñà        | 5/25 [00:00<00:02,  9.29it/s]\u001b[A\n","eval:  24%|‚ñà‚ñà‚ñç       | 6/25 [00:00<00:02,  8.85it/s]\u001b[A\n","eval:  28%|‚ñà‚ñà‚ñä       | 7/25 [00:00<00:02,  8.82it/s]\u001b[A\n","eval:  32%|‚ñà‚ñà‚ñà‚ñè      | 8/25 [00:00<00:01,  8.71it/s]\u001b[A\n","eval:  36%|‚ñà‚ñà‚ñà‚ñå      | 9/25 [00:01<00:01,  8.76it/s]\u001b[A\n","eval:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 11/25 [00:01<00:01, 10.09it/s]\u001b[A\n","eval:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 13/25 [00:01<00:01, 10.85it/s]\u001b[A\n","eval:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 15/25 [00:01<00:01,  9.92it/s]\u001b[A\n","eval:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 16/25 [00:01<00:00,  9.83it/s]\u001b[A\n","eval:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 17/25 [00:01<00:00,  9.76it/s]\u001b[A\n","eval:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 19/25 [00:01<00:00, 10.74it/s]\u001b[A\n","eval:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 21/25 [00:02<00:00, 10.00it/s]\u001b[A\n","eval:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 23/25 [00:02<00:00,  9.42it/s]\u001b[A\n","eval: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25/25 [00:02<00:00,  9.68it/s]\n"," 80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 4/5 [04:55<01:13, 73.14s/it]"]},{"name":"stdout","output_type":"stream","text":["save the model to finetune-5-2e-05.pt\n","epoch 3: train loss :: 0.597, train acc :: 0.877, dev acc :: 0.857\n"]},{"name":"stderr","output_type":"stream","text":["\n","train-4:   0%|          | 0/171 [00:00<?, ?it/s]\u001b[A\n","train-4:   1%|          | 1/171 [00:00<00:44,  3.85it/s]\u001b[A\n","train-4:   1%|          | 2/171 [00:00<00:41,  4.03it/s]\u001b[A\n","train-4:   2%|‚ñè         | 3/171 [00:00<00:41,  4.09it/s]\u001b[A\n","train-4:   2%|‚ñè         | 4/171 [00:00<00:38,  4.30it/s]\u001b[A\n","train-4:   3%|‚ñé         | 5/171 [00:01<00:42,  3.88it/s]\u001b[A\n","train-4:   4%|‚ñé         | 6/171 [00:01<00:45,  3.59it/s]\u001b[A\n","train-4:   4%|‚ñç         | 7/171 [00:01<00:44,  3.69it/s]\u001b[A\n","train-4:   5%|‚ñç         | 8/171 [00:02<00:47,  3.46it/s]\u001b[A\n","train-4:   5%|‚ñå         | 9/171 [00:02<00:48,  3.31it/s]\u001b[A\n","train-4:   6%|‚ñå         | 10/171 [00:02<00:48,  3.35it/s]\u001b[A\n","train-4:   6%|‚ñã         | 11/171 [00:03<00:46,  3.41it/s]\u001b[A\n","train-4:   7%|‚ñã         | 12/171 [00:03<00:42,  3.74it/s]\u001b[A\n","train-4:   8%|‚ñä         | 13/171 [00:03<00:44,  3.55it/s]\u001b[A\n","train-4:   8%|‚ñä         | 14/171 [00:03<00:44,  3.52it/s]\u001b[A\n","train-4:   9%|‚ñâ         | 15/171 [00:04<00:47,  3.28it/s]\u001b[A\n","train-4:   9%|‚ñâ         | 16/171 [00:04<00:43,  3.60it/s]\u001b[A\n","train-4:  10%|‚ñâ         | 17/171 [00:04<00:43,  3.53it/s]\u001b[A\n","train-4:  11%|‚ñà         | 18/171 [00:04<00:42,  3.63it/s]\u001b[A\n","train-4:  11%|‚ñà         | 19/171 [00:05<00:43,  3.47it/s]\u001b[A\n","train-4:  12%|‚ñà‚ñè        | 20/171 [00:05<00:42,  3.54it/s]\u001b[A\n","train-4:  12%|‚ñà‚ñè        | 21/171 [00:05<00:42,  3.50it/s]\u001b[A\n","train-4:  13%|‚ñà‚ñé        | 22/171 [00:06<00:44,  3.33it/s]\u001b[A\n","train-4:  13%|‚ñà‚ñé        | 23/171 [00:06<00:46,  3.21it/s]\u001b[A\n","train-4:  14%|‚ñà‚ñç        | 24/171 [00:06<00:45,  3.24it/s]\u001b[A\n","train-4:  15%|‚ñà‚ñç        | 25/171 [00:07<00:41,  3.53it/s]\u001b[A\n","train-4:  15%|‚ñà‚ñå        | 26/171 [00:07<00:45,  3.17it/s]\u001b[A\n","train-4:  16%|‚ñà‚ñå        | 27/171 [00:07<00:44,  3.27it/s]\u001b[A\n","train-4:  16%|‚ñà‚ñã        | 28/171 [00:08<00:45,  3.15it/s]\u001b[A\n","train-4:  17%|‚ñà‚ñã        | 29/171 [00:08<00:45,  3.10it/s]\u001b[A\n","train-4:  18%|‚ñà‚ñä        | 30/171 [00:08<00:45,  3.11it/s]\u001b[A\n","train-4:  18%|‚ñà‚ñä        | 31/171 [00:09<00:42,  3.28it/s]\u001b[A\n","train-4:  19%|‚ñà‚ñä        | 32/171 [00:09<00:41,  3.34it/s]\u001b[A\n","train-4:  19%|‚ñà‚ñâ        | 33/171 [00:09<00:44,  3.11it/s]\u001b[A\n","train-4:  20%|‚ñà‚ñâ        | 34/171 [00:09<00:39,  3.47it/s]\u001b[A\n","train-4:  20%|‚ñà‚ñà        | 35/171 [00:10<00:40,  3.37it/s]\u001b[A\n","train-4:  21%|‚ñà‚ñà        | 36/171 [00:10<00:43,  3.08it/s]\u001b[A\n","train-4:  22%|‚ñà‚ñà‚ñè       | 37/171 [00:10<00:45,  2.92it/s]\u001b[A\n","train-4:  22%|‚ñà‚ñà‚ñè       | 38/171 [00:11<00:43,  3.06it/s]\u001b[A\n","train-4:  23%|‚ñà‚ñà‚ñé       | 39/171 [00:11<00:44,  2.97it/s]\u001b[A\n","train-4:  23%|‚ñà‚ñà‚ñé       | 40/171 [00:11<00:43,  3.01it/s]\u001b[A\n","train-4:  24%|‚ñà‚ñà‚ñç       | 41/171 [00:12<00:40,  3.22it/s]\u001b[A\n","train-4:  25%|‚ñà‚ñà‚ñç       | 42/171 [00:12<00:42,  3.02it/s]\u001b[A\n","train-4:  25%|‚ñà‚ñà‚ñå       | 43/171 [00:12<00:43,  2.93it/s]\u001b[A\n","train-4:  26%|‚ñà‚ñà‚ñå       | 44/171 [00:13<00:43,  2.93it/s]\u001b[A\n","train-4:  26%|‚ñà‚ñà‚ñã       | 45/171 [00:13<00:39,  3.20it/s]\u001b[A\n","train-4:  27%|‚ñà‚ñà‚ñã       | 46/171 [00:13<00:35,  3.53it/s]\u001b[A\n","train-4:  27%|‚ñà‚ñà‚ñã       | 47/171 [00:14<00:36,  3.44it/s]\u001b[A\n","train-4:  28%|‚ñà‚ñà‚ñä       | 48/171 [00:14<00:36,  3.41it/s]\u001b[A\n","train-4:  29%|‚ñà‚ñà‚ñä       | 49/171 [00:14<00:34,  3.55it/s]\u001b[A\n","train-4:  29%|‚ñà‚ñà‚ñâ       | 50/171 [00:14<00:35,  3.45it/s]\u001b[A\n","train-4:  30%|‚ñà‚ñà‚ñâ       | 51/171 [00:15<00:35,  3.37it/s]\u001b[A\n","train-4:  30%|‚ñà‚ñà‚ñà       | 52/171 [00:15<00:33,  3.60it/s]\u001b[A\n","train-4:  31%|‚ñà‚ñà‚ñà       | 53/171 [00:15<00:32,  3.60it/s]\u001b[A\n","train-4:  32%|‚ñà‚ñà‚ñà‚ñè      | 54/171 [00:15<00:30,  3.81it/s]\u001b[A\n","train-4:  32%|‚ñà‚ñà‚ñà‚ñè      | 55/171 [00:16<00:28,  4.07it/s]\u001b[A\n","train-4:  33%|‚ñà‚ñà‚ñà‚ñé      | 56/171 [00:16<00:32,  3.59it/s]\u001b[A\n","train-4:  33%|‚ñà‚ñà‚ñà‚ñé      | 57/171 [00:16<00:31,  3.64it/s]\u001b[A\n","train-4:  34%|‚ñà‚ñà‚ñà‚ñç      | 58/171 [00:17<00:28,  3.92it/s]\u001b[A\n","train-4:  35%|‚ñà‚ñà‚ñà‚ñç      | 59/171 [00:17<00:32,  3.49it/s]\u001b[A\n","train-4:  35%|‚ñà‚ñà‚ñà‚ñå      | 60/171 [00:17<00:30,  3.60it/s]\u001b[A\n","train-4:  36%|‚ñà‚ñà‚ñà‚ñå      | 61/171 [00:17<00:32,  3.39it/s]\u001b[A\n","train-4:  36%|‚ñà‚ñà‚ñà‚ñã      | 62/171 [00:18<00:30,  3.55it/s]\u001b[A\n","train-4:  37%|‚ñà‚ñà‚ñà‚ñã      | 63/171 [00:18<00:32,  3.30it/s]\u001b[A\n","train-4:  37%|‚ñà‚ñà‚ñà‚ñã      | 64/171 [00:18<00:30,  3.46it/s]\u001b[A\n","train-4:  38%|‚ñà‚ñà‚ñà‚ñä      | 65/171 [00:19<00:32,  3.30it/s]\u001b[A\n","train-4:  39%|‚ñà‚ñà‚ñà‚ñä      | 66/171 [00:19<00:31,  3.33it/s]\u001b[A\n","train-4:  39%|‚ñà‚ñà‚ñà‚ñâ      | 67/171 [00:19<00:30,  3.39it/s]\u001b[A\n","train-4:  40%|‚ñà‚ñà‚ñà‚ñâ      | 68/171 [00:19<00:27,  3.80it/s]\u001b[A\n","train-4:  40%|‚ñà‚ñà‚ñà‚ñà      | 69/171 [00:20<00:26,  3.82it/s]\u001b[A\n","train-4:  41%|‚ñà‚ñà‚ñà‚ñà      | 70/171 [00:20<00:26,  3.78it/s]\u001b[A\n","train-4:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 71/171 [00:20<00:25,  3.90it/s]\u001b[A\n","train-4:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 72/171 [00:20<00:26,  3.79it/s]\u001b[A\n","train-4:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 73/171 [00:21<00:26,  3.67it/s]\u001b[A\n","train-4:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 74/171 [00:21<00:25,  3.85it/s]\u001b[A\n","train-4:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 75/171 [00:21<00:27,  3.55it/s]\u001b[A\n","train-4:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 76/171 [00:22<00:25,  3.77it/s]\u001b[A\n","train-4:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 77/171 [00:22<00:27,  3.39it/s]\u001b[A\n","train-4:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 78/171 [00:22<00:27,  3.41it/s]\u001b[A\n","train-4:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 79/171 [00:23<00:27,  3.39it/s]\u001b[A\n","train-4:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 80/171 [00:23<00:28,  3.22it/s]\u001b[A\n","train-4:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 81/171 [00:23<00:28,  3.18it/s]\u001b[A\n","train-4:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 82/171 [00:23<00:27,  3.24it/s]\u001b[A\n","train-4:  49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 83/171 [00:24<00:27,  3.15it/s]\u001b[A\n","train-4:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 84/171 [00:24<00:27,  3.21it/s]\u001b[A\n","train-4:  50%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 85/171 [00:24<00:25,  3.39it/s]\u001b[A\n","train-4:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 86/171 [00:25<00:24,  3.41it/s]\u001b[A\n","train-4:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 87/171 [00:25<00:25,  3.29it/s]\u001b[A\n","train-4:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 88/171 [00:25<00:24,  3.36it/s]\u001b[A\n","train-4:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 89/171 [00:26<00:25,  3.21it/s]\u001b[A\n","train-4:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 90/171 [00:26<00:27,  2.99it/s]\u001b[A\n","train-4:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 91/171 [00:26<00:27,  2.88it/s]\u001b[A\n","train-4:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 92/171 [00:27<00:25,  3.13it/s]\u001b[A\n","train-4:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 93/171 [00:27<00:23,  3.34it/s]\u001b[A\n","train-4:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 94/171 [00:27<00:22,  3.39it/s]\u001b[A\n","train-4:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 95/171 [00:27<00:22,  3.39it/s]\u001b[A\n","train-4:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 96/171 [00:28<00:22,  3.32it/s]\u001b[A\n","train-4:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 97/171 [00:28<00:21,  3.39it/s]\u001b[A\n","train-4:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 98/171 [00:28<00:19,  3.65it/s]\u001b[A\n","train-4:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 99/171 [00:29<00:19,  3.61it/s]\u001b[A\n","train-4:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 100/171 [00:29<00:19,  3.65it/s]\u001b[A\n","train-4:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 101/171 [00:29<00:20,  3.42it/s]\u001b[A\n","train-4:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 102/171 [00:29<00:20,  3.35it/s]\u001b[A\n","train-4:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 103/171 [00:30<00:17,  3.80it/s]\u001b[A\n","train-4:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 104/171 [00:30<00:17,  3.91it/s]\u001b[A\n","train-4:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 105/171 [00:30<00:17,  3.80it/s]\u001b[A\n","train-4:  62%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 106/171 [00:30<00:17,  3.67it/s]\u001b[A\n","train-4:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 107/171 [00:31<00:18,  3.44it/s]\u001b[A\n","train-4:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 108/171 [00:31<00:16,  3.75it/s]\u001b[A\n","train-4:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 109/171 [00:31<00:17,  3.63it/s]\u001b[A\n","train-4:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 110/171 [00:32<00:17,  3.57it/s]\u001b[A\n","train-4:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 111/171 [00:32<00:15,  3.82it/s]\u001b[A\n","train-4:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 112/171 [00:32<00:16,  3.68it/s]\u001b[A\n","train-4:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 113/171 [00:32<00:17,  3.35it/s]\u001b[A\n","train-4:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 114/171 [00:33<00:17,  3.32it/s]\u001b[A\n","train-4:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 115/171 [00:33<00:17,  3.15it/s]\u001b[A\n","train-4:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 116/171 [00:34<00:18,  3.03it/s]\u001b[A\n","train-4:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 117/171 [00:34<00:17,  3.10it/s]\u001b[A\n","train-4:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 118/171 [00:34<00:18,  2.94it/s]\u001b[A\n","train-4:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 119/171 [00:34<00:16,  3.19it/s]\u001b[A\n","train-4:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 120/171 [00:35<00:17,  2.87it/s]\u001b[A\n","train-4:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 121/171 [00:35<00:16,  2.96it/s]\u001b[A\n","train-4:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 122/171 [00:35<00:14,  3.36it/s]\u001b[A\n","train-4:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 123/171 [00:36<00:14,  3.24it/s]\u001b[A\n","train-4:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 124/171 [00:36<00:13,  3.36it/s]\u001b[A\n","train-4:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 125/171 [00:36<00:13,  3.34it/s]\u001b[A\n","train-4:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 126/171 [00:37<00:13,  3.27it/s]\u001b[A\n","train-4:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 127/171 [00:37<00:13,  3.27it/s]\u001b[A\n","train-4:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 128/171 [00:37<00:12,  3.33it/s]\u001b[A\n","train-4:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 129/171 [00:38<00:12,  3.26it/s]\u001b[A\n","train-4:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 130/171 [00:38<00:12,  3.29it/s]\u001b[A\n","train-4:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 131/171 [00:38<00:11,  3.38it/s]\u001b[A\n","train-4:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 132/171 [00:38<00:11,  3.35it/s]\u001b[A\n","train-4:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 133/171 [00:39<00:11,  3.23it/s]\u001b[A\n","train-4:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 134/171 [00:39<00:10,  3.57it/s]\u001b[A\n","train-4:  79%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 135/171 [00:39<00:10,  3.55it/s]\u001b[A\n","train-4:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 136/171 [00:39<00:09,  3.67it/s]\u001b[A\n","train-4:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 137/171 [00:40<00:09,  3.51it/s]\u001b[A\n","train-4:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 138/171 [00:40<00:09,  3.35it/s]\u001b[A\n","train-4:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 139/171 [00:40<00:08,  3.64it/s]\u001b[A\n","train-4:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 140/171 [00:41<00:08,  3.84it/s]\u001b[A\n","train-4:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 141/171 [00:41<00:08,  3.64it/s]\u001b[A\n","train-4:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 142/171 [00:41<00:08,  3.42it/s]\u001b[A\n","train-4:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 143/171 [00:41<00:07,  3.69it/s]\u001b[A\n","train-4:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 144/171 [00:42<00:06,  4.18it/s]\u001b[A\n","train-4:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 145/171 [00:42<00:06,  3.88it/s]\u001b[A\n","train-4:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 146/171 [00:42<00:07,  3.54it/s]\u001b[A\n","train-4:  86%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 147/171 [00:43<00:07,  3.35it/s]\u001b[A\n","train-4:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 148/171 [00:43<00:06,  3.34it/s]\u001b[A\n","train-4:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 149/171 [00:43<00:07,  3.09it/s]\u001b[A\n","train-4:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 150/171 [00:44<00:06,  3.03it/s]\u001b[A\n","train-4:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 151/171 [00:44<00:06,  3.06it/s]\u001b[A\n","train-4:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 152/171 [00:44<00:05,  3.28it/s]\u001b[A\n","train-4:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 153/171 [00:44<00:05,  3.28it/s]\u001b[A\n","train-4:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 154/171 [00:45<00:05,  3.36it/s]\u001b[A\n","train-4:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 155/171 [00:45<00:04,  3.37it/s]\u001b[A\n","train-4:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 156/171 [00:45<00:04,  3.39it/s]\u001b[A\n","train-4:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 157/171 [00:46<00:03,  3.65it/s]\u001b[A\n","train-4:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 158/171 [00:46<00:04,  3.23it/s]\u001b[A\n","train-4:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 159/171 [00:46<00:03,  3.25it/s]\u001b[A\n","train-4:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 160/171 [00:47<00:03,  3.34it/s]\u001b[A\n","train-4:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 161/171 [00:47<00:03,  3.29it/s]\u001b[A\n","train-4:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 162/171 [00:47<00:02,  3.44it/s]\u001b[A\n","train-4:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 163/171 [00:48<00:02,  3.14it/s]\u001b[A\n","train-4:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 164/171 [00:48<00:02,  3.32it/s]\u001b[A\n","train-4:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 165/171 [00:48<00:01,  3.47it/s]\u001b[A\n","train-4:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 166/171 [00:48<00:01,  3.99it/s]\u001b[A\n","train-4:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 167/171 [00:48<00:00,  4.05it/s]\u001b[A\n","train-4:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 168/171 [00:49<00:00,  3.73it/s]\u001b[A\n","train-4:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 169/171 [00:49<00:00,  3.54it/s]\u001b[A\n","train-4:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 170/171 [00:49<00:00,  3.45it/s]\u001b[A\n","train-4: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 171/171 [00:50<00:00,  3.41it/s]\n","\n","eval:   0%|          | 0/171 [00:00<?, ?it/s]\u001b[A\n","eval:   1%|          | 2/171 [00:00<00:15, 11.16it/s]\u001b[A\n","eval:   2%|‚ñè         | 4/171 [00:00<00:15, 10.64it/s]\u001b[A\n","eval:   4%|‚ñé         | 6/171 [00:00<00:15, 10.93it/s]\u001b[A\n","eval:   5%|‚ñç         | 8/171 [00:00<00:15, 10.36it/s]\u001b[A\n","eval:   6%|‚ñå         | 10/171 [00:00<00:15, 10.70it/s]\u001b[A\n","eval:   7%|‚ñã         | 12/171 [00:01<00:15, 10.05it/s]\u001b[A\n","eval:   8%|‚ñä         | 14/171 [00:01<00:16,  9.24it/s]\u001b[A\n","eval:   9%|‚ñâ         | 15/171 [00:01<00:17,  9.14it/s]\u001b[A\n","eval:  10%|‚ñâ         | 17/171 [00:01<00:15,  9.70it/s]\u001b[A\n","eval:  11%|‚ñà         | 19/171 [00:01<00:15, 10.12it/s]\u001b[A\n","eval:  12%|‚ñà‚ñè        | 21/171 [00:02<00:15,  9.54it/s]\u001b[A\n","eval:  13%|‚ñà‚ñé        | 23/171 [00:02<00:14, 10.00it/s]\u001b[A\n","eval:  15%|‚ñà‚ñç        | 25/171 [00:02<00:15,  9.26it/s]\u001b[A\n","eval:  16%|‚ñà‚ñå        | 27/171 [00:02<00:15,  9.48it/s]\u001b[A\n","eval:  16%|‚ñà‚ñã        | 28/171 [00:02<00:14,  9.55it/s]\u001b[A\n","eval:  17%|‚ñà‚ñã        | 29/171 [00:02<00:14,  9.50it/s]\u001b[A\n","eval:  18%|‚ñà‚ñä        | 30/171 [00:03<00:15,  9.05it/s]\u001b[A\n","eval:  18%|‚ñà‚ñä        | 31/171 [00:03<00:15,  8.83it/s]\u001b[A\n","eval:  19%|‚ñà‚ñâ        | 33/171 [00:03<00:14,  9.71it/s]\u001b[A\n","eval:  20%|‚ñà‚ñâ        | 34/171 [00:03<00:14,  9.76it/s]\u001b[A\n","eval:  20%|‚ñà‚ñà        | 35/171 [00:03<00:15,  9.05it/s]\u001b[A\n","eval:  21%|‚ñà‚ñà        | 36/171 [00:03<00:15,  8.69it/s]\u001b[A\n","eval:  22%|‚ñà‚ñà‚ñè       | 37/171 [00:03<00:15,  8.76it/s]\u001b[A\n","eval:  22%|‚ñà‚ñà‚ñè       | 38/171 [00:03<00:15,  8.80it/s]\u001b[A\n","eval:  23%|‚ñà‚ñà‚ñé       | 39/171 [00:04<00:14,  8.87it/s]\u001b[A\n","eval:  24%|‚ñà‚ñà‚ñç       | 41/171 [00:04<00:12, 10.13it/s]\u001b[A\n","eval:  25%|‚ñà‚ñà‚ñç       | 42/171 [00:04<00:13,  9.57it/s]\u001b[A\n","eval:  25%|‚ñà‚ñà‚ñå       | 43/171 [00:04<00:14,  8.87it/s]\u001b[A\n","eval:  26%|‚ñà‚ñà‚ñå       | 44/171 [00:04<00:14,  8.58it/s]\u001b[A\n","eval:  26%|‚ñà‚ñà‚ñã       | 45/171 [00:04<00:14,  8.51it/s]\u001b[A\n","eval:  27%|‚ñà‚ñà‚ñã       | 46/171 [00:04<00:14,  8.82it/s]\u001b[A\n","eval:  28%|‚ñà‚ñà‚ñä       | 48/171 [00:05<00:12,  9.98it/s]\u001b[A\n","eval:  29%|‚ñà‚ñà‚ñâ       | 50/171 [00:05<00:12,  9.89it/s]\u001b[A\n","eval:  30%|‚ñà‚ñà‚ñà       | 52/171 [00:05<00:11, 10.27it/s]\u001b[A\n","eval:  32%|‚ñà‚ñà‚ñà‚ñè      | 54/171 [00:05<00:11, 10.55it/s]\u001b[A\n","eval:  33%|‚ñà‚ñà‚ñà‚ñé      | 56/171 [00:05<00:11,  9.98it/s]\u001b[A\n","eval:  34%|‚ñà‚ñà‚ñà‚ñç      | 58/171 [00:06<00:11, 10.07it/s]\u001b[A\n","eval:  35%|‚ñà‚ñà‚ñà‚ñå      | 60/171 [00:06<00:11,  9.71it/s]\u001b[A\n","eval:  36%|‚ñà‚ñà‚ñà‚ñå      | 61/171 [00:06<00:11,  9.72it/s]\u001b[A\n","eval:  37%|‚ñà‚ñà‚ñà‚ñã      | 63/171 [00:06<00:10, 10.16it/s]\u001b[A\n","eval:  38%|‚ñà‚ñà‚ñà‚ñä      | 65/171 [00:06<00:11,  9.33it/s]\u001b[A\n","eval:  39%|‚ñà‚ñà‚ñà‚ñâ      | 67/171 [00:06<00:10, 10.10it/s]\u001b[A\n","eval:  40%|‚ñà‚ñà‚ñà‚ñà      | 69/171 [00:07<00:10, 10.15it/s]\u001b[A\n","eval:  42%|‚ñà‚ñà‚ñà‚ñà‚ñè     | 71/171 [00:07<00:09, 10.48it/s]\u001b[A\n","eval:  43%|‚ñà‚ñà‚ñà‚ñà‚ñé     | 73/171 [00:07<00:09, 10.45it/s]\u001b[A\n","eval:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 75/171 [00:07<00:09, 10.14it/s]\u001b[A\n","eval:  45%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 77/171 [00:07<00:09,  9.99it/s]\u001b[A\n","eval:  46%|‚ñà‚ñà‚ñà‚ñà‚ñå     | 79/171 [00:08<00:09, 10.03it/s]\u001b[A\n","eval:  47%|‚ñà‚ñà‚ñà‚ñà‚ñã     | 81/171 [00:08<00:09,  9.57it/s]\u001b[A\n","eval:  48%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 82/171 [00:08<00:09,  9.41it/s]\u001b[A\n","eval:  49%|‚ñà‚ñà‚ñà‚ñà‚ñä     | 83/171 [00:08<00:09,  9.43it/s]\u001b[A\n","eval:  49%|‚ñà‚ñà‚ñà‚ñà‚ñâ     | 84/171 [00:08<00:09,  9.28it/s]\u001b[A\n","eval:  50%|‚ñà‚ñà‚ñà‚ñà‚ñà     | 86/171 [00:08<00:09,  9.40it/s]\u001b[A\n","eval:  51%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 88/171 [00:09<00:08, 10.27it/s]\u001b[A\n","eval:  53%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé    | 90/171 [00:09<00:07, 10.61it/s]\u001b[A\n","eval:  54%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 92/171 [00:09<00:07, 10.58it/s]\u001b[A\n","eval:  55%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç    | 94/171 [00:09<00:07, 10.38it/s]\u001b[A\n","eval:  56%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå    | 96/171 [00:09<00:07,  9.73it/s]\u001b[A\n","eval:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 97/171 [00:09<00:07,  9.71it/s]\u001b[A\n","eval:  57%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã    | 98/171 [00:10<00:07,  9.59it/s]\u001b[A\n","eval:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 99/171 [00:10<00:08,  9.00it/s]\u001b[A\n","eval:  58%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä    | 100/171 [00:10<00:07,  8.98it/s]\u001b[A\n","eval:  59%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 101/171 [00:10<00:07,  8.88it/s]\u001b[A\n","eval:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ    | 102/171 [00:10<00:08,  8.52it/s]\u001b[A\n","eval:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 103/171 [00:10<00:07,  8.80it/s]\u001b[A\n","eval:  61%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè   | 105/171 [00:10<00:06, 10.14it/s]\u001b[A\n","eval:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 107/171 [00:11<00:06, 10.04it/s]\u001b[A\n","eval:  63%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé   | 108/171 [00:11<00:06,  9.87it/s]\u001b[A\n","eval:  64%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 110/171 [00:11<00:06, 10.14it/s]\u001b[A\n","eval:  65%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç   | 111/171 [00:11<00:06,  9.18it/s]\u001b[A\n","eval:  66%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå   | 113/171 [00:11<00:06,  9.22it/s]\u001b[A\n","eval:  67%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã   | 115/171 [00:11<00:05,  9.72it/s]\u001b[A\n","eval:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 116/171 [00:11<00:05,  9.39it/s]\u001b[A\n","eval:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 117/171 [00:12<00:05,  9.28it/s]\u001b[A\n","eval:  69%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 118/171 [00:12<00:05,  9.41it/s]\u001b[A\n","eval:  70%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ   | 119/171 [00:12<00:05,  9.29it/s]\u001b[A\n","eval:  71%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà   | 121/171 [00:12<00:04, 10.00it/s]\u001b[A\n","eval:  72%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè  | 123/171 [00:12<00:04,  9.94it/s]\u001b[A\n","eval:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 124/171 [00:12<00:04,  9.94it/s]\u001b[A\n","eval:  73%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé  | 125/171 [00:12<00:04,  9.45it/s]\u001b[A\n","eval:  74%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç  | 127/171 [00:13<00:04, 10.49it/s]\u001b[A\n","eval:  75%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 129/171 [00:13<00:04, 10.48it/s]\u001b[A\n","eval:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 131/171 [00:13<00:04,  9.60it/s]\u001b[A\n","eval:  77%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã  | 132/171 [00:13<00:04,  9.54it/s]\u001b[A\n","eval:  78%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä  | 134/171 [00:13<00:03,  9.65it/s]\u001b[A\n","eval:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ  | 136/171 [00:14<00:03,  9.82it/s]\u001b[A\n","eval:  80%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà  | 137/171 [00:14<00:03,  9.45it/s]\u001b[A\n","eval:  81%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 139/171 [00:14<00:03,  9.95it/s]\u001b[A\n","eval:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 140/171 [00:14<00:03,  9.75it/s]\u001b[A\n","eval:  82%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè | 141/171 [00:14<00:03,  9.55it/s]\u001b[A\n","eval:  83%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé | 142/171 [00:14<00:03,  9.34it/s]\u001b[A\n","eval:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 144/171 [00:14<00:02, 10.06it/s]\u001b[A\n","eval:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 145/171 [00:14<00:02,  9.57it/s]\u001b[A\n","eval:  85%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå | 146/171 [00:15<00:02,  9.54it/s]\u001b[A\n","eval:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 148/171 [00:15<00:02, 10.18it/s]\u001b[A\n","eval:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 149/171 [00:15<00:02, 10.08it/s]\u001b[A\n","eval:  88%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä | 150/171 [00:15<00:02,  9.09it/s]\u001b[A\n","eval:  89%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ | 152/171 [00:15<00:01,  9.57it/s]\u001b[A\n","eval:  90%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 154/171 [00:15<00:01, 10.05it/s]\u001b[A\n","eval:  91%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà | 155/171 [00:15<00:01, 10.02it/s]\u001b[A\n","eval:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 157/171 [00:16<00:01, 10.43it/s]\u001b[A\n","eval:  93%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 159/171 [00:16<00:01,  9.75it/s]\u001b[A\n","eval:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñé| 160/171 [00:16<00:01,  9.21it/s]\u001b[A\n","eval:  94%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 161/171 [00:16<00:01,  9.37it/s]\u001b[A\n","eval:  95%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç| 162/171 [00:16<00:00,  9.44it/s]\u001b[A\n","eval:  96%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå| 164/171 [00:16<00:00, 10.08it/s]\u001b[A\n","eval:  97%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã| 166/171 [00:17<00:00, 10.56it/s]\u001b[A\n","eval:  98%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä| 168/171 [00:17<00:00,  9.31it/s]\u001b[A\n","eval:  99%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñâ| 169/171 [00:17<00:00,  8.98it/s]\u001b[A\n","eval: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 171/171 [00:17<00:00,  9.73it/s]\n","\n","eval:   0%|          | 0/25 [00:00<?, ?it/s]\u001b[A\n","eval:   4%|‚ñç         | 1/25 [00:00<00:02,  8.77it/s]\u001b[A\n","eval:   8%|‚ñä         | 2/25 [00:00<00:02,  8.42it/s]\u001b[A\n","eval:  12%|‚ñà‚ñè        | 3/25 [00:00<00:02,  8.94it/s]\u001b[A\n","eval:  20%|‚ñà‚ñà        | 5/25 [00:00<00:02,  9.86it/s]\u001b[A\n","eval:  24%|‚ñà‚ñà‚ñç       | 6/25 [00:00<00:02,  9.22it/s]\u001b[A\n","eval:  28%|‚ñà‚ñà‚ñä       | 7/25 [00:00<00:01,  9.16it/s]\u001b[A\n","eval:  32%|‚ñà‚ñà‚ñà‚ñè      | 8/25 [00:00<00:01,  8.98it/s]\u001b[A\n","eval:  36%|‚ñà‚ñà‚ñà‚ñå      | 9/25 [00:00<00:01,  8.91it/s]\u001b[A\n","eval:  44%|‚ñà‚ñà‚ñà‚ñà‚ñç     | 11/25 [00:01<00:01, 10.19it/s]\u001b[A\n","eval:  52%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè    | 13/25 [00:01<00:01, 10.95it/s]\u001b[A\n","eval:  60%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    | 15/25 [00:01<00:01,  9.99it/s]\u001b[A\n","eval:  68%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñä   | 17/25 [00:01<00:00,  9.86it/s]\u001b[A\n","eval:  76%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñå  | 19/25 [00:01<00:00, 10.59it/s]\u001b[A\n","eval:  84%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñç | 21/25 [00:02<00:00,  9.93it/s]\u001b[A\n","eval:  92%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñè| 23/25 [00:02<00:00,  9.40it/s]\u001b[A\n","eval: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25/25 [00:02<00:00,  9.77it/s]\n","100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 5/5 [06:05<00:00, 73.18s/it]\n","<ipython-input-14-1602b14e3c66>:279: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  saved = torch.load(args.filepath)\n"]},{"name":"stdout","output_type":"stream","text":["epoch 4: train loss :: 0.355, train acc :: 0.907, dev acc :: 0.845\n"]},{"name":"stderr","output_type":"stream","text":["<ipython-input-5-6906fe08d67a>:332: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n","  checkpoint_dict = torch.load(checkpoint, map_location=device)\n"]},{"name":"stdout","output_type":"stream","text":["load model from finetune-5-2e-05.pt\n","load 245 data from data/cfimdb-dev.txt\n","load 488 data from data/cfimdb-test.txt\n"]},{"name":"stderr","output_type":"stream","text":["eval: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 25/25 [00:02<00:00,  9.93it/s]\n","eval: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 49/49 [00:04<00:00, 10.13it/s]"]},{"name":"stdout","output_type":"stream","text":["dev acc :: 0.857\n","test acc :: 0.469\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["def get_args():\n","    # Instead of using argparse, we define a simple class to hold our parameters\n","    class Args:\n","        def __init__(self):\n","            self.train = \"data/cfimdb-train.txt\"\n","            self.dev = \"data/cfimdb-dev.txt\"\n","            self.test = \"data/cfimdb-test.txt\"\n","            self.label_names = \"data/cfimdb-label-mapping.json\"\n","            self.pretrained_model_path = \"stories42M.pt\"\n","            self.max_sentence_len = None\n","            self.seed = 1337\n","            self.epochs = 5\n","            self.option = \"finetune\"  # ('generate', 'prompt', 'finetune')'prompt: the Llama parameters are frozen; finetune: Llama parameters are updated',\n","            self.use_gpu = True  # Set to True if you want to use GPU\n","            self.generated_sentence_low_temp_out = \"generated-sentence-temp-0.txt\"\n","            self.generated_sentence_high_temp_out = \"generated-sentence-temp-1.txt\"\n","            self.dev_out = \"cfimdb-dev-finetuning-output.txt\"\n","            self.test_out = \"cfimdb-test-finetuning-output.txt\"\n","            self.batch_size = 10 # sst: 64, cfimdb: 8 can fit a 12GB GPU\n","            self.hidden_dropout_prob = 0.3\n","            self.lr = 2e-5 # default lr for 'pretrain': 1e-3, 'finetune': 1e-5\", default=2e-5\n","\n","    args = Args()\n","    print(f\"args: {vars(args)}\")\n","    return args\n","\n","if __name__ == \"__main__\":\n","    args = get_args()\n","    args.filepath = f'{args.option}-{args.epochs}-{args.lr}.pt'  # save path\n","    seed_everything(args.seed)  # fix the seed for reproducibility\n","\n","    if args.option == \"generate\":\n","        # Step 1\n","        # Complete this sentence to test your implementation!\n","        prefix = \"I have wanted to see this thriller for a while, and it didn't disappoint. Keanu Reeves, playing the hero John Wick, is\"\n","        generate_sentence(args, prefix, args.generated_sentence_low_temp_out, max_new_tokens=75, temperature=0.0)\n","        generate_sentence(args, prefix, args.generated_sentence_high_temp_out, max_new_tokens=75, temperature=1.0)\n","    elif args.option == \"prompt\":\n","        # Step 2\n","        # Solve this task with prompted language modeling\n","        test_with_prompting(args)\n","    elif args.option == \"finetune\":\n","        # Step 3\n","        # Finetune a classification model\n","        train(args)\n","\n","        # Step 4\n","        # Evaluate your model on the dev and test sets\n","        test(args)\n","    else:\n","        raise ValueError(f\"Invalid option: {args.option}\")"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.8"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":0}
